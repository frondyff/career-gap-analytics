post_id,title,body,created_utc,score,num_comments,url
1qwztq8,Snowflake native dbt question,"My organization that I work for is trying to move off of ADF and into Snowflake native dbt. Nobody at the org has really any experience in this, so I've been tasked to look into how do we make this possible.

Currently, our ADF setup uses templates that include a set of maintenance tasks such as row count checks, anomaly detection, and other general validation steps. Many of these responsibilities can be handled in dbt through tests and macros, and Iâ€™ve already implemented those pieces.

What Iâ€™d like to enable is a way for every new dbt project to automatically include these generic tests and macrosâ€”essentially a shared baseline that should apply to all dbt projects. The approach Iâ€™ve found in Snowflakeâ€™s documentation involves storing these templates in a GitHub repository and referencing that repo in dbt deps so new projects can pull them in as dependencies.

That said, weâ€™ve run into an issue where the GitHub integration appears to require a username to be associated with the repository URL. Itâ€™s not yet clear whether we can supply a personal access token instead, which is something weâ€™re currently investigating.

Given that limitation, Iâ€™m wondering if thereâ€™s a better or more standard way to achieve this patternâ€”centrally managed, reusable dbt tests and macros that can be easily consumed by all new dbt projects.",1770330535.0,1,0,https://www.reddit.com/r/dataengineering/comments/1qwztq8/snowflake_native_dbt_question/
1qwx5xo,Fresher data engineer - need guidance on what to be careful about when in production,"Hi everyone,

I am junior data engineer at one of the MBB. itâ€™s been a few moneths since I joined the workforce. There has been concerns raised on two projects i worked on that i use a lot of AI to write my code. i feel when it comes to production-grade code, i am still a noob and need help from AI. my reviews have been f\*\*ked because of using AI. I need guidance on what to be careful about when it comes to working in production environments. i feel youtube videos are not very production-friendly. I work on core data engineering and devops. Recently i learned about self-hosted and github hosted runners the hard way when i was trying to add Snyk into Github Actions in one of my projectâ€™s repository and i used youtube code and took help from AI which basically ran on github hosted runner instead of self hosted ones which I didnâ€™t know about and it wasnâ€™t clarified at any point of time that they have self hosted ones. This backfired on me and my stakeholders lost trust in my code and knowledge.

Asking for guidance and help from the experienced professionals here, what precautions(general or specific ones to your experience that you learned the hard way or are aware of) to take when working with production environments. need your guidance based on your experience so i donâ€™t make such mistakes and not rely on AIâ€™s half-baked suggestions.

Any help on core data engineering and devops is much appreciated.",1770324602.0,0,24,https://www.reddit.com/r/dataengineering/comments/1qwx5xo/fresher_data_engineer_need_guidance_on_what_to_be/
1qwwgeo,Is a MIS a good foundation for DE?,"I just graduated with a Statistics major and Computer Programming minor. I'm currently self-learning working with APIs and data mining. I have done a lot of data cleaning and validating in my degree courses and own projects. I worked through the recent Databricks boot camp by Baraa which gave me some idea of what DE is like. The point is, from what I see and others tell, is that tools are easier to learn but the theory and thinking is key.

I'm fortunate enough to be able to pursue a MS and that's my goal. I wanted to hear y'all's thoughts on a Masters in Information Sciences. Specifically something like this: https://ecatalog.nccu.edu/preview\_program.php?catoid=34&poid=6710

My goal is to learn everything data related (DA, DS & DE). I can do analysis but no one's hiring and so it's difficult to get domain experience. I'm working on contacting local businesses and offering free data analysis services in the hopes of getting some useful experience. I'm learning a lot of the DS tools myself and I have the Statistics knowledge to back me but there's no entry-level DS anymore. DE is the only one that appears to be difficult to self-learn and relies on learning on the job which is why I'm thinking a MS that helps me with that is better than a MS in DS (which are mostly new and cash-grabs).

I could also further study Applied Statistics but that's a different discussion. I wanted to get advice on MIS for DE specifically. Thanks!",1770323031.0,1,2,https://www.reddit.com/r/dataengineering/comments/1qwwgeo/is_a_mis_a_good_foundation_for_de/
1qwwdsl,Exporting date from Star rocks Generated Views with consistency,"Has anyone figured out a way to export a view or a Materialized view data from Star rocks out to a format like CSV / JSON mainly by making sure the data doesn't refresh or update during the export process.

I explored a workaround wherein we can create a materialized view on top of the existing view to be exported -- which will be created just for the purpose of Exporting as that secondary view would not update even if the earlier ( base ) view did.

But that would create a lot of load on Star rocks as we have lot of exports running parallelly / concurrently in a queue across multiple environments on a stack .

The OOB functionality from Star rocks like EXPORT keyword / Files feature does not work in our use case",1770322868.0,1,1,https://www.reddit.com/r/dataengineering/comments/1qwwdsl/exporting_date_from_star_rocks_generated_views/
1qwvy5y,Data Modeling expectations at Senior level,"Iâ€™m currently studying data modeling. Can someone suggest good resources?

Iâ€™ve read Kimballs book but really from experience questions were quite difficult.

Is there any video where person is explaining a Data Modeling round and is covering most of the things that Sr engineer should talk. 

English is not my first language so communication has been barrier, watching videos will help me understand what and how to talk.

What has helped you all?

Thank you in advance!",1770321890.0,13,3,https://www.reddit.com/r/dataengineering/comments/1qwvy5y/data_modeling_expectations_at_senior_level/
1qwsx9s,Which course is best for Job Ready,"If you had to choose a Course within data engineering, which one would you choose?",1770315381.0,0,1,https://www.reddit.com/r/dataengineering/comments/1qwsx9s/which_course_is_best_for_job_ready/
1qwrxe8,"Notebooks, Spark Jobs, and the Hidden Cost of Convenience","[Notebooks, Spark Jobs, and the Hidden Cost of Convenience | Miles Cole](https://milescole.dev/data-engineering/2026/02/04/Notebooks-vs-Spark-Jobs-in-Production.html)",1770313298.0,180,34,https://www.reddit.com/r/dataengineering/comments/1qwrxe8/notebooks_spark_jobs_and_the_hidden_cost_of/
1qwre7d,Migrating to the Lakehouse Without the Big Bang: An Incremental Approach,,1770312143.0,1,0,https://www.reddit.com/r/dataengineering/comments/1qwre7d/migrating_to_the_lakehouse_without_the_big_bang/
1qwn9ak,How do you document business logic in DBT ?,"Hi everyone, 

I have a question about business rules on DBT. It's pretty easy to document KPI or facts calculations as they are materialized by columns. In this case, you just have to add a description to the column. 

But what about filterng business logic ? 

  
Example: 

    # models/gold_top_sales.sql
    
    1 SELECT product_id, monthly_sales 
    2 FROM ref('bronze_monthly_sales') 
    3 WHERE country IN ('US', 'GB') AND category LIKE ""tech""

Where do you document this filter condition (line 3)?

For now I'm doing this in the YAML docs:

    version: 2
    models:
      - name: gold_top_sales
        description: |
          Monthly sales on our top countries and the top product catergory defined by business stakeholdes every 3 years.
          
          Filter: Include records where country is in the list of defined countries and category match the top product category selected.
          

Do you have more precise or better advices?",1770302986.0,14,21,https://www.reddit.com/r/dataengineering/comments/1qwn9ak/how_do_you_document_business_logic_in_dbt/
1qwleru,What happened to PMs? Do you still have someone filling those responsibilities?,"I'm at a comp that recently started delivery teams and due to politics it's difficult to understand what's not working because we're not doing it correctly or it's the new norm. 

Do you have someone on the team you can toss random ideas/thoughts at as they come up? Like today I realized we no longer use a handful of views and we're moving the source folder, great time to clean up inventory. I feel like I'm supposed to do more than simply sending an IM to the person leading the project. 

I want to focus on technical details but it seems like more and more planning/organization is being pushed down to engineers. The specs are slowly getting better but because we're agile we often build before they're ready. I expect this to eventually be fixed but damn is it frustrating. It almost ruins the job, if I wanted to deal with this stuff I would have gone down the analyst route. 

Is this likely due to my unique situation and the combination of agile/changing workflow makes it seem more chaotic than it would be after things settle down? ",1770298449.0,7,6,https://www.reddit.com/r/dataengineering/comments/1qwleru/what_happened_to_pms_do_you_still_have_someone/
1qwk3ci,Text-to-queries,"As a researcher, I found a lot of solutions that talk about text-to-sql.  
But I want to work on something more large: text to any databases.

  
is this a good idea? anyone interested working on this project?

  
Thank you for your feedback",1770294848.0,0,12,https://www.reddit.com/r/dataengineering/comments/1qwk3ci/texttoqueries/
1qwjl09,Data Lakehouse - Silver Layer Pattern,"Hi! I've been to several data warehousing projects lately, built with the ""medallion"" architecture and there are a few things which make me quite disturbed.

First - on all of these projects we were pushed by the ""data architect"" to use the Silver layer as a copy of the Bronze, only with SCD 2 logic on each table, leaving the original normalised table structure. No joining of tables, or other preparation of data allowed (the messy data preparation tables go to the Gold next to the star schema).

Second - it was decided, that all the tables and their columns are renamed to english (from Polish), which means that now we have three databases (Bronze, Silver and Gold), each with different names for the same columns and tables. Now when I get a SQL script with business logic from the analyst, I need to transcribe all the table and column names to the english (Silver layer) and then implement the transformation towards Gold. Whenever there is a discussion about the data logic, or I need to go back to the analyst with a question, I need to transpose all the english table&column names back to the Polish (Bronze) again. It's  time consuming. Then Gold has still different column names, as the star schema is adjusted to the reporting needs of the users.

Are you also experiencing this, is this some kind of a new trend? Would't it be so much easier to leave it with the original Polish names in the Silver, since there is no change to the data anyway and the lineage would be so much cleaner?

I understand the architects don't care what it takes to work with this as it's not their pain, but I don't understand that no one cares about the cost of this.. : D

Also I can see that people tend to think about the system as something developed once, not touching it afterwards. That goes completely against my experience. If the system is alive, then changes are required all the time, as the business evolves, which means the costs are heavily projecting to the future..

What are your views on this? Thanks for you opinion!",1770293328.0,3,6,https://www.reddit.com/r/dataengineering/comments/1qwjl09/data_lakehouse_silver_layer_pattern/
1qwfjfr,Lakeflow vs Fivetran,"My company is on databricks, but we have been using fivetran since before starting databricks. We have Postgres rds instances that we use fivetran to replicate from, but fivetran has been a rough experience - lots of recurring issues, fixing them usually requires support etc. 

We had a demo meeting with our databricks rep of lakeflow today, but it was a lot more code/manual setup than expected. We were expecting it to be a bit more out of the box, but the upside to that is we have more agency and control over issues and donâ€™t have to wait on support tickets to fix. 

We are only 2 data engineers, (were 4 but layoffs) and I sort of sit between data eng and data science so Iâ€™m less capable than the other, who is the tech lead for the team. 

Has anyone had experience with lakeflow, both, made this switch etc that can speak to the overhead work and maintainability of lakeflow in this case? Fivetran being extremely hands off is nice but weâ€™re a sub 50 person start up in a banking related space so data issues are not acceptable, hence why we are looking at just getting lakeflow up. ",1770278963.0,0,4,https://www.reddit.com/r/dataengineering/comments/1qwfjfr/lakeflow_vs_fivetran/
1qwerfi,AI that debugs production incidents and data pipelines - just launched,"Built an AI SRE that gathers context when something breaks - checks logs, recent deploys, metrics, runbooks - and posts findings in Slack. Works for infra incidents and data pipeline failures.

It reads your codebase and past incidents on setup so it actually understands your system. Auto-generates integrations for your internal tools instead of making you configure everything manually.

GitHub: [github.com/incidentfox/incidentfox](http://github.com/incidentfox/incidentfox)

Would love feedback from data engineers on what's missing for pipeline debugging!",1770276074.0,0,1,https://www.reddit.com/r/dataengineering/comments/1qwerfi/ai_that_debugs_production_incidents_and_data/
1qwdlph,Offered a client a choice of two options. I got a thumbs up in return.,"I'm building out a data source from a manually updated Excel file. The file will be ingested into a warehouse for reporting. I gave the client two options for formatting the file based on their existing setup. One option requires more work from the client upfront, but will save time when adding data in the future. The second one I can implement as-is without extra work on their end but will mean they have to do extra manual work when they want to update the source.

I sent them a message explaining this and asking which one they preferred. As the title suggests, their response was a thumbs up.

It's late and I don't have bandwidth to deal with this... Looks like a problem for Tomorrow Man (my favourite superhero, incidentally).

EDIT: I hate you all ğŸ˜‚",1770272100.0,27,18,https://www.reddit.com/r/dataengineering/comments/1qwdlph/offered_a_client_a_choice_of_two_options_i_got_a/
1qwd5of,Is someone using DuckDB in PROD?,"As many of you, I heard a lot about DuckDB then tried it and liked it for it's simplicity.

By the way, I don't see how it can be added in my current company production stack.

Does anyone use it on production? If yes, what are the use cases please?

I would be very happy to have some feedbacks",1770270673.0,76,46,https://www.reddit.com/r/dataengineering/comments/1qwd5of/is_someone_using_duckdb_in_prod/
1qwd3bg,Salesforce to S3 Sync,Iâ€™ve spoken with many teams that want Salesforce data in S3 but canâ€™t justify the cost of ETL tools. So I built anÂ open-source serverless utility you can deploy in your own AWS account. It exports Salesforce data to S3 and keeps it Athena-queryable via Glue. No AWS DevOps skills required. Write-up here: \[https://docs.supa-flow.io/blog/salesforce-to-s3-serverless-export\](https://docs.supa-flow.io/blog/salesforce-to-s3-serverless-export),1770270465.0,0,3,https://www.reddit.com/r/dataengineering/comments/1qwd3bg/salesforce_to_s3_sync/
1qw4d41,SynthForge IO: Free-to-use data modeler and data generator,"Hello!

We've built a FREE TO USE splendid little application for devs, data engineers, QA folks, and more. We're currently looking for beta testers.

[https://synthforge.io](https://synthforge.io)

There are no plans to charge for this service! We hope it will be kept alive through donations from the community (we'll set up a link for that soon). For now, we're eating the cost. Why? Honestly, because we like to build and see people use what we build. AND.... we ran a few BBSs back in the 80s/90s and love to provide these kinds of things.

There is a feedback system in the profile menu if you have suggestions, find bugs or want to leave any kind of comment. We have put a few rate limiters in place, simply because it's a free service and we want to make resources available to everyone. But if the defaults don't meet your needs, just leave a comment to us (click the quota icon in the menu) and just request it, we'll likely approve it.

Looking forward to your feedback and suggestions. Once we have some good testing we'll announce it on other platforms as well. And we GREATLY appreciate your help in making this a better product!",1770246909.0,1,2,https://www.reddit.com/r/dataengineering/comments/1qw4d41/synthforge_io_freetouse_data_modeler_and_data/
1qw2p4x,Is there value in staying at the same company >3 years to see it grow?,"I know typically people stay in the same company for 2-3 years. But it takes time to build Data projects and sometimes you have to stay for a while to see the changes, convince people internally the value of data and how to utilize it. It takes many years for data infrastructure to become mature. Consulting projects sometimes are messy because it can be short-sighted. 

  
However the field moves so fast. It feels like it might be better to go into consulting or contracting for example. Then you'd go from projects to projects and stay sharp. On the other hand, it also feels like that approach is missing the bigger picture. 

  
For people who are in the field for a long time, what's your experience? ",1770243009.0,25,30,https://www.reddit.com/r/dataengineering/comments/1qw2p4x/is_there_value_in_staying_at_the_same_company_3/
1qw09c0,How do you handle *individual* performance KPIs for data engineers?,"Hello,

  
First off, I am not a data engineer, but more of like a PO/Technical PM for the data engineering team. 

I'm looking for some perspective from other DE teams...My leadership is asking my boss and I to define \*individual performance\* KPIs for data engineers. It is important to say they aren't looking for team level metrics. There is pressure to have something measurable and consistent across the team.

I know this is tough...I don't like it at all. I keep trying to steer it back to the TEAM's performance/delivery/whatever, but here we are. :(

One initial idea I had was tracking story points committed vs completed per sprint, but I'm concerned this doesn't map well to reality. Especially because points are team relative, work varies in complexity, and of course there are always interruptions/support work that can get unevenly distributed.

I've also suggested tracking cycle time trends per individual (but NOT comparisons...), and defining role specific KPIs, since not every single engineer does the same type of work.

Unfortunately leadership wants something more uniform and explicitly individual.

So I'm curious to know from DE or even leaders that browse this subreddit:

* if your org tracks individual performance KPIs for data engineers and data scientists, what does that actually look like?
   * what worked well? what backfired?

Any real world examples would be appreciated. 

",1770237638.0,20,22,https://www.reddit.com/r/dataengineering/comments/1qw09c0/how_do_you_handle_individual_performance_kpis_for/
1qvz9ql,Data Transformation Architecture,"Hi All,

I work at a small but quickly growing start-up and we are starting to run into growing pains with our current data architecture and enabling the rest of the business to have access to data to help build reports/drive decisions.

Currently we leverage Airflow to orchestrate all DAGs and dump raw data into our datalake and then load into Redshift. (No CDC yet). Since all this data is in the raw as-landed format, we can't easily build reports and have no concept of Silver or Gold layer in our data architecture.

Questions

* What tooling do you find helpful for building cleaned up/aggregated views? (dbt etc.)
* What other layers would you think about adding over time to improve sophistication of our data architecture?

Thank you!

https://preview.redd.it/u9ejlj309jhg1.png?width=1762&format=png&auto=webp&s=a54502f37ea9f49efd92e864e8c27afbaa9b4755

",1770235498.0,5,13,https://www.reddit.com/r/dataengineering/comments/1qvz9ql/data_transformation_architecture/
1qvqvyc,Should I take up this gig?,"I currently work for Boeing as a Lead Data Engineer in India. 11 years of work experience. Work here is slow but steady. Low pressure but career progression is not very clear.

Got an opportunity. A small Indian services company gave a juicy offer. They will staff me into a boutique consulting firm (sounds like staff augmentation). The work sounds interesting- working on technical consulting efforts (hands on at first and then hopefully into more client engagement at the consulting firm).

Should I be worried about the model - I will effectively be a contractor at the consulting firm. Is it worth the risk? Which factors should I evaluate that can help me make this decision?

(I am excited about consulting- but not sure what % of my role will it entail)

Any advice is appreciated! ",1770217411.0,0,4,https://www.reddit.com/r/dataengineering/comments/1qvqvyc/should_i_take_up_this_gig/
1qvpix1,Advanced Kafka Schema Registry Patterns: Multi-Event Topics,"Schemas are a critical part of successful enterprise-wide Kafka deployments.

In this video I'm covering a problem I find interesting - when and how to keep different event types in a single Kafka Topic - and I'm talking about quite a few problems around this topic.

The video also contains two short demos - implementing Fat Union Schema in Avro and Schema References in Protobuf.

I'm talking mostly about Karapace and Apicurio with some mentions of other Schema Registries.

**Topics / patterns / problems covered in the video:**

* Single topic vs separate topics
* Subject Name Strategies
* Varying support for Schema References
* Server-side dereferencing",1770214152.0,3,0,https://www.reddit.com/r/dataengineering/comments/1qvpix1/advanced_kafka_schema_registry_patterns/
1qvoj0i,Financial engineering at its finest,"Iâ€™ve been spending time lately looking into how big tech companies use specific phrasing to mask (or highlight) their updates, especially with all the chip investment deals going on. 

Earlier this week, I was going through the Microsoft earnings call transcript and (based on what seems like shared sentiment in the market), I was curious how Fabric was represented. From my armchair analyst position, its adoption just doesnâ€™t seem to line up with what I assumed would exist by now...

On the recent FY26 Q2 call, Satya said:

>Two years since it became broadly available, Fabric's annual revenue run rate is now over $2 billion with over 31,000 customers... revenue up 60% year over year.

The first thing that made me skeptical is the type of metrics used for Fabric. â€œAnnual revenue run rateâ€ is NOT the same as â€œwe actually generated $2B over the last 12 months.â€ This is super normal when startups report earnings, since if a product is growing, run rate can look great even when realized trailing revenue is still catching up. Microsoft chose run rate wording here.

Then I looked at the previous earnings where Fabric was discussed. In FY25 Q3, they said Fabric had 21k paid customers and â€œ40% using Real-Time Intelligenceâ€ five months after GA, but â€œusingâ€ isnâ€™t defined in a way thatâ€™s tangible, which usually is telling. In last weekâ€™s earnings, Satya immediately discusses specific metrics, customer references, etc. for other products.

A huge part of why Iâ€™m also not convinced on adoption is because of the forced Power BI capacity migration. I know the world is all about financial engineering, and since Microsoft forced us all to migrate off of P-SKUs, itâ€™s not hard to advertise those numbers as great. The conspiracist in me says the numbers line up a little too neatly with the SKU migration:

* $2B in revenue run rate / 31,000 customers â‰ˆ $64.5k per customer per year.Â 
* Thatâ€™s conveniently right around the published price of an F64 reservation

Obviously an average is oversimplifying it, and I donâ€™t think Microsoft is lying about the metrics whatsoever, but I do think the phrasing doesnâ€™t line up with the marketing and what my account team saysâ€¦

The other thing I saw was how Microsoft talks when they have deeper adoption. They normally use harder metrics like customers >$1M, big deployments, customer references, etc. In the same FY26 Q2 transcript, Fabric gets the run-rate/customer count and then the conversation moves on. And thatâ€™s it. After that, I was surprised that Fabric was never mentioned on its own again, nor expanded upon, and outside of that sentence, Fabric was always mentioned with Foundry.

Earnings reports aren't everything, and 31,000 customers is a lot, so I went looking for proof in customer stories, and the majority of the stories are just implementation partners and consultancies whose practices depend on selling Fabric (Boutiques/Avanade types), not a flood of end-customer production migrations with scale numbers. (There are are a couple of enterprise stories like LSEG and Microsoftâ€™s internal team, but it doesnâ€™t feel like â€œno shortage.â€)

Please check me. Am I off base here? Or is the growth just because of the forced migration from Power BI?

",1770211632.0,43,4,https://www.reddit.com/r/dataengineering/comments/1qvoj0i/financial_engineering_at_its_finest/
1qvjogq,From business analyst to data engineering/science.. worth it or too late?,"Here's the thing...

I'm a senior business analyst now. I have comfortable job currently on pretty much every level. I could stay here until I retire. Legacy company, cool people, very nice atmosphere, I do well, team is good, boss values my work, no rush, no stress, you get the drift. The job itself however has become very boring. The most pleasant part of the work is unnecessary (front end) so I'm left with same stuff over and over again, pumping quite simple reports wondering if end users actually get something out of them or not. Plus the salary could be a bit higher (it's always the case) but objectively it is OK.

So here I am, getting this scary thoughts that... this is it for me. That I could just coast here until I get old. I'd miss better jobs, better money, better life.

So

The most ""smooth"" transition path for me would to break into data engineering. It seems logical, probable and interesting to me. Sometimes I read what other people do as DE and I simply get jealous. It just seems way more important, more technology based, better learning experience, better salaries, and just more serious job so to speak.

Hence my question..

**With this new AI era is it too late to get into data engineering at this point?**

* I read everywhere how hard it is to break through and change jobs now
* Tech is moving forward
* AI can write code in seconds that it would take me some time to learn
* Juniors DE seem to be obsolete cause mids can do their job as well Seniors DE are even more efficient now

**If anyone changed positions recently from BA/DA to DE I'd be thankful if you shared your experience.**

Thanks",1770195830.0,1,7,https://www.reddit.com/r/dataengineering/comments/1qvjogq/from_business_analyst_to_data_engineeringscience/
1qvizak,Data Engineering as an After Thought,,1770193210.0,453,19,https://www.reddit.com/r/dataengineering/comments/1qvizak/data_engineering_as_an_after_thought/
1qvi7fu,DBT orchestrator,"Hi everyone,

I have to choose an open source solution to orchestrate DBT and I would like to have some REX or advices please. 

There are a lot of them especially Airflow, Dragster, Kestra or even Argo workflows.

  
Do you have some feedbacks or why not to use one ?

  
Thank you very much for your contribution ",1770190339.0,20,40,https://www.reddit.com/r/dataengineering/comments/1qvi7fu/dbt_orchestrator/
1qvi7ae,Can the 'modern' data stack be fixed?,"I worked on multiple SMEs data stacks and data projects, and most of their issues came from lack of a centralized data governance.

Mainly due to juggling with dozens of SaaS tools and data connectors with varying data quality/governance. So each data source was managed separately from each other and without any consideration from other data sources, in terms of consistency and quality.

A true headache for analytics, and data-driven decision making.

I feel that the sensible solution is to outsource all data processes to all-in-one platforms to solve data governance issues, which most data issues stem from.

But then, that's my opinion.",1770190324.0,12,10,https://www.reddit.com/r/dataengineering/comments/1qvi7ae/can_the_modern_data_stack_be_fixed/
1qveu1c,wage compression,got clipped from my last job a few weeks ago and have been looking for a new gig. anyone notice the wage compression? im seeing sr DE jobs that were once paying 150k a year now down to 120k or even less.,1770179438.0,5,12,https://www.reddit.com/r/dataengineering/comments/1qveu1c/wage_compression/
1qvd8zc,Need Advice. Tech Stack for Organization that lack of human resource.,"Hello. Iâ€™d like to start by saying that this is my first time asking a question in this kind of format. If there are any mistakes, I apologize in advance. I should also mention that I have very little experience in the Data Engineering field, and I havenâ€™t worked in an organization that has a standard or mature Data Engineering team. My knowledge mostly comes from what I studied, and for some topics itâ€™s only at a surface level, with little real hands-on experience.

I currently work in an organization that does not have sufficient resources to recruit highly skilled Data Engineering personnel, and most of the work is driven by the data analytics team. The current systems were mostly built to solve immediate, short-term problems. Because of this, I have several questions and would like to seek advice from experienced members of this community.

My questions are divided into several parts, as follows:

* What kind of Data Tech Stack would be most appropriate (Open Source, Cloud Services, or Hybrid)?
* For a Data Orchestrator, is a code-based approach (such as Dagster or Airflow) or a GUI-based approach (such as SSIS) better in the long run, especially if the Data Engineering team needs to scale?
* What roles should exist within a Data Engineering team (e.g., Lead, Infrastructure, Operational Service), or is it actually unnecessary to divide the team into sub-roles?
* How should we choose Data Storage to suit each layer? Is it necessary to use newer technologies (such as Data Warehouse or Data Lakehouse), or should we choose based on the expertise of the organizationâ€™s IT department, which is likely more familiar with OLTP databases?
* For a Data Dictionary, should it be embedded directly into table names for convenience, documented separately, or handled through a dedicated platform (such as DataHub)?
* To comply with PDPA / security audits, should data be masked or encrypted before it reaches the data storage that the Data Engineering team has access to? And which department in the organization is typically responsible for this?
* As someone who can be considered a new Data Engineer, could you please recommend skills that I should learn or further develop?

Lastly, if there are any parts of my questions where I used incorrect terminology or misunderstood certain concepts, please feel free to point them out and explain. Iâ€™m still not fully confident in my understanding of this field.

Thank you in advance to everyone who takes the time to share their opinions and advice.   
PS. English is not my native language.",1770175015.0,4,4,https://www.reddit.com/r/dataengineering/comments/1qvd8zc/need_advice_tech_stack_for_organization_that_lack/
1qv7786,People who moved from DE to Analytics Engineering,"I want to learn about experiences of people who moved from DE to Analytics Engineering. Why did you make the change? What has been your learning so far and how do you see your career progress like how you would brand yourself? Is it a step up from previous role or a step down?

P.s Iâ€™m a DE with 8 years of experience curious to know if itâ€™s a good career move",1770159489.0,33,32,https://www.reddit.com/r/dataengineering/comments/1qv7786/people_who_moved_from_de_to_analytics_engineering/
1qv74oi,Not providing schema evolution in bronze,"We are giving a client an option for schema evolution in bronze, but they aren't having it.  Risk and cost is their concern.  It will take a bit more effort to design, build, and test the ingestion into bronze with schema drift/evolution.

Although implementing schema-evolution isn't a big deal, a more controlled approach to new columns still provides a viable trade off.

I'm looking at some different options to mitigate it.

We'll enforce schema (for the standard included fields) and ignore any new fields.  The source database is a production RDBMs, so ingesting RDMBS change tracking rows into bronze (append only) is going to really be valuable to the client.  However, the client is aware that they won't be getting new columns automatically.

We're approaching new columns like a change request.  If they want them in the data platform, we need to include into bronze first, then update the model in silver and then gold.

To approach it, we'd get the new field they want; include it into the ETL pipeline.  We'd also have to execute a one-off pipeline that would write all records for the table into bronze where there was a non-null value for that new field as a 'change' record first.

Then we turn on the ETL pipeline, and life continues on as normal and bronze is up to date with the new column.

Thoughts?  Would you approach it differently?",1770159323.0,1,6,https://www.reddit.com/r/dataengineering/comments/1qv74oi/not_providing_schema_evolution_in_bronze/
1qv5w7i,Fivetran cut off service without warning over a billing error,"I need to vent and have a shoulder to cry on (ib4 ""I told you so""). 

We've been a Fivetran customer since the early days. Renewed in August and provided a new email address for billing. Our account rep confirmed IN WRITING that they would do that. They didn't. Sent the invoice to old contacts intead, we never saw it. 

No past due notice.   
No grace period. 

This morning 10;30 am services turned off. 

We're a reverse-ELT shop: data warehouse feed *everything.* Salesforce to ERP. ERP to Salesforce, EAM to ERP, P2P to ERP, holy crap there's so much stuff I've built over the last few years. All down. I mean that's not even calling out the reporting! 

Wired the payment, proof from the bank send. Know what they said? 

""Reinstatement takes 24-48 hours""

Bro. 31k to 45k in our renewal cycle and we moved connectors off. 

I know it's so hot right now to shit on Fivetran. I'm here now. I was a fan (was featured on a dev post too). 

I can't get anyone on the phone, big delays in emails. Horror. ",1770156451.0,151,30,https://www.reddit.com/r/dataengineering/comments/1qv5w7i/fivetran_cut_off_service_without_warning_over_a/
1qv3q0b,Column-level lineage for 50K+ Snowflake tables (Solving problems to make new problems),"Been building lineage systems for the past 3 years. Table-level lineage is basically useless for actual debugging work. I wanted to share some things I learned getting to column-level at scale.

**My main problem**

Someone changes a column in a source table. Which downstream dashboards break Table-level lineage says ""everything connected to this table"" (useless, 200 false positives). Column-level says ""these 3 specific dashboard fields"", which is actually helpful.

**What didn't work**

**My first attempt: Regex parsing SQL**

Wrote a bunch of regex to pull column names from SELECT statements. Worked for simple queries. Completely fell apart with CTEs, subqueries, and window functions.

Example that broke it:

    WITH customers AS (
    Â Â SELECTÂ 
    Â Â Â Â c.id as customer_key,
    Â Â Â Â c.email as contact_email
    Â Â FROM raw.customers c
    )
    SELECT customer_key, contact_email FROM customers

My regex couldn't track that customer\_key came from c.id. I gave up after 2 weeks.

**My 2nd attempt: Query INFORMATION\_SCHEMA only**

Thought we could just use Snowflake's metadata tables to see column relationships. **Nope**. INFORMATION\_SCHEMA tells you schemas exist, not how data flows through queries.

I found success by parsing SQL properly with an actual parser, not regex. I used sqlparse for Python but JSQLParser works if you live in Java world.

Query Snowflake's QUERY\_HISTORY view, parse every SELECT/INSERT/CREATE TABLE AS statement, build a graph of column â†’ column relationships.

**The architecture**

    Snowflake QUERY_HISTORYÂ 
    Â Â â†“
    Extract SQL (last 7 days of queries)
    Â Â â†“
    SQL Parser (sqlparse)
    Â Â â†“
    Column Mapper (track renames/transforms)
    Â Â â†“
    Graph DB (Neo4j) + Search (Elasticsearch)

    import sqlparse
    from snowflake.connector import connect
    
    # Pull recent queries
    queries = snowflake.execute(""""""
    Â Â SELECT query_textÂ 
    Â Â FROM INFORMATION_SCHEMA.QUERY_HISTORYÂ 
    Â Â WHERE query_type IN ('SELECT', 'INSERT', 'CREATE_TABLE_AS_SELECT')
    Â Â AND start_time > DATEADD(day, -7, CURRENT_TIMESTAMP())
    """""")
    
    for query_text in queries:
    Â Â Â Â parsed = sqlparse.parse(query_text)[0]
    Â Â Â Â 
    Â Â Â Â # Extract SELECT columns
    Â Â Â Â select_cols = extract_columns(parsed)
    Â Â Â Â 
    Â Â Â Â # Extract FROM tables and resolve schemas
    Â Â Â Â source_tables = extract_tables(parsed)
    Â Â Â Â 
    Â Â Â Â # Handle SELECT * by querying schema
    Â Â Â Â if has_star_select(select_cols):
    Â Â Â Â Â Â Â Â select_cols = resolve_star_expressions(source_tables)
    Â Â Â Â 
    Â Â Â Â # Build edges: source_col -> output_col
    Â Â Â Â for output_col in select_cols:
    Â Â Â Â Â Â Â Â for input_col in output_col.dependencies:
    Â Â Â Â Â Â Â Â Â Â Â Â graph.add_edge(
    Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â from_col=f""{input_col.table}.{input_col.name}"",
    Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â to_col=f""{output_col.table}.{output_col.name}"",
    Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â transform_type=output_col.transform
    Â Â Â Â Â Â Â Â Â Â Â Â )

**Some issues I ran into**

**1. SELECT \* resolution**

When you see SELECT \* FROM customers JOIN orders, you need to know what columns exist in both tables at query execution time. Can't parse this statically.

The solution is to Query INFORMATION\_SCHEMA.COLUMNS to get table schemas, then expand \* to the actual column list.

**2. Column aliasing chains**

    SELECTÂ 
    Â Â customer_id as c_id,
    Â Â c_id as cust_id,Â  -- references the alias above
    Â Â cust_id as final_id

You have to track the alias chain through the entire query. The symbol table gets really messy really fast.

**3. Subqueries and CTEs**

Each level of nesting creates a new namespace. The parser needs to track which customer\_id is which when you have 3 nested CTEs all selecting customer\_id.

**4. Window functions and aggregates**

SUM(revenue) OVER (PARTITION BY customer\_id) means the output column depends on revenue (for the calculation) and customer\_id (for the partition), but differently.

Your lineage graph needs different edge types: ""aggregates,"" ""partitions\_by,"" ""direct\_reference.""

**Performance at 50K tables**

Parsing 7 days of query history (about 500K queries): 2 hours  
Storage: Neo4j graph (200M edges), Elasticsearch (column name search)  
Query time: ""Show me everything downstream of this column"" = sub-2 seconds  
Query time: ""Where is customer\_id used?"" = sub-1 second

To save yourself a future headache, save the 20% of lineage paths that get queried 80% of the time.

**What Iâ€™m still struggle with**

Cross-warehouse lineage. My data flows Snowflake â†’ Databricks â†’ back to Snowflake. This approach only sees the Snowflake side.

Real-time updates. I run lineage extraction every 6 hours. If someone on my team changes a column and immediately asks ""what breaks?"", they get stale data.

ML pipelines. Notebooks that do df.select(""customer\_id"") don't show up in Snowflake query logs. Thatâ€™s a blind spot.

What's your current table count? Curious where others hit the breaking point. Sorry for the wall of text!",1770151598.0,26,9,https://www.reddit.com/r/dataengineering/comments/1qv3q0b/columnlevel_lineage_for_50k_snowflake_tables/
1qv0iyl,DoorDash Sr Data Engineer,"Recently interviewed at DoorDash. 

Onsite had 4 rounds System Design, Data Modeling, Business Partner and Leadership 

The recruiter who had reached out regarding the role had transferred my profile to other recruiter at onsite process. 

This new recruiter , not friendly. In a cold email said that I should book time on her calendar for a prep call. Well there was not a single slot available for next 3 weeks. I kept checking for couple of days and finally found one. On the day of call she rescheduled for different time. On the call read the same pdf that she had shared with me over the email on what to expect. Not a great conversation. Iâ€™ve  met really good recruiters who are friendly enough. 

System Design question - question was quite big 6-7 lines. Iâ€™ll put it in simple words - Design DataBricks! Yes, you read it right! Interviewer was interested in knowing how will I write exact YAML code for this. I was able to answer all his questions.

Data Modeling - Design fitness app. But the interviewer wanted me to draw visualizations. Well never in my past 8 years of work experience I had to do any visualizations but looks like DE in DoorDash work on visualizations as well. It wasnâ€™t a basic graph , some advanced trend graph. 

Business Partner - DoorDash expanding business how will you go about it etc. basic questions interviewer also seem to be onboarded on the approaches

Leadership - Hiring Manager joined 2-3 minutes late. Didnâ€™t bother to apologize. I ignored that and continued to talk with my positive energy. He said he will leave 10 minutes at the end for me to ask any questions. 

Questions were normal tell me about the time kind. Situation based. I answered all. He had multiple follow up questions. Kept asking something from the list. It was almost 5 minutes to end the meeting and then he stopped and started sharing about the team. Even here he didnâ€™t ask if I have any questions. I had to ask him when we were at time if I can ask couple of questions. I felt like I performed well.

Next day morning Recruiterâ€™s cold email came in that team has not decided to move forward. 

Happy to answer any questions anyone has. ",1770144554.0,187,52,https://www.reddit.com/r/dataengineering/comments/1qv0iyl/doordash_sr_data_engineer/
1quz6kq,Looking for feedback on a self-deployed web interface for exploring BigQuery data by asking questions in natural language,"I built BigAsk, a self-deployed web interface for exploring BigQuery data by asking questions in natural language. Itâ€™s a fairly thin wrapper over the Gemini CLI meant to address some shortcomings it has in overcoming data querying challenges organizations face.

Iâ€™m a Software Engineer in infra/DevOps, but I have a few friends who work in roles where much of their time is spent fulfilling requests to fetch data from internal databases. Iâ€™ve heard it described as a â€œnecessary evilâ€ of their job which isnâ€™t very fulfilling to perform. Recently, Google has released some quite capable tools with the potential to enable those without technical experience using BigQuery to explore the data themselves, both for questions intended to return exact query results, and higher-level questions about more nebulous insights that can be gleaned from data. While these certainly wouldnâ€™t completely eliminate the need for human experts to write some queries or validate results of important ones, it seems to me like they could significantly empower many to save time and get faster answers.

Unfortunately, there are some pretty big limitations to the current offerings from Google that prevent them from actually enabling this empowerment, and this project seeks to fix them.

One is that the best tools are available in a limited set of interfaces. Those scattered throughout the already-lacking-in-user-friendliness BigQuery UI require some foundational BigQuery and data analysis skills to use, making their barrier to entry too high for many who could benefit from them. The most advanced features are only available in the Gemini CLI, but as a CLI, using it requires using a command-line, again putting it out-of-reach for many.

The second is a lack of safe access control. There's a reason BigQuery access is typically limited to a small group. Directly authorizing access to this data via the BigQuery UI or Gemini CLI to individual users who aren't well-versed in its stewardship carries large risks of data deletion or leaks. As someone with experience working professionally with managing cloud IAM within an organization, I know that attempts to distribute permissions to individual users while maintaining a limited scope on them also requires considerable maintenance overhead and comes with itâ€™s own set of security risks.

BigAsk enables anyone within an organization to easily and securely use the most powerful agentic data analysis tools available from Google to self-serve answers to their burning questions. It addresses the problems outlined above with a user-friendly web interface, centralized access management with a recommended permissions set, and simple, lightweight code and deployment instructions that can easily be extended or customized to deploy into the constraints of an existing Google Cloud project architecture.

Code here: [https://github.com/stevenwinnick/big-ask](https://github.com/stevenwinnick/big-ask)

Iâ€™d love any feedback on the project, especially from anyone who works or has worked somewhere where this could be useful. This is also my first time sharing a project to online forums, and Iâ€™d value feedback on any ways I could better share my work as well.",1770141713.0,1,4,https://www.reddit.com/r/dataengineering/comments/1quz6kq/looking_for_feedback_on_a_selfdeployed_web/
1quz677,DAMA CDMP DQ,"Hello guys, I am trying to study and get certified CDMP DQ Specialist Exam. Does anyone knows if there is Dumps or practice questions out there? I canâ€™t find any.",1770141691.0,2,0,https://www.reddit.com/r/dataengineering/comments/1quz677/dama_cdmp_dq/
1quxzvb,Are Python UDFs in Spark still less efficient than UDFs written in Scala or Java?,"I'm reading ""Spark: The Definitive"" guide and there's a part about how user defined functions in Python can be inefficient. This is the quote: 

""When you use the function, there are essentially two different things that occur. If the function is written in Scala or Java, you can use it within the Java Virtual Machine (JVM). This means that there will be little performance penalty aside from the fact that you canâ€™t take advantage of code generation capabilities that Spark has for builtin functions. There can be performance issues if you create or use a lot of objects; we cover that in the section on optimization in Chapter 19. 

If the function is written in Python, something quite different happens. Spark starts a Python process on the worker, serializes all of the data to a format that Python can understand (remember, it was in the JVM earlier), executes the function row by row on that data in the Python process, and then finally returns the results of the row operations to the JVM and Spark.

Starting this Python process is expensive, but the real cost is in serializing the data to Python. This is costly for two reasons: it is an expensive computation, but also, after the data enters Python, Spark cannot manage the memory of the worker. This means that you could potentially cause a worker to fail if it becomes resource constrained (because both the JVM and Python are competing for memory on the same machine). We recommend that you write your UDFs in Scala or Javaâ€”the small amount of time it should take you to write the function in Scala will always yield significant speed ups, and on top of that, you can still use the function from Python!"" 

I heard from Reddit that this book was written a long time ago and some things may be outdated. Is this still relevant with the latest versions of Spark? Are Python UDFs still significantly slower than Scala/Java UDFs in Spark? If yes, have you ever encountered a situation at work where someone actually wrote a UDF in Scala or Java and avoided using Python for the sake of performance increases?",1770139205.0,33,11,https://www.reddit.com/r/dataengineering/comments/1quxzvb/are_python_udfs_in_spark_still_less_efficient/
1quwnj3,Alternatives after MotherDuck Price Hike,"I was planning to finally move my data analytics from a dump of \~100 GiB parquet files in a file system, a collection of ad-hoc SQL files, Python and DuckDB notebooks, and an InfluxDB2 instance running with the same data for Grafana dashboards to Motherduck. I was planning a proper ingestion pipeline, raw data in S3, transformations, analysis and documentation with dbt, and using the Motherduck datasource to be able to query the same data in Grafana.

Now (February 2026) MotherDuck has changed their pricing scheme: instead of the [Lite Plan at $25](https://web.archive.org/web/20251219041116/https://motherduck.com/product/pricing/) monthly, the cheapest option now is the [Business Plan at $250](https://motherduck.com/product/pricing/) monthly, a 10-fold increase.

Does anyone have a suggestion on where to look for alternatives?",1770136313.0,22,16,https://www.reddit.com/r/dataengineering/comments/1quwnj3/alternatives_after_motherduck_price_hike/
1quvm92,"Are we all becoming ""Full Stack-something"" nowadays?","Whats up?

Without further ado... I've found myself in the position where I went from a standard data engineer where I took care of a couple of data services, some ETLs, moving a client infrastructure from one architecture to another...

Nowadays I'm already designing the 6th architecture of a project which includes Data Engineering + AI + ML. Besides doing that I did at the start, I also develop and design LLM applications, deploy ML algorithms, create tasks and project plannins and do follow-up with my team. I'm still a ""Senior DE"" on paper but I feel like a weird mix of coordinator (or tech lead whatever u call) and a ""Full Stack Data"" since I'm working in every step of the process. Master of none but an improviser of all arts.

I wonder if this is happening at other companies or in the market in general?",1770134047.0,79,19,https://www.reddit.com/r/dataengineering/comments/1quvm92/are_we_all_becoming_full_stacksomething_nowadays/
1quvcv4,"Tired of Airflow overhead for local dev? I built a minimal, local-first CLI orchestrator.",,1770133464.0,1,0,https://www.reddit.com/r/dataengineering/comments/1quvcv4/tired_of_airflow_overhead_for_local_dev_i_built_a/
1quv70v,Where to apply for jobs besides LinkedIn?,"Have 3+ years of experience in Data Engineering. Skills/Tools include: SQL, Python, Spark Databricks, creating API's, PowerBI, SQL Server, Azure/AWS, ETL, Pipeline Creation and Optimization, some production Data Science stuff involving NLP and Classification .

Looking for any sort of Data Science/Engineering/Analyst role that has a bit more strategy involved rather than just pure coding. 

Any websites that you use to find roles doing this other than Linkedin?

Is linkedin premium worth it?

Thanks",1770133105.0,3,7,https://www.reddit.com/r/dataengineering/comments/1quv70v/where_to_apply_for_jobs_besides_linkedin/
1quu7rb,Lessons learned from building AI analytics agents: build for chaos,"A writeâ€‘up on everything that went wrong (and eventually right) while building an AI analytics agent.

  
The post walks through:

* How local optimization (different teams tuning pieces in isolation) created a chaotic context window for the LLM
* The concrete patterns that actually helped in production: LLMâ€‘optimized schema/field representations, justâ€‘inâ€‘time tool instructions, and explicit recovery paths for errors
* Why our benchmarks looked great while real users were still asking â€œwhy is revenue down?â€ and getting useless answers
* Why we ended up with â€œbuild for chaos, not happy pathsâ€ as the main design principle",1770130905.0,2,0,https://www.reddit.com/r/dataengineering/comments/1quu7rb/lessons_learned_from_building_ai_analytics_agents/
1quu6j3,"Weird position, don't know where to go from here","I am on an integration pipeline and manage the repo for it. I inherited it and made some slight tweaks but did not make the system from scratch. Its really a one way integration between two OLTP systems where webhook from System A writes to apigw which routes to a lambda which writes to dynamo where another lambda on a schedule reads from dynamo and writes unprocessed records to System B. There's also another flow where we have a lamdba check System B to see if users have a token and if if they don't, the lambda writes them to SQS where we have another lambda batch from SQS, grab a token from System A and write them back to users in System B.

Its my first real pipeline as a ""Data Engineer"" but it feels like integration engineering not DE. It gets slightly more DE related where we are landing that same webhook data into S3 and materializing it in Redshift for downstream analysis. But im not involved in the code for this part, just the SQL querying for our group's own reporting on the data. 

Not sure where to go for next job, I am lacking the DBT and airflow skills most DE roles require and I have these cloud skills and some IaC stuff as well to sprinkle on top of my SQL and python skills. I have been a DE title for a year now (was Senior Data Analyst at same company and did some DA/DS for a few years before that) at diff companies) and I want to change roles and make more money for my family. Not sure what jobs to shoot for or how to market myself... Event Driven DE? Not sure. 

Also, I'm new to reddit so forgive me if there's a better board for this. One more thing is I have been reading books like DDIA so my systems thinking is getting good but my coding skills are rotting because I'm really just trouble shooting the logs and only enriching the code maybe once a month. This post is all over the place but I just need some general guidance. ",1770130833.0,1,0,https://www.reddit.com/r/dataengineering/comments/1quu6j3/weird_position_dont_know_where_to_go_from_here/
1qut137,Secure who can trigger a Teams webhook workflow when source is Snowflake webhook?,"Hey everyone

  
I'm creating a Snowflake webhook alert to Teams channel and the first block in the Teams workflow receives the request and has the ""Who can trigger the flow?"" set to ""Anyone"".  That doesn't sound right to me as it sounds like its open to the Internet (although they need to get the secret right) so how do you go about securing the channel so its not ""Anyone"" whether it is a user or only accept requests from Snowflake?",1770128086.0,3,1,https://www.reddit.com/r/dataengineering/comments/1qut137/secure_who_can_trigger_a_teams_webhook_workflow/
1qunozk,Data with zach,"I had been studying from zacksâ€™s community bootcamp from youtube, he had removed it. 
I had not completed it yet, and his paid courses are way too expensive, given my countryâ€™s currency is on the weaker side.
Where how should I keep learning data engineering topics, any type of resources is welcomed ",1770111648.0,12,8,https://www.reddit.com/r/dataengineering/comments/1qunozk/data_with_zach/
1qunhxb,Switching from Data Science to Data Engineering,"Hi everyone, I'm currently working in a data science role but was thinking about making the switch to data engineering. I have a background in statistics and have been working as a data scientist in biomedical research in academia for 1.5 years. This is my first job since finishing my Masters in statistics. When I first started the job, I was responsible for cleaning datasets from clinical trials (this was 90% of the work), statistical modeling, creating visual aids like graphs and charts, and presenting and explaining my work to biologists. After 6 months, my manager told me I ""wasn't a good fit"" for the role because I ""lack curiosity"". I wouldn't say he was wrong. I didn't mind the work but it also didn't excite me and I didn't find it that fulfilling.

I was transferred to a different team within the same company and my main project became writing programming scripts to automate compression of thousands of files from patient databases, and creating lookup tables containing information on all the files (such as patient identifiers, visit dates, etc.) This involved a lot of identifying and sometimes renaming files that were mislabeled, had missing information, or used different naming conventions, and make sure these edge cases were accounted for in the compression process. We also received multiple batches of files from different sources, and I had to modify the scripts to account for all the nuances between different sources. 

I noticed I enjoyed these projects much more and that I'm very precise and good at paying close attention to small details. I liked how expectations were more well-defined with this project and was more like ""it either works or it doesn't"", rather than the previous data science role which was much more open-ended. I feel like I do better when expectations are more structured and consistent, rather than exploratory. My new manager also noticed the new role was a much better fit for me.

That being said, I'm thinking about pivoting into data engineering for my next role because I feel it may be a better fit for me. I've been looking at job postings for data engineering roles, but I don't have many of the skills required for a lot of these roles. My work so far has mainly been in R since that's what my company uses, and I've had some exposure to SQL and Python. I know Python and SQL are important in data engineering and tech is all about transferable skills, but I feel like I don't yet have the toolbox to switch to data engineering, nor do I have strong software engineering skills. I'm also not sure if I will be a strong candidate considering how competitive the job market is nowadays. My plan for now is to learn the important skills so that I'm able to make the switch.

Those of you who switched from data science to data engineering, what was your experience like and how did you navigate that shift? What are the most important data engineering skills/tools I should familiarize myself with to become a competitive candidate and be ready for interviews? What are some good resources you would suggest for learning these skills/tools? And do you have any general advice for me? ",1770110871.0,12,9,https://www.reddit.com/r/dataengineering/comments/1qunhxb/switching_from_data_science_to_data_engineering/
1qumuao,I want to use a big 2 TB to work for my agent,"I have a database of Judgement of courts in India those file are in pdf mostly

i want to convert that database so that my Al agent can use it for research purposes

what would be the best way to do that in a effective and efficient way

details - judgement of all the court including supreme court and high court which are used as reference in court to cite those case in court, there are almost 14M judgement that are used as reference.

now i want to use that data so that my Al agent can access that and use it

also please suggest what would be the better option to deal with that data and what would be cheapest way to do so

and if any one can brake down the pricing do let me know

please tell me the best approach to this, Thank you",1770108383.0,0,5,https://www.reddit.com/r/dataengineering/comments/1qumuao/i_want_to_use_a_big_2_tb_to_work_for_my_agent/
1qulhge,Is Data Engineering dying? Is it hard to get into as a fresher?,"Iâ€™m a second year AI & DS engineering student, planning on becoming a data engineer.  
But nowadays everywhere I look, people are saying the tech and data industry is dying, especially data engineering.  
Is it really that bad? Is there still scope for freshers or am I walking into a dead field?",1770103320.0,0,7,https://www.reddit.com/r/dataengineering/comments/1qulhge/is_data_engineering_dying_is_it_hard_to_get_into/
1qukzwj,Switching Full stack SOFTWARE engineering to DATA/ML related field in next 2 years,"I'm currently in final year of my cs degree after that I
have to find internship but in my country data or ml
related internship/fulltime are scares. On the other
hand we get many opportunities on traditional software
developer roles. Now as fresher I want to start with
software engineering since I get more opportunities
here and after getting 3 years of experience 1 am willing
to change my career to data or ml related field. Is it
possible? Am missing something? Will it be out to
move on that related field in next 3 years?",1770101633.0,1,3,https://www.reddit.com/r/dataengineering/comments/1qukzwj/switching_full_stack_software_engineering_to/
1qukzhw,I'm building a CLI tool for data diffing,"https://preview.redd.it/ves9ksnz78hg1.png?width=2198&format=png&auto=webp&s=3db49b5c320d0e332b3dca2230d81f330dbafee5

I'm building a simple CLI tool called **tablediff** that allows to quickly perform a data diffing between two tables and print a nice summary of findings.

It works cross-database and also on CSV files (dunno, just in case). Also, there is a mode that allows to only compare schemas (useful to cross-check tables in DWH with their counterparts in the backend DB).

My main focus is usability and informative summary.

You can try it with:

    pip install tablediff-cli[snowflake] # or whatever adapter you need

Usage is straightforward:

    tablediff compare \
      TABLE_A \
      TABLE_B \
      --pk PRIMARY_KEY \
      --conn CONNECTION_STRING
      [--conn2 ...]        # secondary DB connection if needed
      [--extended]         # for extended output
      [--where ""age > 18""] # additional WHERE condition

Let me know what you think.

Source code: [https://libraries.io/pypi/tablediff-cli](https://libraries.io/pypi/tablediff-cli)",1770101597.0,17,20,https://www.reddit.com/r/dataengineering/comments/1qukzhw/im_building_a_cli_tool_for_data_diffing/
1qujowc,WhereScape to dbt,"I am consulting for a client and they use WhereScape RED for their data warehouse and would like to migrate to dbt (cloud/core) on Snowflake. While we can do the manual conversion, this might be quite costly(resource time doing refactoring by hand). Wanted to check if someone has come across tools/services that can achieve this conversion at scale?  ",1770097310.0,3,11,https://www.reddit.com/r/dataengineering/comments/1qujowc/wherescape_to_dbt/
1quddio,What are people transitioning to if they can't find a job?,"I have some time but I'm preparing myself for what will probably be the inevitable in this market. Im using outdated technology and in this market I keep seeing that classes or certs won't help. I've heard some say they changed directions and I'm curious what people are finding? 

I know we can transition to ML but I'm assuming that needs a math background. AI is an option but then you're competing with new grads (do we even stand a chance? Does our background experience help?). I'm asking for more general answers but my background issue is essentially being a jr-mid level at 3-4 different positions, all at smaller companies and more of a startup environment. Platform/cloud (AWS) engineering, bi developer, data engineer and architect. I would be EXTREMELY valuable if this background was at larger companies. 

From what I can see this isn't valuable unless you're senior/staff or a cloud architect level. They don't bring in jr/mid level and train them, at least not right now. ",1770079722.0,40,52,https://www.reddit.com/r/dataengineering/comments/1quddio/what_are_people_transitioning_to_if_they_cant/
1qub19h,"For SQL round, what flavor of SQL (MySQL vs PostgreSQL)?","During SQL round which flavor of SQL is preferred?   
Originally, I was studying using MySQL but then recently switched to Postgresql (because Snowflake is more similar to postgresql).

I found SQL problems to be much easier in MySQL vs Postgresql.. but wondering which flavor is.   
  
I know at the end of the day this is not too important vs the actual SQL concepts..  
  
but the reason I ask is because using MySQL you can group by and SELECT cols without aggregate functions (which imo makes it WAY easier to solve problems)   
  
vs   
  
in Postgresql, in a group by - you cannot simply select \* (you can in MySQL) which makes SQL problems much harder",1770073897.0,0,5,https://www.reddit.com/r/dataengineering/comments/1qub19h/for_sql_round_what_flavor_of_sql_mysql_vs/
1qua7yn,Thoughts on Booz Allen for DE?,"Was wondering if anyone has any positive or negative experiences there, specifically for Junior DE roles. Iâ€™ve been browsing consulting forms and the Reddit consensus is not too keen on Booz. Would it be worth it to work there for the TS/SCI?",1770072008.0,2,9,https://www.reddit.com/r/dataengineering/comments/1qua7yn/thoughts_on_booz_allen_for_de/
1qu9341,Which data lineage tool to use in large MNC?,"We are building a data lineage platform, our source are informatica power center, oracle stored procedure and spring batch jobs. What open source tool should we go for? Anyone has experience setting up lineage for either of these?
",1770069475.0,2,2,https://www.reddit.com/r/dataengineering/comments/1qu9341/which_data_lineage_tool_to_use_in_large_mnc/
1qu7dld,"Data Trap, prep , transformation tools?","Wondering if you all can give insight into some cheap/free tools that can parse/scrape data from text , pdf , etc files and allows for basic transformation and excel export features. Iâ€™ve used Altair Monarch for several years but my company is not renewing licensing bc there isnâ€™t much of a need for it anymore since we get most data stored in a data warehouse, But I still have several smallish jobs that arenâ€™t being stored in a DB. Thanks for your help. ",1770065710.0,3,3,https://www.reddit.com/r/dataengineering/comments/1qu7dld/data_trap_prep_transformation_tools/
1qu57bm,Modeling 1: N relationships for Tableau Consumption,"Hi all,Â 

How would you all model a 1: N relationship in a SQL Data Mart to streamline the consumption for Tableau?Â 

My organization is debating this topic internally and we haven't reached an agreement so far.Â 

A hypothetical use case is our service data. One service can be attached to multiple account codes (and can be offered in multiple branches as well). Â 

Here are the options for the data mart. Â 

**Option A:**Â Basically, the 3NF

https://preview.redd.it/dazl3okpv4hg1.png?width=1009&format=png&auto=webp&s=1132687320f4ff596da43013f4de98559be88eb2

Â 

[](https://preview.redd.it/modeling-1-n-relationships-for-tableau-consumption-v0-ra1u2j2fu4hg1.png?width=1069&format=png&auto=webp&s=f87b68a51f3ab5325790913a50fe587c93e13321)

**Option B:**

A simple bridge tableÂ 

https://preview.redd.it/jbs1f86sv4hg1.png?width=1300&format=png&auto=webp&s=bb085c6801f03fa8e68c0ce35264fcc986c41eea

Â 

[](https://preview.redd.it/modeling-1-n-relationships-for-tableau-consumption-v0-hbqpggfhu4hg1.png?width=1071&format=png&auto=webp&s=4afd71d5672352a3b8adebfab9609551dbcd75f6)

**Option C:**Â A derivation of theÂ [i2b2 model](https://www.researchgate.net/figure/Ontology-driven-data-transformation-in-i2b2-The-ontology-which-defines-concept-metadata_fig1_331215339)Â ([4. Tutorials: Using the i2b2 CDM - Bundles and CDM - i2b2 Community Wiki](https://community.i2b2.org/wiki/display/BUN/4.+Tutorials%3A+Using+the+i2b2+CDM)) Â 

In this case, all 1:N relationships (account code, branches, etc) would be stored at the concept table

https://preview.redd.it/aa16mmwwv4hg1.png?width=955&format=png&auto=webp&s=cb335a755ac547ecfdfe0cb545d17644d063dfeb

Â 

[](https://i.redd.it/k0dqbwblu4hg1.png)

**Option D:**

DenormalizedÂ 

https://preview.redd.it/kpv4bxemv4hg1.png?width=754&format=png&auto=webp&s=7238a2cb7e9a8c0abcd3b6d1333bdf01e0a0c93c

Â 

[](https://i.redd.it/c6k0gj2nu4hg1.png)

*What's the use case for reporting?*

Â The main one would be to generate tabular data through Tableau such as the example below and be able to filter it through a specific field (service name, account code).Â 

Down the line, there would also be some reports of how many clients were serviced by each serviced or the budget/expense amount for each account code Â 

Â 

Example:

https://preview.redd.it/9m950pg0w4hg1.png?width=706&format=png&auto=webp&s=b5833ecad8d518fcea8c6add288ce1e82ab5c9af

Â 

[](https://preview.redd.it/modeling-1-n-relationships-for-tableau-consumption-v0-wcnz2u0cu4hg1.png?width=463&format=png&auto=webp&s=24c74a86a307ea74421317d4618b52f98396c0c5)

Based on your experience, which model would you recommend (or an alternative proposal) to smooth the consumption on Tableau?Â 

Happy to answer additional questions.

We appreciate your support!Â 

Â Thanks!Â ",1770061041.0,7,7,https://www.reddit.com/r/dataengineering/comments/1qu57bm/modeling_1_n_relationships_for_tableau_consumption/
1qu4btu,Technical Screen Time Limits Advice,"I have been looking for a new job after not having any growth in my current job. I have about 4 years experience as an Analytics Engineer and I can't seem to get past technical screens. I think this is because I never finish all the questions in time.

These technical screens can be between 30min to an hour and 4-5 questions. I'm very confident in my SQL abilities but between understanding the problem and writing the code, all my time is consumed. 

I acknowledge that not being able to finish in time could mean that I am may not be qualified for the role but I also think that once on the job, the timed aspect is not as severe due to other factors like being more comfortable with the schemas, and business sense.

I know the job market is tough, but this is not what I'm asking about. How can I be more efficient in these screens? I've tried LeetCode and other things but the structure of the questions don't tend to match or are not as useful.

Or do I need a reality check with not being as qualified as I think I am?

Edit: removed repetition",1770059202.0,5,1,https://www.reddit.com/r/dataengineering/comments/1qu4btu/technical_screen_time_limits_advice/
1qu47hc,When Your Career Doesnâ€™t Go as Planned,"Sometimes in life, what you plan doesnâ€™t work out.

I prepared for a Data Engineer role since college. I got selected on campus at Capgemini, but after joining, I was placed into the SAP ecosystem. When I asked for a domain change, I was told itâ€™s not possible.

Now Iâ€™m studying on my own and applying daily for Data Engineer roles on LinkedIn and Naukri, but Iâ€™m not getting any responses.

It feels like no matter how much we try, our path is already written somewhere else. Still trying. Still learning.",1770058953.0,35,20,https://www.reddit.com/r/dataengineering/comments/1qu47hc/when_your_career_doesnt_go_as_planned/
1qu21jd,Databricks or AWS certification?,Which do you all think holds more value in the data engineer field? I'm looking for a new job and am working on some certifications. I already have experience with AWS but none with Databricks. Trying to weigh the options and decide which would be more valuable as I may only have time for one certification.,1770054508.0,12,10,https://www.reddit.com/r/dataengineering/comments/1qu21jd/databricks_or_aws_certification/
1qu1kc9,Schema3D - Now open-source with shareable schema URLs [Update],"A few months ago I sharedÂ [Schema3D](https://schema3d.com/)Â here - since then, I've implemented several feedback-motivated enhancements, and wanted to share the latest updates.



**What's new:**

* Custom category filtering: organize tables by domain/feature
* Shareable URLs: entire schema & view state encoded in the URL (no backend needed)
* Open source: full code now available onÂ [GitHub](https://github.com/shane-jacobeen/schema3d)

**Links**:

* [Demo](https://schema3d.com/)
* [GitHub](https://github.com/shane-jacobeen/schema3d)
* [Previous post](https://www.reddit.com/r/dataengineering/comments/1pi9a9o/schema3d_an_experiment_to_solve_the_erd_spaghetti/)



The URL sharing means you can embed schema snapshots in runbooks, architecture docs, or migration plans without external dependencies.

I hope this is helpful as a tool to design, document, and communicate relational DB schemas. **What features would make this actually useful for your projects?**",1770053487.0,4,0,https://www.reddit.com/r/dataengineering/comments/1qu1kc9/schema3d_now_opensource_with_shareable_schema/
1qu12iq,Interest,"Iâ€™m looking to get into data engineering after the military in 5 years. Iâ€™ll be at 20 years of service by that point. Iâ€™m really looking into this field. I honestly know nothing about it as of now. I have a background in the communication field, mostly radios and basic understanding of IP addresses.

Right now, I have an associate degree, secret clearance and thinking about doing my bachelors in computer science and also get some certs along the way. 

What are some pointers or tips I should look into? 

\- All help is appreciated ",1770052453.0,0,6,https://www.reddit.com/r/dataengineering/comments/1qu12iq/interest/
1qtyv51,Scrape any site (API/HTML) & get notified of any changes in JSON,"Hi everyone, I recently built tons of scraping infrastructure for monitoring sites, and I wanted an easier way to manage the pipelines. 

I ended up building meter (a tool I own) - you put in a url, describe what you want to extract, and then you have an easy way to extract that content in JSON and get notified of any changes. 

We also have a pipeline builder feature in beta that allows you to orchestrate scrapes in a flow. Example: scrape all jobs in a company page, take all jobs and scrape their details - meter orchestrates and re runs the pipeline on any changes and notifies you via webhook with new jobs and their details. 

Check it out! https://meter.sh  ",1770047838.0,2,0,https://www.reddit.com/r/dataengineering/comments/1qtyv51/scrape_any_site_apihtml_get_notified_of_any/
1qtxjdg,"Looking for a simple way to send large files without confusing clients, whatâ€™s everyone using?","So I needed a way to send large deliverables without hitting email limits or walking people through signups and portals. I'v tried a bunch of file transfer tools and kept running into the same friction, and too many steps, weird ads, or things that just looked sketchy.",1770044892.0,14,11,https://www.reddit.com/r/dataengineering/comments/1qtxjdg/looking_for_a_simple_way_to_send_large_files/
1qtwrba,Data Warehouse Toolkit 3rd vs 2nd edition differences,"Hello there! I just bought a used copy of Kimball's Data Warehouse Toolkit, but unfortunately the website UI was a little confusing so I did not realize I was buying the 2nd edition instead of the 3rd. It was pretty cheap so it's not worth sending it back. 

My question is, is everything in the 3rd edition pretty much rewritten from scratch to account for new technologies? Or is it more like, there are just new chapters and sections to discuss the new techniques? Just wondering if it's worth even starting to read it while I wait for the 3rd edition to arrive, or if the entire thing is so outdated I shouldn't bother at all. 

Thanks! ",1770043100.0,11,6,https://www.reddit.com/r/dataengineering/comments/1qtwrba/data_warehouse_toolkit_3rd_vs_2nd_edition/
1qtwdcf,Best companies to settle as a Senior DE,"So I have been with startups and consulting firms for last few years and really fed up with unreal expectations and highly stressful days. 

  
I am planning to switch and this time I wanted to be really careful with my choice( I know the market is tough but I can wait) 

  
So what companies do you suggest that has good work life balance that I can finally go to gym and sleep well and spend time with my family and friends. I have gathered some feedback from ex colleagues that insurance industry is the best. IS it true? Do you have any suggestions?",1770042170.0,73,47,https://www.reddit.com/r/dataengineering/comments/1qtwdcf/best_companies_to_settle_as_a_senior_de/
1qtsqng,What should be the ideal data compaction setup?,"If you are supposed to schedule a compaction job on your data how easy/intuitive would you want it to be?

1. Do you want to specify how much of the resources each table should use?
2. Do you want compaction to happen when thresholds meet or cron-based?
3. Do you later want to tune the resources based on usage (expected vs actual) or just want to set it and forget it?",1770032338.0,3,0,https://www.reddit.com/r/dataengineering/comments/1qtsqng/what_should_be_the_ideal_data_compaction_setup/
1qtnonk,Iterate almost any data file in Python,"Allows to iterate almost any iterable data file format or database same way as csv.DictReader does in Python. Supports more that 80+ file formats and allows to apply additional data transformation and conversion.

Open source. MIT license.",1770014326.0,8,4,https://www.reddit.com/r/dataengineering/comments/1qtnonk/iterate_almost_any_data_file_in_python/
1qtfbhx,Architecting a realtor analytics system,"Junior Engineer here. I have been tasked with designing a scalable and flexible analytics architecture that shows you realtors performance in different US markets. 

What we need: 

Show aggregated realtor performance (volume sold based on listing/buying side) on different filters like at the state level, county level, zip level, MLS level) and a user can set a date range. This performance needs to be further aggregated together at office level so we can bring out stuff top agents per office.

I currently use 3 datasets (listings, tax/assessor, office data) to create one giant fact table that contains agent performance in the areas I mentioned above aggregated on the year and the month. So I can query the table to find out how a certain agent performed in a certain zip code compared to some other agent, or I can see an agents most sold areas, average listing price etc. 


The Challenge

1) Right now the main issue we are facing is the speed.

The table I made is sitting inside snowflake, and the frontend uses a aws lambda to fetch the data from snowflake. This adds some latency (authentication alone takes 3 seconds) and warehouse startup time + query execution time) and the whole package comes to around 8 seconds. We would ideally want to do this under 2 seconds. 

We had a senior data engineer who designed a sparse GSI schema for dynamodb where the agent metrics were dimensionalized such that i can query a specific GSI to see how an  agent ranks on a leader board for a specific zip code/state/county etc. This architecture presents the problem that we can only compare agents based on 1 dimension. (We trade flexibility over speed). However, we want to be able to filter on multiple filters.

I have been trying to design a similar leader board schema but to be used on OpenSearch, but there's a 2nd problem that I also want to keep in mind.

2) Adding additional datasets in the future 

Right now we are using 3 datasets, but in the future we will likely need to connect more data (like mortgage) with this. As such, I want to design an opensearch schema that allows me to aggregate performance metrics, as well as leave space to add more datasets and their metrics in the future. 

What I am looking for:

I would like to have tips from experienced Data Engineers here who have worked on similar projects like this. I would love any tips on pitfalls/things to avoid and what to think about when designing this schema. 

I know i am making a ridiculous ask, but I am feeling a bit stuck here.",1769990684.0,1,4,https://www.reddit.com/r/dataengineering/comments/1qtfbhx/architecting_a_realtor_analytics_system/
1qteway,"[Project] I built a CLI to find ""Zombie Vectors"" in Pinecone/Weaviate (and estimate how much RAM you're wasting)","Hey everyone,

Iâ€™m an ex-AWS S3 engineer. In my previous life, we obsessed over ""Lifecycle Policies"" because storing petabytes of data is expensive. If data wasnâ€™t touched in 30 days, we moved it to cold storage.

I noticed a weird pattern in the AI space recently: **We are treating Vector Databases like cold storage.**

We shove 100% of our embeddings into expensive Hot RAM (Pinecone, Milvus, Weaviate), even though for many use cases (like Chat History or Seasonal Catalog Search), 90% of that data is rarely queried after a month. Itâ€™s like keeping your tax returns from 1990 in your wallet instead of a filing cabinet.

I wanted to see exactly how much money was being wasted, so I wrote a simple open-source CLI tool to audit this.

**What it does:**

1. **Connects** to your index (Pinecone currently supported).
2. **Probes** random sectors of your vector space to sample metadata.
3. **Analyzes** the `created_at` or timestamp fields.
4. **Reports** your ""Stale Rate"" (e.g., ""65% of your vectors haven't been queried in >30 days"") and calculates potential savings if you moved them to S3/Disk.

**The ""Trust"" Part:** I know giving API keys to random tools is a bad idea.

* This script runs **100% locally** on your machine.
* Your keys never leave your terminal.
* You can audit the code yourself (itâ€™s just Python).

**Why I built this:** Iâ€™m working on a larger library to automate the ""S3 Offloading"" process, but first I wanted to prove that the problem actually exists.

Iâ€™d love for you to run it and let me know: **Does your stale rate match what you expected?** Iâ€™m seeing \~90% staleness for Chat Apps and \~15% for Knowledge Bases.

**Repo here:** [https://github.com/billycph/VectorDBCostSavingInspector](https://github.com/billycph/VectorDBCostSavingInspector)

Feedback welcome!",1769989626.0,5,0,https://www.reddit.com/r/dataengineering/comments/1qteway/project_i_built_a_cli_to_find_zombie_vectors_in/
1qtdnvi,What are the scenarios where we DON'T need to build a dimensional model?,"As title.  **When** ***shouldn't*** **we go through the efforts of building a dimensional model?**  To me, it's a bit of a grey area.  But how do I pick out the black and white?  When I'm giving feedback, questioning and making suggestions about the aspects of the design as developed - and it's not a dim model - I'll tend to default to ""should be a dim model"".  I'm concerned that's a rigid and incorrect stance.  I'm vaguely aware that a dim model is not always the way to go, but *when is that?*

Background:  I have 7 years in DE, 3 years before that in SW.  I've learned a bunch, but often fall back on what are considered best practices if I lack the depth or breadth of experience.  When, and when *not* to use a dim model is one of these areas.

Most our use cases are A) Reports in Power BI.  Occasionally, B) Returning specific, flat information.  For B, it could *still* come from a dim model.  This leads me to think that a dim model is a go-to, with doing otherwise is the exception.

Problem of the day: There's a repeating theme at work.  Models put together by a colleague are never strict dims/facts.  It's *relational*, so there is a logical star, but it's not as clear-cut as a few facts and their dimensions.  Measures and attributes remain mixed.  They'll often say that the data and/or model is small: there is a handful of tables; less than hundreds of millions of rows.

I get the balance between *ship now* and *do it properly, methodically, follow a pattern*.  But, whether there are 5 tables or 50, I am stuck on the thought that your 5-table data source *still* has some business process to be considered.  There are *still* measures and attributes to break out.

EDIT: Some rephrasing.  I was coming across as ""back up my opinion"".  I'm actually looking for the opposite.",1769986629.0,27,34,https://www.reddit.com/r/dataengineering/comments/1qtdnvi/what_are_the_scenarios_where_we_dont_need_to/
1qtc0ag,Recommended ETL pattern for reference data?,"Hi all,

I have inherited a pipeline where some of the inputs are reference data that are uploaded by analysts via CSV files.

The current ingestion design for these is quite inflexible. The reference data is tied to a year dimension, but the way things have been set up is that the analyst needs to include the year which the data is for in the filename. So, you need one CSV for every year that there is data for.

e.g. we have two CSV files, the first is *some\_data\_2024.csv* which would have contents:

|id|foo|
|:-|:-|
|1|423|
|2|1|

the second is *some\_data\_2021.csv* which would have contents:

|id|foo|
|:-|:-|
|1|13|
|2|10|

These would then appear in the final silver table as 4 rows:

|year|id|foo|
|:-|:-|:-|
|2024|1|423|
|2024|2|1|
|2021|1|13|
|2021|2|10|

Which means that to upload many years' worth of data, you have to create and upload many CSV files all named after the year they belong to. I find this approach pretty convoluted. There is also no way to delete a bad record unless you replace it. (It can't be removed entirely).

Now the pattern I want to go to is just allow the analysts to upload a singular CSV file with a year column. Whatever is in there will be what is in the final downstream table. In other words, the third table above will be what they upload. If they want to remove a record just reupload that singular CSV without that record. I figure this is much simpler. I will have a staging table that captures the entire upload history and then the final silver table just selecting all records from the latest upload.

What do we think? Please let me know if I should add more details.",1769982756.0,6,7,https://www.reddit.com/r/dataengineering/comments/1qtc0ag/recommended_etl_pattern_for_reference_data/
1qt95p7,First time data engineer contract- how do I successfully do a knowledge transfer quickly with a difficult client?,"This is my first data engineering role after graduating and I'm expected to do a knowledge transfer starting on day one. The current engineer has only a week and a half left at the company and I observed some friction between him and his boss in our meeting.  For reference, he has no formal education in anything technical and was before this a police officer for a decade. He admitted himself that there isn't really any documentation for his pipelines and systems, ""it's easy to figure out when you look at the code."" From what my boss has told me about this client their current pipeline is messy, not intuitive, and that there's no common gold layer that all teams are looking at (one of the company's teams makes their reports using the raw data). 

I'm concerned that he isn't going to make this very easy on me, and I've never had a professional industry role before, but jobs are hard to find right now and I need the experience. What steps should I take to make sure that I fully understand what's going on before this guy leaves the company? ",1769976376.0,45,23,https://www.reddit.com/r/dataengineering/comments/1qt95p7/first_time_data_engineer_contract_how_do_i/
1qt4isy,"Agentic AI, Gen AI","I got call from birlasoft recruiter last week. He discussed a DE role and skills: Google cloud data stack, python, scala, spark, kafka, iceberg lakehouse etc matching my experience. Said my L1 would be arranged in a couple of days. Next day called me asking if I have worked on any Agentic AI project and have experience in (un)supervised, reinforcement learning, NLP. They were looking for data engineer + data scientist in one person. Is this the new normal these days. Expecting data engineers to do core data science stuff !!!",1769966621.0,12,7,https://www.reddit.com/r/dataengineering/comments/1qt4isy/agentic_ai_gen_ai/
1qt3vtb,Monthly General Discussion - Feb 2026,"This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.

Examples:

* What are you working on this month?
* What was something you accomplished?
* What was something you learned recently?
* What is something frustrating you currently?

As always, sub rules apply. Please be respectful and stay curious.

**Community Links:**

* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)
* [Data Engineering Events](https://dataengineering.wiki/Community/Events)
* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)
* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)",1769965254.0,11,3,https://www.reddit.com/r/dataengineering/comments/1qt3vtb/monthly_general_discussion_feb_2026/
1qt1543,Handling spark failures,"Recently I've been working on deploying some spark jobs in Amazon eks, the thing is sometimes they just fail intermittently for 4/5 runs continuously due to some issues like executors getting killed/ shuffle partitions lost.. ( I can go on and list the issues but you got the idea ). Right now I'm just either increasing resources or modifying some of the spark properties like increasing shuffle partitions and stuff. 

I've gone through couple of videos/articles, most of them fit well in theory for small scale processing but don't think they would be able to handle heavy shuffle involved ingestions.

Are there any resources where I can learn how to handle such failures with proper reasoning on how/why do we add some specific spark properties?",1769959127.0,3,2,https://www.reddit.com/r/dataengineering/comments/1qt1543/handling_spark_failures/
1qsuov1,Getting a part time/contracting job along with my full time role that is based in the UK.,"Hi guys,

Thought I would reach out here to see where fellow data engineers tend to get part-time / consulting work. As the working week progresses I tend to have more time on my hands and would like to work & develop things that are bit more exciting (My work is basically ETL'ing data from source to sink using the medallion architecture - nothing fancy).

Any tips would be greatly appreciated. :)",1769940544.0,9,8,https://www.reddit.com/r/dataengineering/comments/1qsuov1/getting_a_part_timecontracting_job_along_with_my/
1qsqhzm,How to become senior data engineer,I am trying to develop my skills be become senior data engineer and I find myself under confident during interviews .How do you analyze a candidate who can be fit as senior position?,1769926096.0,60,18,https://www.reddit.com/r/dataengineering/comments/1qsqhzm/how_to_become_senior_data_engineer/
1qsm2gq,How to learn OOP in DE?,"Iâ€™m trying to learn OOP in the context of DE, while I do a lot of work DE work, I havenâ€™t found a reason why to use classes which is probably due lack of knowledge. So I was wondering are there sources that you recommend that could help fill in the gaps on OOP in DE?",1769913170.0,68,77,https://www.reddit.com/r/dataengineering/comments/1qsm2gq/how_to_learn_oop_in_de/
1qskarh,Ready to switch jobs but not sure where to start,"I'm coming up on four years at my current company and between a worsening WLB and lack of growth opportunities I'm really eager to land a job elsewhere. Trouble is I don't feel ready to immediately launch myself back out there. We're a .NET shop and the team I'm on mainly focuses on data migrations for new acquisitions to our SAAS offering. Day to day we mainly use C# and SQL with a little Powershell and Azure thrown in there. But it doesn't honestly feel like we use any of these that deeply most of the time for what we need to accomplish and my knowledge of Azure in particular isn't that extensive. Although we're called ""data engineers"" within the context of our company the work we do seems shallow compared to what I see other data engineers work on. To be honest I don't feel like a strong candidate at present and that's something I'd like to change. Mainly I'm interested in learning about any resources or tools that have helped anyone reading this also going through the job search. It feels like expectations keep ballooning with regard to what's expected in tech interviews and I'm concerned I'm falling behind.",1769908437.0,12,3,https://www.reddit.com/r/dataengineering/comments/1qskarh/ready_to_switch_jobs_but_not_sure_where_to_start/
1qsg6tc,Puzzle game to learn Apache Spark & Distributed Computing concepts,"https://i.redd.it/fsa3dtvkfrgg1.gif

Hello all!

I'm new in this subreddit! I'm a Data Engineer with +3 years of experience in the field.

As shown in the attached image, I'm making an ETL simulator in JavaScript, that simulates the data flow in a pipeline.

Recently I came across a Linkedin post of a guy showcasing this project : [https://github.com/pshenok/server-survival](https://github.com/pshenok/server-survival)

He made a little tower defense game that interactively teaches Cloud Architecture basics.

It was interesting to see the engagement of the DevOps community with the project. Many have starred and contributed to the Github repo.

I'm thinking about building something silimar for Data Engineers, given that I have some background in Game Dev and UI/UX too. I still need your opinion though, to see whether or not it is going to be that useful, especially that it will take some effort to come up with something polished, and AI can't help much with that (I'm coding all of the logic manually).

The idea is that I want to make it easy to learn Apache Spark internals and distributed computing principles. I noticed that many Data Engineers (at least here in France), including seniors/experts, say they know how to use Apache Spark, yet they don't deeply understand what's happening under the hood.

Through this game, I'll try to concretize the abstract concepts and show how they impact the execution performance, such as : transformations/actions, wide/narrow transformations, shuffles, repartition/coalesce, partitions skew, spills, node failures, predicate pushdown, ...etc

You'll be able to build pipelines by stacking transformer blocks. The challenge will be to produce a given dataframe using the provided data sources, while avoiding performance killers and node failures. In the animated image above, the sample pipeline is equivalent to the following Spark line : `new_df = source_df.filter($""shape"" === ""star"").withColumn(""color"", lit(""orange""))`

I represented the rows with shapes. The dataframe schema will remain static (shape, color, label) and the rendering of each shape reflects the content of the row it represents. Dataframe here is a set of shapes.

I'm still hesitant about this representation. Do you think it is intuitive and easy to understand ? I can always revert to the standard tabular visualisation of rows with dynamic schemas, but I guess it won't look user friendly when there are a lot of rows in action.

The next step will be to add logical multi-node clusters in order to simulate the distributed computing. The heaviest task that I estimated would be the implementation of the data shuffling.

I'll share the source code within the next few days, the project needs some final cleanups.

In the meanwhile, feel free to comment or share anything helpful :)",1769898097.0,64,3,https://www.reddit.com/r/dataengineering/comments/1qsg6tc/puzzle_game_to_learn_apache_spark_distributed/
1qscp60,What is your experience like with Marketing teams?,"Iâ€™ve mostly been on the infrastructure and pipeline side, supporting Product. Some of my recent roles have all included supporting Marketing teams as well and I have to say it hasnâ€™t been a positive experience. 

One or two of the teams have been okay, but in general it seems like:
1. Data gets blamed for poor Marketing performance, a lot more than Product. â€œWe donâ€™t have the data to do our jobâ€
1. Along those lines, everything is a fire, e.g. feature is released in the evening and the data/reports need to be ready the next morning. 


What has your experience been like? Is this just bad luck on my part?",1769889909.0,14,14,https://www.reddit.com/r/dataengineering/comments/1qscp60/what_is_your_experience_like_with_marketing_teams/
1qsccxn,Read S3 data using Polars,"One of our application generated 1000 CSV files that totals to 102GB. These files are stored in an S3 bucket. 
I wanted to do some data validation on these files using Polars but it's taking lot of time to read the data and display it in my local laptop. I tried using scan_csv() but still it just kept on trying to scan and display the data for 15 mins but no result.
Since these CSV files do not have a header I tried to pass the headers using new_columns but that didn't work either.
Is there any way to work with these huge file size without using tools like Spark Cluster or Athena.",1769889137.0,17,24,https://www.reddit.com/r/dataengineering/comments/1qsccxn/read_s3_data_using_polars/
1qsc10c,How to securely use prod-like data for non-prod scenarios and use cases?,"Hi guys, how are you people generating test data which is as close as possible to prod data, without data breach of PII or loosing relationships or data integrity.

Any manual scripts or tools or masking generators? Any SaaS available for this? 

All suggestions are helpful.

Thanks",1769888384.0,1,2,https://www.reddit.com/r/dataengineering/comments/1qsc10c/how_to_securely_use_prodlike_data_for_nonprod/
1qsbaun,Quorum-free replicated state machine atop S3,,1769886755.0,4,0,https://www.reddit.com/r/dataengineering/comments/1qsbaun/quorumfree_replicated_state_machine_atop_s3/
1qsaom3,Looking for advice as a junior DE,"Hello everyone!
I just finished my CS engineering degree and got my first job as a junior DE. The project I am working on is using Palantir foundry and I have two questions :

1. I feel like foundry is oversimplified to the point it becomes restrictive on what you can and connot do. Also, most of the time all you have to do is click on a button and it feels like monkey work to me. I have this feeling that I am not even learning the basics of DE from this job. Do we all agree that foundry is not the good way to start a DE career ?

2. For now the only thing I enjoy about my work is writing pyspark transformations. I would like to take some courses in order to have a good understanding of how spark really works. I am also planning to take a AWS certification this year. Which courses/certifications (I am working for a consulting firm) would you suggest me as a junior ?

Would appreciate any career advice from people with some experience in DE.

Thanks :)
",1769885396.0,6,5,https://www.reddit.com/r/dataengineering/comments/1qsaom3/looking_for_advice_as_a_junior_de/
1qsaey4,DBT Analytics Engineering Certification: My Journey and Top Prep Resources,"I am very exited to share that I recently passed the dbt Analytics Engineering Certification. With about 6 to 7 months of hands on experience in dbt, I focused on the official study guide, but also emphasized real world practice.

**Prep Highlights:**

* I drilled down into key topics like incremental models, materializations, and model governance.

* Practiced debugging, using ref() macros, and managing data pipelines.

* For exam readiness, I relied on quality practice questions from p2pcerts, which helped solidify my understanding for exam prep.

The exam was challenging but fair, and hands-on experience plus targeted practice made a big difference.

I am happy to assist with any queries you might have. Best wishes as you embark on your prep!
",1769884797.0,18,2,https://www.reddit.com/r/dataengineering/comments/1qsaey4/dbt_analytics_engineering_certification_my/
1qs44qq,Entry Level Questions,"Hello all!

I had posted on here about a month ago talking about healthcare data engineering, and since then Iâ€™ve learned a ton of awesome stuff about data engineering, mainly the cloud services interest me the most (AWS). However, the jobs search for data engineering or anyway to get my foot in the door is justâ€¦ demoralizing. I have a BS in biomedical engineering and an in progress masters in CS and Iâ€™m really trying to get into tech because itâ€™s what I enjoy working with, but I have a few questions to people that have been in my shoes before:

Where are you looking for jobs? Indeed and LinkedIn seem to have jobs that get hundreds of apps it seems like. LinkedIn I just donâ€™t really understand I guess, how do I find places that will actually hire someone junior level that has skills (projects, great self-learner, super driven)? When I do, what are the best approaches for networking? The job search is just kinda melting my brain and there never really is a light at the end of the tunnel until you get an offer. Any words of advice or just general pointers would be greatly appreciated as this makes me feel super incapable of my skills I know I have. ",1769870615.0,5,7,https://www.reddit.com/r/dataengineering/comments/1qs44qq/entry_level_questions/
1qs3tbl,"Big brothers, I summon your wisdom. Need a reality check as an entry level engineer!","Hi big brothers, I am an entry level ETL developer working with Snowflake, Python, IDMC, Fabric (although I call myself data enginer on linkedin, let me know if this is ok). So, my background has been in data science and I have explored a lot, learned a lot, worked on a lot of personal project including gen ai. I am good with Python coding (solved 300+ leetcode), SQL and great intuition such that I can learn any tool thrown at me. So, I got hired at a SBC and they got me into ETL development. I can see based on the tasks I have got so far and things people around me are doing, I wont be doing anything other than migrating etl pipelines from a legacy tool (like SAS DI, denodo, etc.) to modern tech like Snowflake, IDMC, Fabric.   
  
Is this okay to be considered for an entry level data engineer? If yes, then should I try to leave in 1 year of exp or is it safe to stay for 2 years and is the market ready to hire someone like me? Also, how do people upgrade themselves in this domain? Also, the tools are the backbone of this domain, how do poeple learn them even though they have not worked in any project around them in the job, I mean based on my exp, it is little difficult to learn them without actually working on them and way easier to forget? Do people usually fake the tool exp and then learn on the job? Also, when I have 1 year of exp, what are the expecations from me? Also, should I start working on my system design knowledge? My aim is to leave etl and get a proper data engineering job within next 12 months. Pls try to answer and also give any advice you would give to your younger etl dev brother.",1769869828.0,18,27,https://www.reddit.com/r/dataengineering/comments/1qs3tbl/big_brothers_i_summon_your_wisdom_need_a_reality/
1qrzk69,Any major drawbacks of using self-hosted Airbyte?,"I plan on self-hosting Airbyte to run 100s of pipelines.

So far, I have installed it using abctl (kind setup) on a remote machine and have tested several connectors I need (postgres, hubspot, google sheets, s3 etc). Everything seems to be working fine.

And I love the fact that there is an API to setup sources, destinations and connections.

The only issue I see right now is it's slow. 

For instance, the HubSpot source connector we had implemented ourselves is at least 5x faster than Airbyte at sourcing. Though it matters only during the first sync - incremental syncs are quick enough.

  
Anything I should be aware of before I put this in production and scale it to all our pipelines? Please share if you have experience hosting Airbyte.",1769857517.0,12,28,https://www.reddit.com/r/dataengineering/comments/1qrzk69/any_major_drawbacks_of_using_selfhosted_airbyte/
1qrwves,Create BigQuery Link for a GA4 property using API,"Struggling to get this working (auth scopes issue), wondering if anyone experienced this issue before?

I'm trying to create the bigquery link in a ga4 property using the following API via a shell command:Â [https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1alpha/properties.bigQueryLinks/create](https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1alpha/properties.bigQueryLinks/create)

Note:

* Client has given my service account Editor access to their GA4 property.
* I've enabled the Google Analytics Admin API in the GCP project.
* SA has access to write to BigQuery.

My attempt:

    # Login to gcloud
    gcloud auth application-default login \
      --impersonate-service-account=$TF_SA_EMAIL \
      --scopes=https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/analytics.edit
    
    # Make API request
    curl -X POST \
      -H ""Authorization: Bearer $(gcloud auth application-default print-access-token)"" \
      -H ""Content-Type: application/json"" \
      ""https://analyticsadmin.googleapis.com/v1alpha/properties/${GA4_PROPERTY_ID}/bigQueryLinks"" \
      -d '{
            ""project"": ""projects/'""${GCP_PROJECT_ID}""'"",
            ""datasetLocation"": ""'""${GCP_REGION}""'"",
            ""dailyExportEnabled"": true,
            ""streamingExportEnabled"": false
          }'

Response:

    {
      ""error"": {
        ""code"": 403,
        ""message"": ""Request had insufficient authentication scopes."",
        ""status"": ""PERMISSION_DENIED"",
        ""details"": [
          {
            ""@type"": ""type.googleapis.com/google.rpc.ErrorInfo"",
            ""reason"": ""ACCESS_TOKEN_SCOPE_INSUFFICIENT"",
            ""domain"": ""googleapis.com"",
            ""metadata"": {
              ""method"": ""google.analytics.admin.v1alpha.AnalyticsAdminService.CreateBigQueryLink"",
              ""service"": ""analyticsadmin.googleapis.com""
            }
          }
        ]
      }
    }",1769847862.0,2,0,https://www.reddit.com/r/dataengineering/comments/1qrwves/create_bigquery_link_for_a_ga4_property_using_api/
1qrqz6i,Got a chance to change title to Data Engineer what should I expect?,I'm in US and company is in a DoD contracting field. I am currently M365 sysadmin and got a chance to move laterally due to my position being eliminated. One of the available position is Data Engineering. My goal is to become a cloud architect later in my career so I think this will help a lot. I would be expected to design and set up a data lake and data warehouse but what else should I be expecting? Is this even a good idea for me? I need some guidance guys :( ,1769829309.0,2,23,https://www.reddit.com/r/dataengineering/comments/1qrqz6i/got_a_chance_to_change_title_to_data_engineer/
1qrmmjl,Migrating to data,"Hello, I've been working in the tax/fiscal area for 9 years, with tax entries and reconciliations, which has given me a high level of business understanding in the field.

However, it's something I don't enjoy doing. I have a degree in Financial Management and decided to migrate to the data area after a few years performing tax loading tasks, which brought me closer to consultants in the field.

From there, I decided to do a postgraduate degree in Data Analysis and I'm taking some courses, such as SQL, BI...

As with any transition, there are risks and fears. I've been researching a lot and I see dissatisfaction among people in the area because AI is stealing their spaces.

Please tell me honestly, how is the area doing for new hires?

My current annual salary as a senior tax analyst is around 70k.",1769817854.0,8,15,https://www.reddit.com/r/dataengineering/comments/1qrmmjl/migrating_to_data/
1qrm6z3,Modeling Financial Data,"I'm curious for input. I've over the last couple of years developed some financial reports in all that produce trial balances and gl transaction reports. When it comes to bringing this in to BI, I'm not sure if I should connect to the flat reports, or build out a dimensional model for the financials. Thoughts?",1769816811.0,10,12,https://www.reddit.com/r/dataengineering/comments/1qrm6z3/modeling_financial_data/
1qrlhoj,Shopify coding assessment - recommendations for how to get extremely fluent in SQL,"I have an upcoming coding assessment for a data engineer position at Shopify. I've used SQL to query data and create pipelines, and to build the tables and databases themselves. I know the basics (WHERE clauses, JOINs, etc) but what else should I be learning/practicing.

I haven't built a data pipeline with just sql before, it's mostly python.",1769815101.0,71,28,https://www.reddit.com/r/dataengineering/comments/1qrlhoj/shopify_coding_assessment_recommendations_for_how/
1qrk9lj,SAP Hana sync to Databricks,"Hey everyone,

Weâ€™ve got a homegrown framework syncing SAP HANA tables to Databricks, then doing ETL to build gold tables. The sync takes hours and compute costs are getting high.

From what I can tell, weâ€™re basically using Databricks as expensive compute to recreate gold tables that already exist in HANA. Iâ€™m wondering if thereâ€™s a better approach, maybe CDC to only pull deltas? Or a different connection method besides Databricks secrets? Honestly questioning if we even need Databricks here if weâ€™re just mirroring HANA tables.

Trying to figure out if this is architectural debt or if Iâ€™m missing something. Anyone dealt with similar HANA Databricks pipelines?

Thanks",1769812211.0,3,19,https://www.reddit.com/r/dataengineering/comments/1qrk9lj/sap_hana_sync_to_databricks/
1qrhvt7,Managing embedding migrations - dimension mapping approaches,"Data engineering question for those working with vector embeddings at scale.

The problem:

You have embeddings in production:  
â€¢ Millions of vectors from text-embedding-ada-002 (1536 dim)  
â€¢ Stored in your vector DB  
â€¢ Powering search, RAG, recommendations

Then you need to:  
â€¢ Test a new embedding model with different dimensions  
â€¢ Migrate to a model with better performance  
â€¢ Compare quality across providers

Current options:

1. Re-embed everything - expensive, slow, risky
2. Parallel indexes - 2x storage, sync complexity
3. Never migrate - stuck with original choice

What I built:

An embedding portability layer with actual dimension mapping algorithms:  
â€¢ PCA - principal component analysis for reduction  
â€¢ SVD - singular value decomposition for optimal mapping  
â€¢ Linear projection - for learned transformations  
â€¢ Padding/expansion - for dimension increase

Validation metrics:  
â€¢ Information preservation calculation (variance retained)  
â€¢ Similarity ranking preservation checks  
â€¢ Compression ratio tracking

Data engineering considerations:  
â€¢ Batch processing support  
â€¢ Quality scoring before committing to migration  
â€¢ Rollback capability via checkpoint system

Questions:

1. How do you handle embedding model upgrades currently?
2. What's your re-embedding strategy? Full rebuild vs incremental?
3. Would dimension mapping with quality guarantees be useful?

Looking for data engineers managing embeddings at scale. DM to discuss.",1769806839.0,2,1,https://www.reddit.com/r/dataengineering/comments/1qrhvt7/managing_embedding_migrations_dimension_mapping/
1qrh4tp,State of the Apache Iceberg Ecosystem Survey 2026,"Fill out the survey, report will probably released end of feb or early march detailing the results.",1769805153.0,3,0,https://www.reddit.com/r/dataengineering/comments/1qrh4tp/state_of_the_apache_iceberg_ecosystem_survey_2026/
1qre1py,Is copartitioning necessary in a Kafka stream application with non stateful operations?,"Co partitioning is required when joins are initiated

However if pipeline has joins at the phase (start or mid or end)

And other phases have stateless operations like merge or branch etc

Do we still need Co partitioning for all topics in pipeline? Or it can be only done for join candidates and other topics can be with different number of partitions?

Need some guidance on this",1769798512.0,3,0,https://www.reddit.com/r/dataengineering/comments/1qre1py/is_copartitioning_necessary_in_a_kafka_stream/
1qr6cml,"Certified Data Management
Professionals","Hi everyone, has anyone taken the CDMP certification exam? Is there a simulator for the exam?",1769781916.0,6,1,https://www.reddit.com/r/dataengineering/comments/1qr6cml/certified_data_management_professionals/
1qr4dfn,Fit check for my IoT data ingestion plan,"Hi everyone! Long-time listener, first-time caller. I have an opportunity to offer some design options to a firm for ingesting data from an IoT device network. The devices (which are owned by the firm's customers) produce a relatively modest number of records: Let's say a few hundred devices producing a few thousand records each every day. The firm wants 1) the raw data accessible to their customers, 2) an analytics layer, and 3) a dashboard where customers can view some basic analytics about their devices and the records. The data does not need to be real-time, probably we could get away with refreshing it once a day.

My first thought (partly because I'm familiar with it) is to ingest the records into a BigQuery table as a data lake. From there, I can run some basic joins and whatnot to verify, sort, and present the data for analysis, or even do more intensive modeling or whatever they decide they need later. Then, I can connect the BigQuery analytics tables to Looker Studio for a basic dashboard that can be shared easily. Customers can also query/download their data directly.

That's the basics. But I'm also thinking I might need some kind of queue in front of BigQuery (Pub/Sub?) to ensure nothing gets dropped. Does that make sense, or do I not have to worry about it with BigQuery? Lastly, just kind of conceptually, I'm wondering how IoT typically works with POSTing data to cloud storage. Do you create a GCP service account for each device? Is there an API key on each physical device that it uses to make the requests? What's best practice? Anything really, really stupid that people often do here that I should be sure to avoid?

Thanks for your help and anything you want to comment on, I'm sure I'm still missing a lot. This is a fun project, I'm really hoping I can cover all my bases!",1769776842.0,4,3,https://www.reddit.com/r/dataengineering/comments/1qr4dfn/fit_check_for_my_iot_data_ingestion_plan/
1qr1ah1,Alternate careers from IT/Data ??,"Switched to data field \~2yrs back ( had to do a masters degree) while I enjoy it I feel the time I spent in the industry isn't sufficient. There is so much more I could do would have wanted to do. Heck I have just been in one domain also.

My company lately have been asking us to prepare datasets to feed to agentic AI. While it answers the basics right it still fails at complex things which require deep domain and business knowledge.

There are several prompts injected and several key business indicators defined so the Agent performs good ( honestly if we add several more layers of prompt and chain few more agents it would get to answer come hard questions involving joining 6+ tables as well)

Since it already answers some easy to medium questions based on your prompts the headcounts are just slashing. No I am good at what I do but I won't self proclaim as top 1%.  
I have very strong skillset to figure things out if I don't know about it. A coworker of mine has been the company for 6 years and didn't even realize how to solve things which I could do it ( even though I had no idea in the first place as well) . I just guess this person has become way more comfy and isn't aware how wild things are outside.

Is there anyone actively considering goose farming or something else out of this AI field ?

There is joy in browsing the internet without prompts and scrolling across website. There is joy in navigating UIs, drop downs and looking at the love they have put in. There is joy in minimizing the annoying chat pop that open ups at the website.

And last thing I want to read is AI slop books by my fav authors.

There is reason why chess is still played by humans and journalist still put heart out in their writing. There will also be a reason human DE/DS/DA/AE would be present in future but maybe a lot less.

What's the motivation to still pursue this field ? I love anything related to data to be honest and for me that is the only one. I love eat and breathe data even if I am jobless now because of AI first policy my company has taken.",1769766938.0,53,22,https://www.reddit.com/r/dataengineering/comments/1qr1ah1/alternate_careers_from_itdata/
1qr143d,discord channel for data engineers,"the author of Fundamentals of DE (Joe Reis) has a discord channel if anyone is interested, we discuss on it multiple interesting things about DE, AI, life...

https://discord.gg/7SENuNVG

please make sure to drop a small message in introductions when you join. and as usual no spamming

Thanks everyone!",1769766302.0,22,0,https://www.reddit.com/r/dataengineering/comments/1qr143d/discord_channel_for_data_engineers/
1qr0iz8,Building a search engine for asx announcements,"hi all I just finished a write up / post mortem for a data engineering(ish) project that I recently killed. It may be of interesting to the sub considering a core part of the challenge was building an ETL pipeline to handle complex pdfs. 

[you can read here](https://damonphilipross.github.io/2026/01/28/building-a-search-engine-for-the-asx/) there was a lot of learning and i still feel like anything to do with complex pdfs is a very interesting space to play in for data engineering.",1769764137.0,2,0,https://www.reddit.com/r/dataengineering/comments/1qr0iz8/building_a_search_engine_for_asx_announcements/
1qqzkp5,Iceberg S3 migration to databricks/snowflake,"We have petabye scale S3, parquet iceberg data lake with aws glue catalog. Has anyone migrated a similar setup to Databricks or Snowflake?

Both of them support the Iceberg format. Do they manage Iceberg maintenance tasks automatically? Do they provide any caching layer or hot zone for external Iceberg tables?",1769760598.0,7,0,https://www.reddit.com/r/dataengineering/comments/1qqzkp5/iceberg_s3_migration_to_databrickssnowflake/
1qqyv6f,How do you keep your sanity when building pipelines with incremental strategy + timezones?,"I keep running into the same conflict between my incremental strategy logic and the pipeline schedule, and then on top off that timezone make it worse. Here's an example from one of our pipelines:

\- a job runs hourly in UTC

\- logic is ""process the next full day of data"" (because predictions are for the next 24 hours)

\- the run at 03:10 UTC means different day boundaries for clients in different timezones

Delayed ML inference events complicate cutoffs, and daily backfills overlap with hourly runs. Also for our specific use case, ML inference is based on client timezones, so inference usually runs between 06:00 and 09:00 local time, but each energy market has regulatory windows that change when they need data by and it is best for us to run the inference closest to the deadline so that the lag is minimized.

Interested in hearing about other data engineers' battle wounds when working with incremental/schedule/timezone conflicts.",1769758044.0,8,6,https://www.reddit.com/r/dataengineering/comments/1qqyv6f/how_do_you_keep_your_sanity_when_building/
1qqy67i,We unified 5 query engines under one catalog and holy shit it actually worked,"So we had Spark, Trino, Flink, Presto, and Hive all hitting different catalogs and it was a complete shitshow. Schema changes needed updates in 5 different places. Credential rotation was a nightmare. Onboarding new devs took forever because they had to learn each engine's catalog quirks.

Tried a few options. Unity Catalog would lock us into Databricks. Building our own would take 6+ months. Ended up going with Apache Gravitino since it just became an Apache TLP and the architecture made sense - basically all the engines talk to Gravitino which federates everything underneath.

Migration took about 6 weeks. Started with Spark since that was safest, then rolled out to the others. Pretty smooth honestly.

The results have been kind of crazy. New datasets now take 30 mins to add instead of 4~6 hours. Schema changes went from 2~3 hours down to 15 mins. Catalog config incidents dropped from 3~4 per month to maybe 1 per quarter. Dev onboarding for the catalog stuff went from a week to 1~2 days.

Unexpected win: Gravitino treats Kafka topics as metadata objects so our Flink jobs can discover schemas through the same API they use for tables. That was huge for our streaming pipelines. Also made our multi-cloud setup way easier since we have data in both AWS and GCP.

Not gonna sugarcoat the downsides though. You gotta self-host another service (or pay for managed). The UI is pretty basic so we mostly use the API. Community is smaller than Databricks/Snowflake. Lineage tracking isn't as good as commercial tools yet.

But if you're running multiple engines and catalog sprawl is killing you, it's worth looking at. We went from spending hours on catalog config to basically forgetting it exists. If you're all-in on one vendor it's probably overkill.

Anyone else dealing with this? How are you managing catalogs across multiple engines?

---

**Disclosure:** I work with Datastrato (commercial support for Gravitino). Happy to answer questions about our setup.

Apache Gravitino: https://github.com/apache/gravitino
",1769755610.0,3,6,https://www.reddit.com/r/dataengineering/comments/1qqy67i/we_unified_5_query_engines_under_one_catalog_and/
1qqx556,Why not a open transformation standard,"Open semantic interchange recently released it's initial version of specifications. Tools like dbt metrics flow will leverage it to build semantic layer.

Looking at the specification, why not have a open transformation specification for ETL/ELT which can dynamically generate code based on mcp for tools or AI for code generation that can then transorm it to multiple sql dialects or calling spark python dsl calls 

Each piece of transformation using various dialects can then be validated by something similar to  dbt unit tests 

Building infra now is abstracted in eks, same is happening in semantic space, same should happen for data transformation ",1769752221.0,3,8,https://www.reddit.com/r/dataengineering/comments/1qqx556/why_not_a_open_transformation_standard/
1qqvw49,Data Engineer with Analytics Background (International Student) â€“ What Should I Focus on in 2026?,"Hi everyone,  
I recently graduated with a Masterâ€™s in Data Analytics in the US, and Iâ€™m trying to transition into a Data Engineering role. My bachelorâ€™s was in Mechanical Engineering, so I donâ€™t have a pure CS background.

Right now, Iâ€™m on OPT (STEM OPT coming later), and Iâ€™m honestly feeling a bit overwhelmed about how competitive the market is. I know basic Python and SQL, and Iâ€™m currently learning:

* AWS (S3, Glue, Lambda, Athena)
* Data modeling (fact/dimension tables)
* dbt and Airflow
* Some PySpark 

My goal is to land an entry-level or junior Data Engineer role in the next few months.  
Iâ€™d really appreciate advice on:

1. What skills are actually critical for junior Data Engineers in 2026?
2. What projects would make my cv stand out?
3. Should I focus more on Spark/Databricks, AWS pipelines, or software engineering fundamentals (DSA, system design)?
4. Any tips for international students on finding sponsors or W-2 roles?

Be brutally honest; even if the path is hard, I want realistic guidance on what to prioritize.",1769748385.0,24,13,https://www.reddit.com/r/dataengineering/comments/1qqvw49/data_engineer_with_analytics_background/
1qqvfu4,New Graduate Imposter Syndrome,"I'm a new grad in CS and I feel like I know nothing about this Data Engineering role I applied for at this startup, but somehow I'm in the penultimate round. I got through the recruiter call and the Hackerranks which were super easy (just some Python & SQL intermediates and an advanced problem solving). Now, I'm onto the live coding round, but I feel so worried and scared that I know nothing. Don't get me wrong, my Python & SQL fundamentals are pretty solid; however, the theory really scares me. Everything I know is through practical experience through my personal projects and although I got good grades, I never really learned the material or let it soak in because I never used it (the normalization, partitions, etc.) because my projects never practically needed it. 

  
Now, I""m on the live coding round (Python + SQL) and I don't know anything about what's going to be tested since this will be my first live coding round ever (all my internships prior, I've never had to do one of these). I've been preparing like a crazy person every day, but I don't even know if I'm preparing correctly. All I'm doing is giving AI the job description and it's asking me questions which I then solve by timing myself (which to be fair, I've solved all of them only looking something up once). I'm also using SQLZoo, LC SQL questions (which I seem to be able to solve mediums fine), and I think I've completed all of Hackerranks SQL by now lol... My basic data structure (e.g., lists, hashmaps, etc.) knowledge is solid and so are the main stdlib of python (e.g., collections, json, csv, etc.). 

The worst part is, the main technology they use (Snowflake/Snowpark), I've never even touched with a 10ft pole. The recruiter mentioned that all they're looking for is a core focus on Python & SQL which I definitely have, but I mean this is a startup we're talking about, they don't have time to teach me everything. I'm a fast learner and am truly confident in being able to pick up anything quickly, I pride myself in being adaptable if nothing else, but it's not like they would care? Maybe I'm just scared shitless and just worried about nothing. 

Has anyone else felt like this? Like I really want this position to workout and land the job, because I think I'll really like it. Any advice at all? ",1769747058.0,0,9,https://www.reddit.com/r/dataengineering/comments/1qqvfu4/new_graduate_imposter_syndrome/
1qqsfmm,Got told â€˜No one uses Airflow/Hadoop in 2026â€™.,They wanted me to manage a PySpark + Databricks pipeline inside a specific cloud ecosystem (Azure/AWS). Are we finally moving away from standalone orchestration tools?,1769738790.0,139,89,https://www.reddit.com/r/dataengineering/comments/1qqsfmm/got_told_no_one_uses_airflowhadoop_in_2026/
1qqoayv,Degree Apprenticeships (UK) - student and employer perspectives?,"Iâ€™m looking for views on degree apprenticeships, particularly from people whoâ€™ve done one or whoâ€™ve been involved in hiring. This is mainly a UK thing, so feel free to skip if youâ€™re unfamiliar.

**Background:**  
Iâ€™m 13 years into my data career. I started as a data analyst, moved into a BI developer role, and last week stepped into a data engineering position (though I plan to keep some analytics work alongside it).

Iâ€™ve spent my entire career at the same UK public sector organisation. Itâ€™s a very stable environment, but I donâ€™t have a degree (just a secondary school education) and Iâ€™m starting to feel that gap more keenly. Iâ€™d like to strengthen my long-term position, fill in some theory gaps, and - now that I have a young family - set a good example by continuing my education.



So, I currently have two realistic options to consider:

**Option 1 - traditional part-time distance-learning degree (Open University):**  
One of the following...

* BSc (Hons) Computing & IT
* BSc (Hons) Computing & IT and Mathematics
* BSc (Hons) Computing & IT and Statistics

These would be around 15 hours per week and take six years to complete.

**Option 2 - degree apprenticeship (Open University, but employer/levy-funded)**

* BSc (Hons) Digital and Technology Solutions

  
This would take three years, with 20% of my paid working time allocated to study. The remaining credits come from work-based projects.

  
The apprenticeship route is obviously much faster and more manageable time-wise, but I assume the breadth and depth wonâ€™t get close to a traditional degree, especially in maths/stats. On the other hand, six years is a very long time to commit to alongside work and family.

So my questions are...

* Has anyone here done a degree apprenticeship - especially well into their career - and how did you find it?
* From an employerâ€™s perspective, how are degree apprenticeships viewed aside regular degrees?
* Is the title '*Digital and Technology Solutions'* likely to be taken seriously, or could it be off-putting?

Links to the courses for reference...

* [https://www.open.ac.uk/courses/computing-it/degrees/](https://www.open.ac.uk/courses/computing-it/degrees/)
* [https://business.open.ac.uk/apprenticeships/digital-technology-solutions-degree](https://business.open.ac.uk/apprenticeships/digital-technology-solutions-degree)

Any insights or advice appreciated, cheers!",1769728237.0,1,7,https://www.reddit.com/r/dataengineering/comments/1qqoayv/degree_apprenticeships_uk_student_and_employer/
1qqo2n7,I built a tiny CSV auditing tool!,"The goal of this tool is to scan spreadsheets and CSV files for errors, then report them back to me so I can fix them.

When a file is run through it, it can detect:

\* missing data in cells

\* invalid date formats

\* bad numeric values

\* rows that lack data

Itâ€™s intentionally non-destructive â€” it doesnâ€™t modify any data or auto-fix anything on its own. It simply reports what the errors are and where they happen to occur, allowing me to quickly correct the problems safely.

If you have a messy CSV file youâ€™d like audited, feel free to send it my way! Iâ€™m currently looking to battle-test this app with real-world files so, I can improve it further!

I'm more than happy to answer any questions about how it works as well!

I put a screenshot of the report log on my dummy csv file!

[https://drive.google.com/file/d/1g0-iRZh9JQV3ZD\_8jhyg\_lSok\_SgaUMw/view?usp=sharing](https://drive.google.com/file/d/1g0-iRZh9JQV3ZD_8jhyg_lSok_SgaUMw/view?usp=sharing)

Thanks for reading through my post! Be well!

DISCLAIMER: Please donâ€™t send any sensitive data (IE. files containing phone numbers or addresses).",1769727689.0,0,2,https://www.reddit.com/r/dataengineering/comments/1qqo2n7/i_built_a_tiny_csv_auditing_tool/
1qqndyj,Should I Pivot from Web Development to Data Engineering?,"Iâ€™m a software engineer with 3 years of experience in web development. With frontend, backend, and full stack SWE roles becoming saturated and AI improving, I want to future-proof my career. Iâ€™ve been considering a pivot to Data Engineering.

Iâ€™ve dabbled in the Data Engineering Zoomcamp and am enjoying it, but Iâ€™d love some insight and advice before fully committing. Is the Data Engineering job market any better than the SWE job market? Would you recommend the switch from SWE to Data Engineering? Will my 3 years of SWE experience allow me to break into a data engineering role?   
  
Any advice would be greatly appreciated!",1769726065.0,0,8,https://www.reddit.com/r/dataengineering/comments/1qqndyj/should_i_pivot_from_web_development_to_data/
1qqm5vk,"Being the ""data guy"", need career advice","I started in the company around 7 months ago as a Junior Data Analyst, my first job. I am one of the 3 data analysts. However, I have become the ""data guy"". Marketing needs a full ETL pipeline and insights? I do it. Product team need to analyze sales data? I do it. Need to set up PowerBI dashboards, again, it's me.

  
I feel like I do data engineering, analytics engineering, and data analytics. Is this what the industry is now? I am not complaining, I love the end-to-end nature of my job, and I am learning a lot. But for long-term career growth and salary, I don't know what to do.

  
Salary: 60k",1769723256.0,152,49,https://www.reddit.com/r/dataengineering/comments/1qqm5vk/being_the_data_guy_need_career_advice/
1qqm3io,Was asked by a client to build a Finance Cube in 1.5 months,"As title says!

4 ERPS, no infrastructure, just an existing SQL Server!

They said okay start with 1 ERP and to be able to deliver by Q1, daily refresh, drill down functionality! I said this is not possible in such a short timeframe!

They said; data is clean, only a few tables in ERP, why would you say it takes longer than that? They said Architecture is at most 2 days, and there are only a few tables! I said for a temporary solution since they are interested not to do these excel reports manually most I can offer is an automated excel report, not a full blown cube! Otherwise Im not able to commit a 1.5 months timeline without having seen myself the ERP landscape, ERP connectors, precisely what metrics/kpis are needed etc! They got mad and accused me of â€œsales pitchingâ€ for presenting the longer timeline of discovery->architecture->data modelling->medallion architecture steps!!",1769723105.0,67,32,https://www.reddit.com/r/dataengineering/comments/1qqm3io/was_asked_by_a_client_to_build_a_finance_cube_in/
1qqlucd,Oops it's a Drakanian Product,,1769722512.0,55,3,https://www.reddit.com/r/dataengineering/comments/1qqlucd/oops_its_a_drakanian_product/
1qql3wg,ADBC Arrow Driver for Databricks,,1769720870.0,2,0,https://www.reddit.com/r/dataengineering/comments/1qql3wg/adbc_arrow_driver_for_databricks/
1qqjh1h,Preparing for new job,"Hi Guys!

Currently, I have around 4 years experience as a junior data scientist in tech. As titles donâ€™t mean a lot I will list my experiences wrt programming languages and tools:

\- Python: much experience (pandas, numpy, simpy, pytorch, gurobi/pyomo)

Query languages

\- SQL: little experience (basic queries only)

\- SPARQL: much experience (optimized/wrote advanced queries)

Tools

\- AWS: wrote some AWS lambda functions, helped with some ETL processes (mainly transformation)

\- Databricks: similar to AWS

So, in 2 months Iâ€™m starting my new job where I will be doing analytics and AI/ML but especially require solid data engineering skills. As the latter is what Iâ€™m least known with, I was wondering what types of python packages, tools, or you name it would be most beneficial to gain some extra experience with. Or what do you think the essentials for a data engineer â€œstarter packâ€ should contain?",1769717220.0,4,4,https://www.reddit.com/r/dataengineering/comments/1qqjh1h/preparing_for_new_job/
1qqi4gz,What's your personal approach to documenting workflows?,"I have a crapload of documentation that I have to keep chiseling away at.   Not gonna go into detail, but it's enough to shake a stick at. 

Right now I'm using VS Code amd writing .md files with an internal git repo.

I'm early enough to consider building a wiki.  Wikis fit my brain like a glove.  I feel they're easy to compartmentalize and keep subjects focused.  Easy to select only what you need in its entirety, things like that.

If it matters, the stuff I'm documenting is how systems are configered and linked, tracking any custom changes to data replications from one system to another.

So.  Does this sound familiar to anyone?  Have you seem this kind of stuff documented in a way that you really enjoyed?  Any personal suggestions?

PS-  In case anyone gets excited: No, I'm not reproducing documentation that vendors already provide.  

This is for the internal things about how our infrastructure is built, and workflows related to breakfix and change manement.",1769714300.0,8,7,https://www.reddit.com/r/dataengineering/comments/1qqi4gz/whats_your_personal_approach_to_documenting/
1qqhvik,Should I use redis or rocksdb for check pointing my message broker deliveries?,"For ***at least once processing*** or more complicated delivery guarantees (i.e ***exactly once unordered*** or ***exactly once ordering***) we need to check point that we received the message to some data system before we finish processing to the downstream sink and then acknowledging back to the message broker that we received the message.

Recall that we need this checkpoint in the situation the consumer fails post processing data sink pre message broker acknowledgment.

If we don't have this checkpoint we risk the message never getting delivered at all because the alternative is acknowledging the message ***pre*** data sink or not at all resulting in the message never being in our sink if a downstream sink replica fails or the consumer itself fails.

My question is what are t**he pros and cons of different checkpointing stores such as** ***rocksdb or redis*** \- and when would we use one over the other?",1769713765.0,3,1,https://www.reddit.com/r/dataengineering/comments/1qqhvik/should_i_use_redis_or_rocksdb_for_check_pointing/
1qqhdks,Thoughts on Metadata driven ingestion,"Iâ€™ve been recently told to implement a metadata driven ingestion frameworks, basically you define the bronze and silver tables by using config files, the transformations from bronze to silver are just basic stuff you can do in a few SQL commands.

However, Iâ€™ve seen multiple instances of home-made metadata driven ingestion frameworks, and Iâ€™ve seen none of them been successful.

I wanted to gather feedback from the community if youâ€™ve implemented a similar pattern at scale and it worked great ",1769712709.0,26,22,https://www.reddit.com/r/dataengineering/comments/1qqhdks/thoughts_on_metadata_driven_ingestion/
1qqgdkj,"Are Databricks and Snowflake going to start ""verticalizing""?","I think we're going to see Databricks and Snowflake start offering more vertical specific functionality over the next year or two. I wrote about why I think so in the linked blog post, but I'm curious if anyone has a different perspective. 

The counterargument is that AI is going to be all consuming and encompass the entire roadmap, but I think these companies need to try a few strategies to continue their (objectively impressive) growth. ",1769710599.0,20,12,https://www.reddit.com/r/dataengineering/comments/1qqgdkj/are_databricks_and_snowflake_going_to_start/
1qqfdx8,Insights on breaking into DA/AE/DE in 2027/2028,"\*\* repost because it was mistakenly removed twice. Mod approved 

I'm currently working in a role similar to a product manager, but leaning more toward the engineering side. While I currently earn an ok wage (working in the EU and coming from a third world country), I feel like I donâ€™t really see myself working in this line of work forever, and I donâ€™t see strong career/wage progression here.

While looking for a possible career shift that could play to my strengths, I stumbled upon analytics engineering/data engineering. A lot of articles and people Iâ€™ve read on gave me the impression that it might be possible to break into the field without having a degree specifically in the area (I have a degree in materials science and if my impression of this is wrong then sorry). Btw I basically dont have any programming or analytics background except the limited amount of time I had with Matlab.

My question is:

1. Do you think this will still be true in the coming years? Considering that Iâ€™m currently working full time and can only learn in my spare time after work, I donâ€™t plan to break into DE immediately, as I know thatâ€™s basically impossible. But maybe breaking into data analytics or analytics engineering could be more realistic and doable?
2. I'm currently starting with SQL and then plan on moving to Python, Git, some visualization tools and then dbt and cloud warehouses. Is this a solid plan or are there any other stuffs I should take into account? Any tips on typical mistakes that one can do early in these phase that might hinder/slow down my progress?
3. What are your best resources for learning and for having a decent roadmap or plan to become a data analyst, analytics engineer, or data engineer? I donâ€™t mind paying for a course if itâ€™s worth it. So far I'm using SQLBolt, w3schools, thoughtspot for their free courses as a start. Are there websites where I can practice writing SQL queries a lot? Any youtubers who make quality videos?

There is also the worry of AI coming in and disrupting the future job market but that is a topic that probably is gonna derail my questions here so lets skip that for now.

I know no one can really predict what the future will be like, but Iâ€™d love to hear perspectives and experiences from people who have been in the industry, or even those just starting out.

Thank you for reading and your help!",1769708539.0,7,4,https://www.reddit.com/r/dataengineering/comments/1qqfdx8/insights_on_breaking_into_daaede_in_20272028/
1qqe8e7,Why are most jobs remote?,I have been on the job market for 6 months and applying to data engineering/ data scientists roles (finishing my masters in CS). I am wondering why data engineering jobs are most often remote. Do you think these jobs are real? Are these just ghost postings? Are most data engineers WFH? ,1769706085.0,0,18,https://www.reddit.com/r/dataengineering/comments/1qqe8e7/why_are_most_jobs_remote/
1qqe5up,keeping up with new data engg tools,"Hi - all the engineers who have been around for years, how do you keep up with new tools tested for data engineering roles? I have 9 YoE, 6 years in data. I work primarily on SQL and SSIS, but companies want data engineers to have all newer skillsets. I am trying to prove my worth by doing personal projects (building end to end pipelines with newer tools). Any other suggestions/pointers please?",1769705939.0,2,2,https://www.reddit.com/r/dataengineering/comments/1qqe5up/keeping_up_with_new_data_engg_tools/
1qqdpgu,DSRs are doable until you need to explain backups and logs,"Everything's fine when someone says delete my data, the problem starts when the request is confirm where my data exists including logs, backups, analytics and third parties.

Answers are there but theyâ€™re spread out and depending on who replies the wording of course changes slightly, which I want to avoid.

Can we make a single source of truth for DSR responses?",1769704972.0,13,2,https://www.reddit.com/r/dataengineering/comments/1qqdpgu/dsrs_are_doable_until_you_need_to_explain_backups/
1qqdp7l,"With ""full stack"" coming to data, how should we adapt?","I recently posted a diagram of how in 2026 the job market is asking for generalists.

Seems we all see the same, so what's next? 

If AI engineers are getting salaries 2x higher than DEs while lacking data fundamentals, what's stopping us from picking up some new skills and excelling?",1769704958.0,238,104,https://www.reddit.com/r/dataengineering/comments/1qqdp7l/with_full_stack_coming_to_data_how_should_we_adapt/
1qqdoh4,I'm a student and I don't know anything.,"Hi, I'm currently studying systems engineering and I'd really like to specialize as a data engineer. I wanted to know what I need to learn to find a job. (My English is intermediate and I'm still studying  btw).",1769704916.0,4,3,https://www.reddit.com/r/dataengineering/comments/1qqdoh4/im_a_student_and_i_dont_know_anything/
1qqcuy7,Do online courses actually matter to companies hiring?,"Like, are they actually enough on their own to get entry level jobs? Please, I am just looking for answers. I don't have a college degree, but due to family, health, and mental health issues getting in the way, not intelligence. Codecademy has courses that are like 70 hours, 90 hours, labeled as career paths for Data Warehousing, Data Analysts and Data Engineers. They even have one that supposedly ends in a test that sounds like a genuine marker outside of Codecademy, CompTIAData+ certification. I am putting my all into working through, learning, and completing these, hours every day outside my (stupid, minimum wage) full time job. I need to know so I know if I'm simply wasting my time. If they are nice additions that reflect skill, but at the end of the day, not enough on their own, and businesses really want a college degree.",1769703191.0,4,17,https://www.reddit.com/r/dataengineering/comments/1qqcuy7/do_online_courses_actually_matter_to_companies/
1qqcgkt,Data Quality on Databricks,"I'm planning to work on Data Quality improvement project at work, where we heavily rely on Databricks and to dig dipper considered small practical exercise. Appreciate your feedback. [https://levelup.gitconnected.com/data-quality-on-databricks-55b3aa83fd57](https://levelup.gitconnected.com/data-quality-on-databricks-55b3aa83fd57) ",1769702347.0,3,0,https://www.reddit.com/r/dataengineering/comments/1qqcgkt/data_quality_on_databricks/
1qqbp2q,"How useful are certifications? (SnowPro, specifically)","Hey all! 

I'm a data engineer with 4 years of experience, and I'm currently on the lookout for a new job as I moved countries. I'm getting callbacks from recruiters for jobs but something that's been regularly tripping me up is that a LOT of these are looking for snowflake hands on experience which I do not have. I've primarily worked with AWS and Oracle cloud and some databricks. 

I'm debating the SnowPro Data Engineer certification as a result. Is it worth the time studying and money put into it? Obviously, it's not going to give me a GREAT step up over a candidate that has actual work experience in it, but have you gotten more consideration with the cert? How useful is the certification and the knowledge gained from prepping for it? ",1769700696.0,2,2,https://www.reddit.com/r/dataengineering/comments/1qqbp2q/how_useful_are_certifications_snowpro_specifically/
1qqandr,Are you expected to know how to set up your environment in a new role?,"Iâ€™ve noticed in my past few roles, whenever I start, the team seems surprised/annoyed to help me set up the environment. 

For example, in my current company they use Google cloud and ide of your choice(I went with VSCode). But, to me, I donâ€™t know what connectors or connections to use. To my knowledge that wasnt written down. In my last role they used Databricks and again theyâ€™re wasnâ€™t much written down. I get everyone is busy but if the process isnâ€™t documented â€”can you just start in a new environment without the help? 

Maybe Iâ€™m wrong and I need to learn the tools better but Iâ€™m curious if thatâ€™s what everyone else sees. 

Is it standard practice to have set up instructions in this role or is it expected that you can come in and set yourself up? If thatâ€™s the expectation what can I do to get better at that? ",1769698376.0,19,15,https://www.reddit.com/r/dataengineering/comments/1qqandr/are_you_expected_to_know_how_to_set_up_your/
1qq9o22,Reading 'Fundamentals of data engineering' has gotten me confused,"I'm about 2/3 through the book and all the talk about data warehouses, clusters and spark jobs has gotten me confused. At what point is a RDBMS not enough that a cluster system is necessary? ",1769696074.0,61,69,https://www.reddit.com/r/dataengineering/comments/1qq9o22/reading_fundamentals_of_data_engineering_has/
1qq98ku,Data quality stack in 2026,"How are people thinking about data quality and validation in 2026?

1. dbt tests, great expectations, monte carlo, etc?
2. How often do issues slip through checks unnoticed? (weekly for me)
3. Is anyone seeing promise using agents? I've got a few prototypes and am optimistic as a layer 1 review.

Would love to hear what's working and what isn't? ",1769695046.0,3,3,https://www.reddit.com/r/dataengineering/comments/1qq98ku/data_quality_stack_in_2026/
1qq8g3e,SAP data services designer mapping to ST mapping,"hello experts, 

I need your help with scenario below.

 I am working on converting existing workflows and dataflows in Data services to meaningful Source to target mapping (excel sheet). this activity is basically starting off moving away from DS to new tool/technology. 

To automate this, I exported a job in XML format and then fed it to the copilot to generate in the ST mapping template ( copilot generated .py file) . it does to some extent but not completely and misses out some important details. 

has anyone worked on similiar activity or have some more robust solution around it , please suggest.

I also tried to export ATL files , but XML was easier to parse with python. 

please guide. ",1769693056.0,1,2,https://www.reddit.com/r/dataengineering/comments/1qq8g3e/sap_data_services_designer_mapping_to_st_mapping/
1qq69k3,Architecture / Tools for sharing distinct datasets between two different companies?,"I have a requirement to join our 'Customer' table with an external partner's 'Customer' table to find commonalities, but neither side can expose the raw data to the other due to security/trust issues. Is there a 'Data Escrow' pattern or third-party service that handles this compute securely?",1769686697.0,1,4,https://www.reddit.com/r/dataengineering/comments/1qq69k3/architecture_tools_for_sharing_distinct_datasets/
1qq5oso,how to choose a data lake?,"Hello there! So, I was working on a project like photobank/DAM, later we intend to integrate AI to it. So, I joined the project as a data engineer. Now, we are trying to setup a data lake, current setup is just frontend + backend with sqllite but we will be working with big data. I am trying to choose data lake, what factors I should consider? What questions I should ask myself and from the team to find the ""fit"" for us? What I could be missing?",1769684774.0,3,9,https://www.reddit.com/r/dataengineering/comments/1qq5oso/how_to_choose_a_data_lake/
1qq5902,Is Microsoft Fabric really worth it?,"I am a DE with 7 years of experience. I have 3 years of On-prem and 3 years of GCP experience. For the last 1 year, I have been working on a project where Microsoft Fabric is being used. I am currently trying to switch, but I don't see any openings on Microsoft Fabric. I know Fabric is in its early years, but I'm not sure how to continue with this tech stack. Planning to move to GCP related roles. what do you think?",1769683256.0,55,87,https://www.reddit.com/r/dataengineering/comments/1qq5902/is_microsoft_fabric_really_worth_it/
1qq4zrw,Iceberg Rewrite Manifest Files: A Practical Guide,,1769682341.0,4,1,https://www.reddit.com/r/dataengineering/comments/1qq4zrw/iceberg_rewrite_manifest_files_a_practical_guide/
1qq4aw3,DataTalks Zoomcamp vs Deeplearning.ai Data Engineering (Joe Reis),"Hey guys, I'm an early Software Engineer that wants to pivot/specialize in Data Engineering, so I'm looking for a course for structured learning. I'm basically down to DataTalks Zoomcamp vs Deeplearning.ai Data Engineering (Joe Reis), but I was also considering IBM's on Coursera and Datacamp's career path. 

Also side question, what exactly would I be missing if I start the DataTalks Zoomcamp today since the start date has long passed already. Thanks.",1769679917.0,13,9,https://www.reddit.com/r/dataengineering/comments/1qq4aw3/datatalks_zoomcamp_vs_deeplearningai_data/
1qq3se7,Good Data with Databricks - problem with cache in Good Data,"Hey all!

  
got a question for people who had 'pleasure' to work with Good Data. How can I increase the cache so Good Data are not constantly querying dbx? 

The design looks like this:  
databricks is scheduled to run on 3 AM so between 3:01 and 2:59 next day nothing will change in these tables  
Good Data is using these tables to show data but even though it's not direct query its constantly querying dbx after filter change or whatever because it hasn't got enough space to store the refreshed data  
  
I was Power BI developer and tbh it's hard for me to understand this problem with Good Data...  Im not the good data admin so I'm relying on devs team that 'it is what it is' and it's pissing me off because it's ridiculous.

But my main-main problem is that it's laggy even though we (5 people) are the only data consumers. It will be laggy af when clients will start using it and going above Medium warehouse on dbx will be costly and this cost will be undefendable because ROI will be way too low.



Thanks in advance! ",1769677999.0,4,0,https://www.reddit.com/r/dataengineering/comments/1qq3se7/good_data_with_databricks_problem_with_cache_in/
1qq39sr,Is Microsoft Fabric revenue just Power BI revenue?,"Microsoft folks on Linked In have been talking up Fabric's growth and revenue calling it the fastest growing ... 2B $ growing at 60% YoY. 

But then then of our partners pointed out in 2022 when Power BI was mentioned in their financials as part of Power Platform, Power Platform revenue was 2B $ growing at 72% YoY.

Today there is no mention of Power Platform revenue.

Since Fabric is a pay to play subscription with F64s replacing the good old P1s. My guess is that the lion's share of that 2B is Power BI.

Power BI subscriptions still rule :)",1769676068.0,45,17,https://www.reddit.com/r/dataengineering/comments/1qq39sr/is_microsoft_fabric_revenue_just_power_bi_revenue/
1qq2cok,Apache Doris on S3 Express Zones,"This is more of a post to help everyone else out there.

If you are trying to use Apache Doris 3.1 or newer with AWS S3 Express zones, it will currently fail with a message similar to 

    SQL Error [1105] [HY000]: errCode = 2, detailMessage = pingS3 failed(put), please check your endpoint, ak/sk or permissions(put/head/delete/list/multipartUpload), status: [COMMON_ERROR, msg: put object failed: software.amazon.awssdk.services.s3.model.S3Exception

The issue is that by default the connector for Doris attempts to do an PingS3 command, which isn't supported, All you need to do is add the following statement at the end of your Create Vault command. 



**""s3\_validity\_check""** = **""false""**

  
So final version looks Like this:

    CREATE STORAGE VAULT IF NOT EXISTS pv12_s3_express 
    PROPERTIES (
         ""type"" = ""S3"",
         ""s3.endpoint"" = ""https://$S3 EXPRESS ENDPOINT FOR YOUR REGION"",
         ""s3.region"" = ""$REGION"",
         ""s3.bucket"" = ""$BUCKETNAME"", 
        ""s3.role_arn"" = ""arn:aws:iam::{ACCOUNT}:role/$ROLE_NAME"",
         ""s3.root.path"" = ""$FOLDER PATH IN DIRECTORY"",
         ""provider"" = ""S3"",
         ""use_path_style"" = ""false"",
         ""s3_validity_check"" = ""false"" 
    ); ",1769672789.0,2,0,https://www.reddit.com/r/dataengineering/comments/1qq2cok/apache_doris_on_s3_express_zones/
1qpv93e,Data Engineering project ETL/ELT practice,"Hello! I am trying to help some of my friends learn data engineering by creating their own project for their portfolio. Sadly, all the experience I have with ETL has come from working, so Iâ€™ve accessed databases from my company and used their resources for processing. Any ideas on how could I implement this project for them? For example, which data sources would you use for ingestion, would you process your data on the cloud or locally? Etc. please help!",1769651827.0,9,12,https://www.reddit.com/r/dataengineering/comments/1qpv93e/data_engineering_project_etlelt_practice/
1qpurc0,Streamlit Proliferation,"With the push of Claude code at larger enterprises, how are people planning on managing Streamlit proliferation.

Itâ€™s an incredibly powerful tool, and I imagine a situation where someone architects Snowflake to agentically build databases and tables for each app, but Iâ€™m a little nervous that by the end of the year I will have 1000 Streamlit apps with in a single database.

Whatâ€™s everyone else thinking, and how are yâ€™all planning to manage and govern it?",1769650528.0,35,26,https://www.reddit.com/r/dataengineering/comments/1qpurc0/streamlit_proliferation/
1qptvqr,Practice project idea,"Hello!

I want to do a practice project using the community Databricks version. I want to do something involving streaming data, and I want to use real data.

My idea would be do drop files into s3, then build out a medallion layer using either spark structured streaming or declarative pipelines (not sure if this is supported on community version). Finally my gold layer would be some normalized tables where I could do analytics or dashboards. 

Is this a sucky idea? If not, what would be some good real raw data to drop into s3, and how do I set that up? 

Thanks for any insights/help",1769648277.0,3,0,https://www.reddit.com/r/dataengineering/comments/1qptvqr/practice_project_idea/
1qprsx8,"Feedback on ETL Architecture: SaaS Control Plane with a ""Remote Agent"" Data Plane?","Iâ€™m an engineer currently bootstrapping a new ETL platform (Saddle Data). I have already built the core SaaS product (standard cloud-to-cloud sync), but I recently finished building a ""Remote Agent"" capability, and I want to sanity check with this community if this is actually a useful feature or if I'm over-engineering.

The Architecture: Iâ€™ve decoupled the Control Plane from the Data Plane.

* Control Plane (SaaS): Hosted by me. Handles the UI, scheduling, configuration, and state management.
* Data Plane (Your Infrastructure): You run a lightweight binary, or a container image, behind your firewall. It polls the Control Plane for jobs, connects to your local database (e.g., internal Postgres), and moves data directly to your destination.

I have worked at a number of big companies where a SaaS based data platform would never pass security requirements.

For those of you in regulated industries or with strict SecOps teams: Does this ""Hybrid"" model actually solve a problem for you? Or do you prefer to just go 100% SaaS and deal with security exceptions? Or do you prefer 100% Self-Hosted and deal with the maintenance headache?

Iâ€™ve already built the agent, but before I go deep into marketing/documenting it, Iâ€™d love to know if this architecture is something youâ€™d actually use.

Thanks!",1769643117.0,2,1,https://www.reddit.com/r/dataengineering/comments/1qprsx8/feedback_on_etl_architecture_saas_control_plane/
1qprrd8,Is a ~12% pay cut worth it to pivot from Consulting to Analytics Engineering (Databricks) at a stable End Client?,"Hi everyone,

I am facing a career dilemma and would love some insights, especially from those who have transitioned from Consulting to an Internal Role (End Client).

My Profile:

â€¢	Current Role: Data Analyst / BI Consultant.

â€¢	Experience: 5 years (mainly Power BI, SQL, some Python).

â€¢	Current Situation: Working for a Consulting Firm (ESN) in a major French city. My mission ended in December due to budget cuts, and I am currently â€œon the benchâ€ (inter-contract) with my probation period ending soon.

â€¢	The Issue: I am tired of the consulting model (instability, lack of ownership, dependency on random missions). I want to stabilize and, most importantly, transition into Analytics Engineering / Data Engineering.

The Offer (Internal Role):

I have an offer for a permanent contract (CDI) at an End Client (a digital subsidiary of a massive Fortune 500 industrial group, approx. 50 people in this specific entity).

â€¢	Title: Senior Analytics Engineer (New position creation).

â€¢	Tech Stack: Databricks / Spark + Power BI (Medallion architecture, Digital Performance & E-commerce focus). This is exactly the stack I need to master for my future career steps.

â€¢	The â€œCatchâ€: The fixed base salary offer is 12.5% lower than my current base salary in consulting.

â€¢	Variable: There is a 10% variable bonus (performance-based), which brings the total package closer to my current pay, but the guaranteed monthly income is definitely lower.

My Plan / Strategy:

1.	Tech: Acquire deep expertise in Databricks and Data Engineering (highly in demand).

2.	Domain: The role focuses on Digital Performance / E-commerce, which seems valuable.

My Questions for the community:

1.	Does taking a 12.5% step back on base salary seem justified to gain the Databricks expertise + the stability of an internal role?

2.	Is it risky to accept a â€œSeniorâ€ job title that pays below market rate for that level, or will the title itself be valuable on my CV in 2 years?

3.	Has anyone here taken a pay cut to pivot technically? What was the ROI after 2-3 years?

Thanks in advance for your advice!",1769643022.0,39,35,https://www.reddit.com/r/dataengineering/comments/1qprrd8/is_a_12_pay_cut_worth_it_to_pivot_from_consulting/
1qpom8i,NoSQL ReBAC,"Iâ€™m dealing with a production MongoDB system and Iâ€™m still relatively new to MongoDB, but I need to use it to implement an authorization flow.

I have a legacy MongoDB system with a deeply hierarchical data model (5+ levels). The first level represents a tenant (B2B / multi-tenant setup). Under each tenant, there are multiple hierarchical resource levels (e.g., level 2, level 3, etc.), and entity-based access control (ReBAC) can be applied at any of these levels, not only at the leaf level. Granting access to a higher-level resource should implicitly allow access to all of its descendant resources.

The main challenge is that the lowest level contains millions of records that users need to access. I need to implement a permission system that includes standard roles/permissions in addition to ReBAC, where access is granted by assigning specific entity IDs to users at different hierarchy levels under a tenant.

I considered using Auth0 FGA, but integrating a third-party authorization service appears to introduce significant complexity and may negatively impact performance in my case. It would require strict synchronization and cleanup between MongoDB and the authorization store especially challenging with hierarchical data (e.g., deleting a parent entity could require removing thousands of related relationships/tuples via external APIs). Additionally, retrieving large allow-lists for filtering and search operations may be impractical or become a performance bottleneck.

Given this context, would it be reasonable to keep authorization data within MongoDB itself and build a dedicated collection that stores entity type/ID along with the allowed users or roles? If so, how would you design a custom authorization module in MongoDB that efficiently supports multi-tenancy, hierarchical access inheritance, and ReBAC at scale?",1769635747.0,2,5,https://www.reddit.com/r/dataengineering/comments/1qpom8i/nosql_rebac/
1qplapl,How and where can i practice PySpark ?,Currently learning PySpark. Want to practice but unable to find any site where i can do that. Can someone please help ? Want a free online source for practicing ,1769628446.0,31,28,https://www.reddit.com/r/dataengineering/comments/1qplapl/how_and_where_can_i_practice_pyspark/
1qpkei4,Noob question: Where exactly should I fit SQL into my personal projects?,"Hi! I've been learning about DE and DA for about three months now. While I'm more interested in the DE side of things, I'm trying to keep things realistic and also include DA tools (I'm assuming landing a DA job is much easier as a trainee). My stack of tools, for now, is Python (pandas), SQL, Excel, and Power BI. I'm still learning about all these tools, but when I'm actually working on my projects, I don't exactly know where SQL would fit in.

  
For example, I'm now working on a project that pulls data of a particular user from the Lichess API, cleans it up, transforms it into usable tables (using a OBT scheme), and then loads it into either SQLite or CSVs. From my understanding, and from my experience in a few previous, simpler projects, I could push all that data directly into either Excel or PowerBI and go from there. 

  
I know that, for starters, I could clean it up even further in pandas (for example, solve those NaNs in the accuracy columns). I also know that SQL does have its usefulness: I thought about finding winrates for different openings, isolating win and lose streaks, and that sort of stuff. But why wouldn't I do that in pandas or Python?



[The current final table after the Python scripts; I'll be analyzing this. I censored the users just in case!](https://preview.redd.it/uywee59py4gg1.png?width=1462&format=png&auto=webp&s=1188c1819ed4115924fbefc9285b217a61109fe6)



Even if I wanted to use SQL, how does that connect to Excel and Power BI? Do I just pull everything into SQLite, create a DB, and then create new columns and tables just with SQL? And then throw that into Excel/Power BI?

  
Sorry if this is a dumb question, but I've been trying to wrap my head around it ever since I started learning this stuff. I've been practicing SQL on its own online, but I have yet to use it on a real project. Also, I know that some tools like SnowFlake use SQL, but I'm wondering how to apply it in a more ""home-made"" environment with a much simpler stack.

  
Thanks! Any help is greatly appreciated.",1769626567.0,4,13,https://www.reddit.com/r/dataengineering/comments/1qpkei4/noob_question_where_exactly_should_i_fit_sql_into/
1qpii5e,"Anyone seeing faster AWS Glue 4.0 jobs lately? (~30% cost drop, no changes)","Hi everyone, 

I wanted to check something weâ€™ve been seeing in my company with AWS Glue and see if anyone else has run into this.

We run several AWS Glue 4.0 batch jobs (around \~10 jobs, pretty stable workloads) that execute regularly. For most of 2025, both execution times and monthly costs were very consistent.

Then, starting around mid-November/early December 2025, we noticed a sudden and consistent drop in execution times across multiple Glue 4.0 jobs, which ended up translating into roughly \~30% lower cost compared to previous months.

Whatâ€™s odd is that nothing obvious changed on our side:

* No code changes.
* Still on Glue 4.0.
* No config changes (DPUs, job params, etc.).
* Data volumes look normal and within expected ranges.
* The improvement showed up almost at the same time across multiple jobs.

Same outputs, same logic. Just faster and cheaper.

I get that Glue is fully managed/serverless, but I couldnâ€™t find any public release notes or announcements that would clearly explain such a noticeable improvement specifically for Glue 4.0 workloads.

Has anyone else noticed Glue 4.0 jobs getting faster recently without changes? Could this be some kind of backend optimization (AMI, provisioning, IO, scheduler, etc.) rolled out by AWS? Any talks, blog posts, or changelogs that might hint at this?

Btw I'm not complaining at all , just trying to understand what happened.

",1769622690.0,6,7,https://www.reddit.com/r/dataengineering/comments/1qpii5e/anyone_seeing_faster_aws_glue_40_jobs_lately_30/
1qpek3a,Cloud storage for a company I'm doing a project in (Need help),"So basically, I'm currently doing a project for a company and one of the aspects is their tech setup. This is for a small/mid size manufacturing company with 60 employees. They currently have a hosted webmail service on outlook, an ERP, MES, hosted shared file server and email backups totalling 5 VM's. They do not have any Microsoft 365 plan.

Tech is definitely not my scope and I'm trying to understand this as I go. Here are the 5 VM's.

WSRVAPP (Shared folders)

CPU: 8 vCPU

RAM: 8 GB

Premium Storage: 80 GB (OS)

Premium Storage: 100 GB (MyBox Share)

Premium Storage: 440 GB (MyBox Share)

Premium Storage: 150 GB (MyBox Share)

WSRVDB (Database) (Assuming this is the ERP database as it's in SQL, maybe the MES too).

CPU: 8 vCPU

RAM: 24 GB

Standard Storage: 80 GB (OS)

Standard Storage: 160 GB (SQL Data)

Standard Storage: 80 GB (SQL Logs)

Standard Storage: 60 GB (SQL Temp)

Premium Storage: 200 GB (database backups)

WSRVERP (ERP)

CPU: 6 vCPU

RAM: 8 GB

Premium Storage: 80 GB (OS)

Premium Storage: 80 GB (Application files)

WSRVTS (Remote access -> Guessing this is for the MES)

CPU: 18 vCPU

RAM: 48 GB

Premium Storage: 230 GB

WSRVDC (This didn't even come with a description, I'm guessing it's for the email backup).

CPU: 4 vCPU

RAM: 6 GB

Premium Storage: 80 GB (OS)

In total, also including phone and wifi services from the same provider, this company is paying around 35-40k yearly. To make matters worse, they have internal servers in which all of this used to be allocated at, but they've since got rid of the two IT people they had due to increase in wages for these roles (I'm guessing they got better offers elsewhere) and thus decided to move everything to an external provider, leaving the servers here basically unused. 

Can someone help me understand what is the correct approach to do here? People complain that the MES is slow, the outlook via the web host is obviously not ideal because no one can sync it to their phones. The price looks pretty high for a company of this size (doing around 4-5M in revenue).

Any suggestions appreciated.",1769614434.0,2,0,https://www.reddit.com/r/dataengineering/comments/1qpek3a/cloud_storage_for_a_company_im_doing_a_project_in/
1qpe75w,Will this internship be useful?,"Hello I got an offer at a very big company for a data engineering internship.

They say it will be Frontend with Typescript/React

And Backend with Python/LowCode Tools

The main tool they use is Palantir Foundry.

Also I dont have real coding experience 

Will this be a useful internship or is it kinda too niche and front end heavy?

thanks",1769613623.0,2,4,https://www.reddit.com/r/dataengineering/comments/1qpe75w/will_this_internship_be_useful/
1qpctqj,Building an On-Premise Intelligent Document Processing Pipeline for Regulated Industries : An architectural pattern for industrializing document processing across multiple business programs under strict regulatory compliance,Quick 5min read: Intelligent Document Processing for Regulated Industries.,1769610497.0,3,0,https://www.reddit.com/r/dataengineering/comments/1qpctqj/building_an_onpremise_intelligent_document/
1qpbkfa,"I got tired of finding out my DAGs failed from Slack messages, so I built an open-source Airflow monitoring tool","Hey guys,

Granyt is a self-hostable monitoring tool for Airflow. I built it after getting frustrated with every existing open source option:

* Sentry is great, but it doesn't know what a dag\_id is. Errors get grouped weirdly and the UI just wasn't designed for data pipelines.
* Grafana + Prometheus feels like it needs a PhD to set up, and there's no real Python integration for error analysis. Spent a week configuring everything, then never looked at it again.
* Airflow UI shows me what happened, not what went wrong. And the interface (at least in Airflow 2) is slow and clunky.

What Granyt does differently:

* Stack traces that show dag\_id, task\_id, and run\_id. Grouped by fingerprint so you see patterns, not noise. Built for DAGs from the ground up - not bolted on as an afterthought.
* Alerts that actually matter. Row count drops? Granyt tells you before the CEO asks on Monday. Just return metrics in XCom and Granyt picks them up automatically.
* Connect all your environments to one source of truth. Catch issues in dev before they hit your production environment.
* 100% open source and self-hostable (Kubernetes and Docker support). Your data never leaves your servers.

Thought it may be useful to others, so I am open sourcing it. Happy to answer any questions!",1769607470.0,15,3,https://www.reddit.com/r/dataengineering/comments/1qpbkfa/i_got_tired_of_finding_out_my_dags_failed_from/
1qp9sys,"Scattered DQ checks are dead, long live Data Contracts","santiviquez from Soda here.

In most teams Iâ€™ve worked with, data quality checks end up split across dbt tests, random SQL queries, Python scripts, and whatever assumptions live in peopleâ€™s heads. When something breaks, figuring out what was supposed to be true is not that obvious.

We just released Soda Core 4.0, an open-source data contract verification engine that tries to fix that by making Data Contracts the default way to define DQ table-level expectations.

Instead of scattered checks and ad-hoc rules, you define data quality once in YAML. The CLI then validates both schema and data across warehouses like Snowflake, BigQuery, Databricks, Postgres, DuckDB, and others.

The idea is to treat data quality infrastructure as code and let a single engine handle execution. The current version ships with 50+ built-in checks.

Repo: [https://github.com/sodadata/soda-core](https://github.com/sodadata/soda-core)  
Release notes: [https://soda.io/blog/introducing-soda-4.0](https://soda.io/blog/introducing-soda-4.0)

",1769602655.0,13,5,https://www.reddit.com/r/dataengineering/comments/1qp9sys/scattered_dq_checks_are_dead_long_live_data/
1qp8tni,would you consider Kubernetes knowledge to be part of data engineering ?,"My school offers some LFIs certifications like CKA, I always see kubernetes here and there on this sub but my understanding is that almost no one uses it. As a student I am jiggling between two paths data engineering & cloud. So I may pull a trigger on it but I want to hear everyone's opinion.",1769599652.0,9,18,https://www.reddit.com/r/dataengineering/comments/1qp8tni/would_you_consider_kubernetes_knowledge_to_be/
1qp7ryc,Am I underpaid for this data engineering role?,"I have \~3.5 years of experience in BI and reporting. About 5 months ago, I joined a healthcare consultancy working on a large data migration and archiving project. Iâ€™m building ETL from scratch and writing JSON-based pipelines using an in-house ETL tool â€” feels very much like a data engineering role.

My current salary is 90k AUD, and Iâ€™m wondering if thatâ€™s low for this kind of work. What salary range would you expect for a role like this?(Iâ€™m based in Melbourne)

Thanks in advance.",1769596120.0,0,1,https://www.reddit.com/r/dataengineering/comments/1qp7ryc/am_i_underpaid_for_this_data_engineering_role/
1qp7fl5,How to adopt Avro in a medium-to-big sized Kafka application,"Hello, 

Wanting to adopt Avro in an existing Kafka application (Java, spring cloud stream, Kafka stream and Kafka binders)

Reason to use Avro:

1) Reduced payload size and even further reduction post compression

2) schema evolution handling and strict contracts 

Currently project uses json serialisers - which are relatively large in size

Reflection seems to be choice for such case - as going schema first is not feasible (there are 40-45 topics with close to 100 consumer groups) 

Hence it should be Java class driven - where reflection is the way to go - then is uploading to registry via reflection based schema an option? - Will need more details on this from anyone who has done a mid-project avro onboarding 

Cheers !",1769594931.0,5,0,https://www.reddit.com/r/dataengineering/comments/1qp7fl5/how_to_adopt_avro_in_a_mediumtobig_sized_kafka/
1qp6j55,CAREER ADVISE,"Hi guys, Iâ€™m a freshman in college now and my major is Data Science. I kinda want to have a career as a Data Engineer and I need advice from all of you. In my school, I have something called â€œConcentrationâ€ in my major so that I could concentrate on what field of Data Science

I have 3 choices now: Statistics, Math and Economics. What so you guys think will be the best choice for me? I would really appreciate your advise. Thank you",1769591601.0,1,10,https://www.reddit.com/r/dataengineering/comments/1qp6j55/career_advise/
1qp5lva,Has anyone successfully converted Spark Dataset API batch jobs to long-running while loops on YARN?,"My code works perfectly when I run short batch jobs that last seconds or minutes. Same exact Dataset logic inside a while(true) polling loop works fine for the first five or six iterations and then the app just disappears. No exceptions. No Spark UI errors. No useful YARN logs. The application is just gone.

Running Spark 2.3 on YARN though I can upgrade to 2.4.1 if needed. Single executor with 10GB memory driver at 4GB which is totally fine for batch runs. Pseudo flow is SparkSession created once then inside the loop I poll config read parquet apply filters groupBy cache transform write results then clear cache. I am wondering if I am missing unpersist calls or holding Dataset references across iterations without realizing it.

I tried calling spark.catalog.clearCache on every loop and increased YARN timeouts. Memory settings seem fine for batch workloads. My suspicion is Dataset references slowly accumulating causing GC pressure then long GC pauses then executor heartbeat timeout so YARN kills it silently. The mkuthan YARN streaming article talks about configs but not Dataset API behavior inside loops.

Has anyone debugged this kind of silent death with Dataset loops. Do I need to explicitly unpersist every Dataset every iteration. Is this just a bad idea and I should switch to Spark Streaming. Or is there a way to monitor per iteration memory growth GC pauses and heartbeat issues to actually see what is killing the app. Batch resources are fine the problem only shows up with the long running loop. So please suggest me what to do here im fully stuckâ€¦. Thaks

",1769588236.0,2,2,https://www.reddit.com/r/dataengineering/comments/1qp5lva/has_anyone_successfully_converted_spark_dataset/
1qp48tg,[Need sanity check on approach] Designing an LLM-first analytics DB (SQL vs Columnar vs TSDB),"Hi Folks,

Iâ€™m designing an LLM-first analytics system and want a quick sanity check on the DB choice.

# Problem

* Existing Postgres OLTP DB (Very clutured, unorganised and JSONB all over the place)
* Creating a read-only clone whose primary consumer is an LLM
* Queries are analytical + temporal (monthly snapshots, LAG, window functions)

we're targeting accuracy on LLM response, minimum hallucinations, high read concurrency for almost 1k-10k users

# Proposed approach

1. Columnar SQL DB as analytics store -> ClickHouse/DuckDB
2. OLTP remains source of truth -> Batch / CDC sync into column DB
3. Precomputed semantic tables (monthly snapshots, etc.)
4. LLM has read-only access to semantic tables only

# Questions

1. Does ClickHouse make sense here for hundreds of concurrent LLM-driven queries?
2. Any sharp edges with window-heavy analytics in ClickHouse?
3. Anyone tried LLM-first analytics and learned hard lessons?

Appreciate any feedback mainly validating direction, not looking for a PoC yet.",1769583440.0,4,4,https://www.reddit.com/r/dataengineering/comments/1qp48tg/need_sanity_check_on_approach_designing_an/
1qp2xul,"Data Engineers learning AI,what are you studying & what resources are you using?","Hey folks,

For the Data Engineers here who are currently learning AI / ML, Iâ€™m curious:

	â€¢	What topics are you focusing on right now?

	â€¢	What resources are you using (courses, books, blogs, YouTube, projects, etc.)?

	

Iâ€™m a transitioning to DE will be starting to go deeper into AI and would love to hear whatâ€™s actually been useful vs hype cause all I hear is AI AI AI LLM AI.",1769579164.0,13,11,https://www.reddit.com/r/dataengineering/comments/1qp2xul/data_engineers_learning_aiwhat_are_you_studying/
1qp1uqw,That feeling of being stuck,"10+ years in a product based company

Working on an Oracle tech stack. Oracle Data Integrator, Oracle Analytics Server, GoldenGate etc.

When I look outside, everything looks scary.

The world of analytics and data engineering has changed. Its mostly about Snowflake or Databricks or few other tools. Add AI to it and its giving me a feeling I just cant catch up

I fear how can i catch up with this. Have close to 18 YOE in this area. Started with Informatica then AbInitio and now onto the Oracle stack. 

Learnt Big Data, but never used it. Forgot it. Trying to cope with the Gen AI stuff and see what can do here (atleast to keep pace with the developments)

But honestly, very clueless about where to restart. I feel stagnant. Whenever I plan to step out of this zone, I step behind thinking I am heavily underprepped for this. 

And all of this being in India. More the YOE, lesser the value opportunities you have in market. ",1769575877.0,23,13,https://www.reddit.com/r/dataengineering/comments/1qp1uqw/that_feeling_of_being_stuck/
1qp0f0i,The Data Engineer Role is Being Asked to Do Way Too Much,"

I've been thinking about how companies are treating data engineers like they're some kind of tech wizards who can solve any problem thrown at them.

Looking at the various definitions of what data engineers are supposedly responsible for, here's what we're expected to handle:

1. Development, implementation, and maintenance of systems and processes that take in raw data
2. Producing high-quality data and consistent information
3. Supporting downstream use cases
4. Creating core data infrastructure
5. Understanding the intersection of security, data management, DataOps, data architecture, orchestration, AND software engineering

That's... a lot. Especially for one position.

I think the issue is that people hear ""engineer"" and immediately assume ""Oh, they can solve that problem."" Companies have become incredibly dependent on data engineers to the point where we're expected to be experts in everything from pipeline development to security to architecture.

I see the specialization/breaking apart of the Data Engineering role as a key theme for 2026. We can't keep expecting one role to be all things to all people.

What do you all think? Are companies asking too much from DEs, or is this breadth of responsibility just part of the job now?
",1769571808.0,435,45,https://www.reddit.com/r/dataengineering/comments/1qp0f0i/the_data_engineer_role_is_being_asked_to_do_way/
1qozv1g,Real-life Data Engineering vs Streaming Hype â€“ What do you think?,"I recently read a post where someone described the reality of Data Engineering like this:

Streaming (Kafka, Spark Streaming) is cool, but itâ€™s just a small part of daily work.
Most of the time weâ€™re doing â€œboring but necessaryâ€ stuff:
    Loading CSVs
    Pulling data incrementally from relational     databases
    Cleaning and transforming messy data
The flashy streaming stuff is fun, but not the bulk of the job.

What do you think?

Do you agree with this?
Are most Data Engineers really spending their days on batch and CSVs, or am I missing something?",1769570284.0,68,46,https://www.reddit.com/r/dataengineering/comments/1qozv1g/reallife_data_engineering_vs_streaming_hype_what/
1qoz0rq,Review about DataTalks Data Engineering Zoomcamp 2026,"How is the zoomcamp for a person like me, i have described my struggles on the previous post as well. But long story short like i am new to DE. I don't have any concurrent courses going on. Like been following and looking freely on youtube and other resources. Also there are plenty of ups and downs regarding the reviews of the zoomcamp in the past.  
So like should i enroll or like explore on my own?  
Your feedback would be a great help for me as well as other who are also looking for the same thing",1769568106.0,7,5,https://www.reddit.com/r/dataengineering/comments/1qoz0rq/review_about_datatalks_data_engineering_zoomcamp/
1qoxz30,Confluence <-> git repo sync?,"has anyone played around with this pattern? I know there is docusaurus but that doesn't quite scratch the itch. I want a markdown first solution where we could keep confluence in sync with git state. 

anyone played around with this? at face value the confluence API doesn't look all that bad, if it doesn't exist why does it not exist? 

I'm sure there is a package in missing. why no clean integration yet?",1769565445.0,1,14,https://www.reddit.com/r/dataengineering/comments/1qoxz30/confluence_git_repo_sync/
1qovfio,AI learning for data engineers,"As a data engineer, what do you all suggest i should learn related to AI.

I have only tried co pilot as assistance but are there any specific skills i should learn to stay relevant as data engineer?",1769558983.0,2,5,https://www.reddit.com/r/dataengineering/comments/1qovfio/ai_learning_for_data_engineers/
1qotieb,Best Practices for Historical Tables?,"Iâ€™m responsible for getting an HR database set up and ready for analytics.

I have some static data that I plan on refreshing on certain schedules for regular data, like location tables, region tables and codes, and especially employee data and applicant tracking data.

As part of the applicant tracking data, they also want real time data with the ATSâ€™s data stream API (Real-Time Streaming Data). The ATS does not expose any historical information from the regular endpoint, historical data NEEDS to be exposed via â€œData Streamâ€ API.

Now, I guess my question is for best practice, should the data stream api be used to update the applicant data table with the candidate data, or have it kept separate and only add rows to a table dedicated for streaming? (Or both?)

So if

userID 123

Name = John

Current workflow status = Phone Screening

Current Workflow Status Date = 01/27/2026 2PMEST

application date = 01/27/2026

The data stream API sends a payload when a candidateâ€™s status is updated. I imagine that the current workflow status and date gets updated, or, should it insert a new row onto the candidate data table to allow us to â€œfollowâ€ the candidate through the stages?

Iâ€™m also seriously considering just hiring a consultant for this.",1769554415.0,6,4,https://www.reddit.com/r/dataengineering/comments/1qotieb/best_practices_for_historical_tables/
1qot5ku,How do you decide between competing tools?,"When you need to make a technical decision between competing tools, where do you go for advice?

I can empathise. It all depends on the requirement, but here's my real question. When you are told that 'Everyone is using Tool X for this use case', how do you actually validate if that's true for your use case?""

I've been struggling with this lately. Example: deciding between a couple of Archtecture decision. Now with AI, everyone sounds smart with one query away.

So my question is, where do you go for advice or validation?

**StackOverflow: Anonymous Experts**

* 2018 - What are the best Python data frames for processing?
* 2018 - (Accepted Answer) Pandas
* 2024 - (comment) Actually, there is something called Polars, eats Pandas for breakfast(+200 upvotes)
* But the 2018 answer stays on top forever.

**Blog posts**

* SEO spam
* Vendor marketing disguised as ""unbiased comparison""
* AI-generated, that sounds smart.

**Colleagues**

* Limited to what they've personally used.
* We use X because... that's what we use.
* Haven't had the luxury to evaluate alternatives.

**Documentation (every tool)**

* Scalable, Performant, Easy
* But missing ""**When NOT to use our tool**""

**What I really want is Human Intelligence(HI)**

Someone who has used both X and Y in production, at a similar scale, who can say:

* I tried both, here's what actually scaled.
* X is better if you have constraint Z
* The docs don't mention this, but the real limitation is...

**Does anyone else feel this pain? How do you solve it?**

Thinking about building something to fix this - would love to hear if this resonates with others or if I'm just going crazy.",1769553594.0,3,12,https://www.reddit.com/r/dataengineering/comments/1qot5ku/how_do_you_decide_between_competing_tools/
1qos08a,Is my Airflow environment setup too crazy?,"I started learning airflow a few weeks ago. I had a very hard time trying to setup the environment. After some suffering the solution I found was to use a modified version of the docker-compose file that airflow tutorial provides here: [https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)

I feel like there must be an easier/cleaner way than this...

Any tips, suggestions?",1769551015.0,1,1,https://www.reddit.com/r/dataengineering/comments/1qos08a/is_my_airflow_environment_setup_too_crazy/
1qoqc3q,Team of data engineers building git for data and looking for feedback.,"Today you can easily adopt AI coding tools (i.e. Cursor) because you have git for branching and rolling back if AI writes bad code. As you probably know, we haven't seen this same capability for data so my friends and I decided to build it ourselves.

Nile is a new kind of data lake, purpose built for using with AI. It can act as your data engineer or data analyst creating new tables and rolling back bad changes in seconds. We support real versions for data, schema, and ETL.  
  
We'd love your feedback on any part of what we are building -Â [https://getnile.ai/](https://getnile.ai/) 

Do you think this is a missing piece for letting AI run on data?

  
DISCLAIMER: I am one of the founders of this company.",1769547418.0,0,34,https://www.reddit.com/r/dataengineering/comments/1qoqc3q/team_of_data_engineers_building_git_for_data_and/
1qonpse,Informatica deploying DEV to PROD,"I'm very new to Informatica and am using the application integration module rather than the data integration module.

I'm curious how to promote DEV work up through the environments. I've got app connectors with properties but can't see how to supply it with environment specific properties.
There are quite a few capabilities that I've taken for granted in other ETL tools that are either well hidden (I've not found them) or don't exist.
I can tell it to run a script but can't get the output from that script other than redirecting it to STDERR. This seems bizarre. ",1769541814.0,2,0,https://www.reddit.com/r/dataengineering/comments/1qonpse/informatica_deploying_dev_to_prod/
1qon39x,Benchmarking DuckDB vs BigQuery vs Athena on 20GB of Parquet data,"I'm building an integrated data + compute platform and couldn't find good apples-to-apples comparisons online. So I ran some benchmarks and wanted to share. Sharing here to gather feedback.

Test dataset is ~20GB of financial time-series data in Parquet (ZSTD compressed), 57 queries total.

---

## TL;DR

Platform | Warm Median | Cost/Query | Data Scanned
:--|:--|:--|:--
DuckDB Local (M) | 881 ms | - | -
DuckDB Local (XL) | 284 ms | - | -
DuckDB + R2 (M) | 1,099 ms | - | -
DuckDB + R2 (XL) | 496 ms | - | -
BigQuery | 2,775 ms | $0.0282 | 1,140 GB
Athena | 4,211 ms | $0.0064 | 277 GB

*M = 8 threads, 16GB RAM | XL = 32 threads, 64GB RAM*

**Key takeaways:**

1. DuckDB on local storage is 3-10x faster than cloud platforms
2. BigQuery scans 4x more data than Athena for the same queries
3. DuckDB + remote storage has significant cold start overhead (14-20 seconds)

---

## The Setup

**Hardware (DuckDB tests):**

- CPU: AMD EPYC 9224 24-Core (48 threads)
- RAM: 256GB DDR
- Disk: Samsung 870 EVO 1TB (SATA SSD)
- Network: 1 Gbps
- Location: Lauterbourg, FR

**Platforms tested:**

Platform | Configuration | Storage
:--|:--|:--
DuckDB (local) | 1-32 threads, 2-64GB RAM | Local SSD
DuckDB + R2 | 1-32 threads, 2-64GB RAM | Cloudflare R2
BigQuery | On-demand serverless | Google Cloud
Athena | On-demand serverless | S3 Parquet

**DuckDB configs:**

    Minimal:  1 thread,  2GB RAM,   5GB temp (disk spill)
    Small:    4 threads, 8GB RAM,  10GB temp (disk spill)
    Medium:   8 threads, 16GB RAM, 20GB temp (disk spill)
    Large:   16 threads, 32GB RAM, 50GB temp (disk spill)
    XL:      32 threads, 64GB RAM, 100GB temp (disk spill)

**Methodology:**

- 57 queries total: 42 typical analytics (scans, aggregations, joins, windows) + 15 wide scans
- 4 runs per query: First run = cold, remaining 3 = warm
- All platforms queried identical Parquet files
- Cloud platforms: On-demand pricing, no reserved capacity

---

## Why Is DuckDB So Fast?

DuckDB's vectorized execution engine processes data in batches, making efficient use of CPU caches. Combined with local SSD storage (no network latency), it consistently delivered sub-second query times.

Even with medium config (8 threads, 16GB), DuckDB Local hit 881ms median. With XL (32 threads, 64GB), that dropped to 284ms.

For comparison:

- BigQuery: 2,775ms median (3-10x slower)
- Athena: 4,211ms median (~5-15x slower)

---

## DuckDB Scaling

Config | Threads | RAM | Wide Scan Median
:--|:--|:--|:--
Small | 4 | 8GB | 4,971 ms
Medium | 8 | 16GB | 2,588 ms
Large | 16 | 32GB | 1,446 ms
XL | 32 | 64GB | 995 ms

Doubling resources roughly halves latency. Going from 4 to 32 threads (8x) improved performance by 5x. Not perfectly linear but predictable enough for capacity planning.

---

## Why Does Athena Scan Less Data?

Both charge $5/TB scanned, but:

- BigQuery scanned 1,140 GB total
- Athena scanned 277 GB total

That's a 4x difference for the same queries.

Athena reads Parquet files directly and uses:

- **Column pruning:** Only reads columns referenced in the query
- **Predicate pushdown:** Applies WHERE filters at the storage layer
- **Row group statistics:** Uses min/max values to skip entire row groups

BigQuery reports higher bytes scanned, likely due to how external tables are processed (BigQuery rounds up to 10MB minimum per table scanned).

---

## Performance by Query Type

Category | DuckDB Local (XL) | DuckDB + R2 (XL) | BigQuery | Athena
:--|:--|:--|:--|:--
Table Scan | 208 ms | 407 ms | 2,759 ms | 3,062 ms
Aggregation | 382 ms | 411 ms | 2,182 ms | 2,523 ms
Window Functions | 947 ms | 12,187 ms | 3,013 ms | 5,389 ms
Joins | 361 ms | 892 ms | 2,784 ms | 3,093 ms
Wide Scans | 995 ms | 1,850 ms | 3,588 ms | 6,006 ms

Observations:

- DuckDB Local is 5-10x faster across most categories
- Window functions hurt DuckDB + R2 badly (requires multiple passes over remote data)
- Wide scans (SELECT *) are slow everywhere, but DuckDB still leads

---

## Cold Start Analysis

This is often overlooked but can dominate user experience for sporadic workloads.

Platform | Cold Start | Warm | Overhead
:--|:--|:--|:--
DuckDB Local (M) | 929 ms | 881 ms | ~5%
DuckDB Local (XL) | 307 ms | 284 ms | ~8%
DuckDB + R2 (M) | 19.5 sec | 1,099 ms | ~1,679%
DuckDB + R2 (XL) | 14.3 sec | 496 ms | ~2,778%
BigQuery | 2,834 ms | 2,769 ms | ~2%
Athena | 3,068 ms | 3,087 ms | ~0%

DuckDB + R2 cold starts range from 14-20 seconds. First query fetches Parquet metadata (file footers, schema, row group info) over the network. Subsequent queries are fast because metadata is cached.

DuckDB Local has minimal overhead (~5-8%). BigQuery and Athena also minimal (~2% and ~0%).

---

## Wide Scans Change Everything

Added 15 SELECT * queries to simulate data exports, ML feature extraction, backup pipelines.

Platform | Narrow Queries (42) | With Wide Scans (57) | Change
:--|:--|:--|:--
Athena | $0.0037/query | $0.0064/query | +73%
BigQuery | $0.0284/query | $0.0282/query | -1%

Athena's cost advantage comes from column pruning. When you SELECT *, there's nothing to prune. Costs converge toward BigQuery's level.

---

## Storage Costs (Often Overlooked)

Query costs get attention, but storage is recurring:

Provider | Storage ($/GB/mo) | Egress ($/GB)
:--|:--|:--
AWS S3 | $0.023 | $0.09
Google GCS | $0.020 | $0.12
Cloudflare R2 | $0.015 | $0.00

R2 is 35% cheaper than S3 for storage. Plus zero egress fees.

**Egress math for DuckDB + remote storage:**

1000 queries/day Ã— 5GB each:

- S3: $0.09 Ã— 5000 = $450/day = **$13,500/month**
- R2: **$0/month**

That's not a typo. Cloudflare doesn't charge egress on R2.

---

## When I'd Use Each

Scenario | My Pick | Why
:--|:--|:--
Sub-second latency required | DuckDB local | 5-8x faster than cloud
Large datasets, warm queries OK | DuckDB + R2 | Free egress
GCP ecosystem | BigQuery | Integration convenience
Sporadic cold queries | BigQuery | Minimal cold start penalty

---

## Data Format

- **Compression:** ZSTD
- **Partitioning:** None
- **Sort order:** (symbol, dateEpoch) for time-series tables
- **Total:** 161 Parquet files, ~20GB

Table | Files | Size
:--|:--|:--
stock_eod | 78 | 12.2 GB
financial_ratios | 47 | 3.6 GB
income_statement | 19 | 1.6 GB
balance_sheet | 15 | 1.8 GB
profile | 1 | 50 MB
sp500_constituent | 1 | <1 MB

---

## Data and Compute Locations

Platform | Data Location | Compute Location | Co-located?
:--|:--|:--|:--
BigQuery | europe-west1 (Belgium) | europe-west1 | Yes
Athena | S3 eu-west-1 (Ireland) | eu-west-1 | Yes
DuckDB + R2 | Cloudflare R2 (EU) | Lauterbourg, FR | Network hop
DuckDB Local | Local SSD | Lauterbourg, FR | Yes

BigQuery and Athena co-locate data and compute. DuckDB + R2 has a network hop explaining the cold start penalty. Local DuckDB eliminates network entirely.

---

## Limitations

- **No partitioning:** Test data wasn't partitioned. Partitioning would likely improve all platforms.
- **Single region:** European regions only. Results may vary elsewhere.
- **ZSTD compression:** Other codecs (Snappy, LZ4) may show different results.
- **No caching:** No Redis/Memcached.

---

## Raw Data

Full benchmark code and result CSVs: [GitHub - Insydia-Studio/benchmark-duckdb-athena-bigquery](https://github.com/Insydia-Studio/benchmark-duckdb-athena-bigquery)

**Result files:**

- duckdb_local_benchmark - 672 query runs
- duckdb_r2_benchmark - 672 query runs
- cloud_benchmark (BigQuery) - 168 runs
- athena_benchmark - 168 runs
- wide_scan_* files - 510 runs total

---

Happy to answer questions about specific query patterns or methodology. Also curious if anyone has run similar benchmarks with different results.",1769540526.0,23,20,https://www.reddit.com/r/dataengineering/comments/1qon39x/benchmarking_duckdb_vs_bigquery_vs_athena_on_20gb/
1qomxj2,Centralizing Airtable Base URLS into a searchable data set?,"I'm not an engineer, so apologies if I am describing my needs incorrectly.  I've been managing a large data set of individuals who have opted in (over 10k members), sharing their LinkedIn profiles.  Because Airtable is housing this data, it is not enriching, and I don't have a budget for a tool like Clay to run on top of thousands (and growing) records.  I need to be able to search these records and am looking for something like Airbyte or another tool that would essentially run Boolean queries on the URL data.  I prefer keyword search to AI.  Any ideas of existing tools that work well at centralizing data for search?  I don't need this to be specific to LinkedIn.  I just need a platform that's really good at combining various data sets and allowing search/data enrichment.  Thank you! ",1769540200.0,2,1,https://www.reddit.com/r/dataengineering/comments/1qomxj2/centralizing_airtable_base_urls_into_a_searchable/
1qolskj,Quick/easy certs to show knowledge of dbt/airflow?,"I have used countless ETL tools over the past 20 years. Started with MS SQL and literal DTS editor way back in dinosaur days, been the analyst and dev and ""default DBA."" Now I'm a director, leading data and analytics teams and architecting solutions. 

I really doubt that there is anything in dbt or airflow that I couldn't deal with, and I would have a team for the day to day.   However, when I'm applying for jobs, the recruiters and ATS tools still gatekeep based on the specific stack their org uses. (Last org was ADF and Matillion, which seem to be out of fashion now)

I want to be able to say that I know these, with a clean conscience, so are there some (not mind-numbing) courses I can complete to ""check the box""?  Same for Py.  I've used R and SAS (ok, mainly in grad school) and can review/edit my team's work fine, but I don't really work in it directly. And I don't like lying. Any suggestions to keep me hirable and my conscience clear? ",1769537857.0,0,4,https://www.reddit.com/r/dataengineering/comments/1qolskj/quickeasy_certs_to_show_knowledge_of_dbtairflow/
1qolp4i,Calling Fabric / OneLake multi-cloud is flat earth syndrome...,"If all the control planes and compute live in one cloud, slapping â€œmultiâ€ on the label doesnâ€™t change reality. 

Come on the earth is not flat folks...",1769537662.0,17,7,https://www.reddit.com/r/dataengineering/comments/1qolp4i/calling_fabric_onelake_multicloud_is_flat_earth/
1qoiat1,Are you seeing this too?,"Hey folks - i am writing a blog and trying to explain the shift in data roles in the last years.

Are you seeing the same shift towards the ""full stack builder"" and the same threat to the traditional roles?

please give your constructive honest observations , not your copeful wishes.",1769530658.0,479,64,https://www.reddit.com/r/dataengineering/comments/1qoiat1/are_you_seeing_this_too/
1qohpkm,SQL question collection with interactive sandboxes,"Made a collection of SQL challenges and exercises that let you practice on actual databases instead of just reading solutions. These are based on real world use cases in network monitoring world, I just slightly adapted to make it use cases more generic 

Covers the usual suspects:

* Complex JOINs and self-joins
* Window functions (RANK, ROW\_NUMBER, etc.)
* Subqueries vs CTEs
* Aggregation edge cases
* Date/time manipulation

Each question runs on real MySQL or PostgreSQL instances in your browser. No Docker, no local setup, no BS - just write queries and see results immediately.

[https://sqlbook.io/collections/7-mastering-ctes-common-table-expressions](https://sqlbook.io/collections/7-mastering-ctes-common-table-expressions)",1769529412.0,7,2,https://www.reddit.com/r/dataengineering/comments/1qohpkm/sql_question_collection_with_interactive_sandboxes/
1qobvpg,How do you reconstruct historical analytical pipelines over time?,"Iâ€™m trying to understand how teams handle reconstructing \*past\* analytical states when pipelines evolve over time.

Concretely, when you look back months or years later, how do you determine what inputs were actually available at the time, which transformations ran and in which order, which configs / defaults / fallbacks were in place, whether the pipeline can be replayed exactly as it ran then?

Do you mostly rely on data versioning / bitemporal tables? pipeline metadata and logs? workflow engines (Airflow, Dagster...)? or accepting that exact reconstruction isnâ€™t always feasible?

Is process-level reproducibility something you care about or is data-level lineage usually sufficient in practice?

Thank you!",1769515107.0,9,8,https://www.reddit.com/r/dataengineering/comments/1qobvpg/how_do_you_reconstruct_historical_analytical/
1qoa34u,Help with time series â€œmissingâ€ values,"Hi all,

Iâ€™m working on time series data prep for an ML forecasting problem (sales prediction).

My issue is handling implicit zeros. I have sales data for multiple items, but records only exist for days when at least one sale happened. When thereâ€™s no record for a given day, it actually means zero sales, so for modeling I need a continuous daily time series per item with missing dates filled and the target set to 0.

Conceptually this is straightforward. The problem is scale: once you start expanding this to daily granularity across a large number of items and long time ranges, the dataset explodes and becomes very memory-heavy.

Iâ€™m currently running this locally in python, reading from a PostgreSQL database. Once I have a decent working version, it will run in a container based environment.

I generally use pandas but I assume it might be time to transition to polars or something else ? I would have to convert back to pandas for the ML training though (library constraints)

Before I brute-force this, I wanted to ask:

â€¢	Are there established best practices for dealing with this kind of â€œmissing means zeroâ€ scenario?

â€¢	Do people typically materialize the full dense time series, or handle this more cleverly (sparse representations, model choice, feature engineering, etc.)?

â€¢	Any libraries / modeling approaches that avoid having to explicitly generate all those zero rows?

Iâ€™m curious how others handle this in production settings to limit memory usage and processing time.",1769509169.0,3,11,https://www.reddit.com/r/dataengineering/comments/1qoa34u/help_with_time_series_missing_values/
1qo864m,ClickHouse at PB Scale: Drawbacks and Gotchas,"Hey everyone:)

Iâ€™m evaluating whether ClickHouse is a good fit for our use case and would love some input from folks with real-world experience.

Context:

â€¢ \~1 PB of data each day

â€¢ Hourly ETL on top of the data (1peta/24)

â€¢ Primarily OLAP workloads

â€¢ Analysts run ad-hoc and dashboard queries

â€¢ Current stack: Redshift

â€¢ Data retention: \~1 month

From your experience, what are the main drawbacks or challenges of using ClickHouse at this scale and workload (ETL, operations, cost, reliability, schema evolution, etc.)?

Any lessons learned or â€œgotchasâ€ would be super helpful",1769502299.0,8,13,https://www.reddit.com/r/dataengineering/comments/1qo864m/clickhouse_at_pb_scale_drawbacks_and_gotchas/
1qo7q0y,Learning LLM and gen ai along with data engineering,"I'm working as a Azure Data Engineer with almost 1.9 YOE 
Now I started learning LLM and gen ai to see how can I use this and utilise this knowledge is changing data engineering role

Just had a doubt is this decision make sense and this will open up me for more opportunities and high pays in near future since combining both knowledge space?",1769500697.0,0,19,https://www.reddit.com/r/dataengineering/comments/1qo7q0y/learning_llm_and_gen_ai_along_with_data/
1qo6cgm,Is nifi good for excel ETL from sftp to sql and excel format stays same does not change.,So i am working on a project where i have to make a pipeline form sftp server to sql with excel reports with fixed format that comes every 5 min or hourly.,1769495849.0,3,8,https://www.reddit.com/r/dataengineering/comments/1qo6cgm/is_nifi_good_for_excel_etl_from_sftp_to_sql_and/
1qo516m,Data Quality Starts in Data Engineering,,1769491658.0,0,0,https://www.reddit.com/r/dataengineering/comments/1qo516m/data_quality_starts_in_data_engineering/
1qo49nt,How are you all building your python models?,"Whether theyâ€™re timeseries forecasting, credit risk, pricing, or whatever types of models/computational processes. Im interested to know how you all are writing your python models, like what frameworks are you using, or are you doing everything in notebook? Is it modularized functions or giant monolithic scripts?

  
Iâ€™m also particularly interested in anyone using dagster assets or apache Hamilton, especially if youâ€™re using the partitioning/parallelizable features of them, and how you like the ergonomics. ",1769489431.0,12,12,https://www.reddit.com/r/dataengineering/comments/1qo49nt/how_are_you_all_building_your_python_models/
1qo0c98,Importing data from s3 bucket.,"Hello everyone
I am loading a cover file from s3 into an amazon redshift table using copy. The file itself is ordered in s3. Example:
Col1 col2
A       B
1      4
A      C
F      G
R      T

However, after loading the data, the rows appear in a different order when I query the table, something like
Col1  Col2
1        4
A        C
A        B
R        T
F         G


There is not any primary key or sort key in the table or data in s3. And the  data very lage  has around 70000+ records. When I analysed, it is said due to parallel processing of redshift. Is there anything I could do to preserve the original order and import the data as it is?

Actually, the project I am working on is to mask the phi values from source table and after masking the masked file is generated in destination folder in s3.
Now, I have to test if each values in each column is masked or not. 
Ex: source file
Col1
John
Richard
Rahul
David
John

Destination file(masked)
Col1
Jsjsh
Sjjs
Rahul
David
Jsjsh

So, now  I have to import these two files source n destination  table if the values are masked or not.
Why I want in order? I am 
I am comparing the first value of col1 in source table with the first value of col1 in destination table. 
I want result, (these are the values that are not masked).

S.Col1   D.Col1
Rahul    Rahul
David    David


I could have tested this using join on s.col1=d.col2, but there could be values like
Sourcetable

Col1   
John 
David 
Leo

Destinatiotable
Col1
David
Djjd
Leo
Here, if I join I get the value that is masked, although David is masked as Djjd
S.col1   d.col1
David    David


EDIT:",1769478924.0,5,4,https://www.reddit.com/r/dataengineering/comments/1qo0c98/importing_data_from_s3_bucket/
1qnyuju,First DE job,"I am starting my first job as an entry level data engineer in a few months. The company I will be working for uses Azure Databricks. 

Any advice you could give someone just starting out? What would you focus on learning prior to day 1? What types of tasks were you assigned when you started out?",1769475065.0,8,11,https://www.reddit.com/r/dataengineering/comments/1qnyuju/first_de_job/
1qnysel,Where Are You From?,"I notice a lot of variability in the types of jobs people talk about based on location. I'm curious where people are from. I would've been more granular with Europe but the poll option doesn't allow more than 6 options.

[View Poll](https://www.reddit.com/poll/1qnysel)",1769474917.0,0,0,https://www.reddit.com/r/dataengineering/comments/1qnysel/where_are_you_from/
1qnueva,DE roles in big pharma : IT vs business-aligned,"

Hey everyone ,  I work as a  data engineer in pharma and Iâ€™m trying to understand how roles are structured at larger pharma companies like J&J, Abbvie, Novo,Novartis etc.

Iâ€™m interested in tech-heavy roles that are still closely tied to business teams (commercial, access, R&D, Finance, therapeutics areas) rather than purely centralized IT.

If anyone here works in data/analytics engineering at these companies or closely with these roles, Iâ€™d love to hear how your team is set up and what the day-to-day looks like. Mainly looking to learn and compare experiences.Iâ€™m also open to casual coffee chats or just exchanging experiences over DM as I explore a potential switch.

",1769464717.0,5,0,https://www.reddit.com/r/dataengineering/comments/1qnueva/de_roles_in_big_pharma_it_vs_businessaligned/
1qnqbqm,The Certifications Scam,"I wrote this because as a head of data engineering I see aload of data engineers who trade their time for vendor badges instead of technical intuition or real projects.

Data engineers lose the direction and fall for vendor marketing that creates a false sense of security where ""Architects"" are minted without ever facing a real-world OOM killer. And, Itâ€™s a win for HR departments looking for lazy filters and vendors looking for locked-in advocates, but it stalls actual engineering growth.

  
As a hiring manager half-baked personal projects matter way more than certification. Your way of working matters way more than the fact that you memoized the pricing page of a vendor.

So yeah, I'd love to hear from the community here:

\- Hiring managers, do ceritication matter?

\- Job seekers. have certificates really helped you find a job?",1769456059.0,139,83,https://www.reddit.com/r/dataengineering/comments/1qnqbqm/the_certifications_scam/
1qnorsn,How to improve ETL pipeline,"I run the data department for a small property insurance adjusting company.

Current ETL pipeline I designed looks like this (using an Azure VM running Windows 11 that runs 24/7):

1. Run \~50 SQL scripts that drop and reinsert tables & views via Python script at \~1 AM using Windows Task Scheduler. This is an on-premise SQL Server database I created so it is free, other than the initial license fee.
2. Refresh \~10 shared Excel reports at 2 AM via Python script using Windows Task Scheduler. Excel reports have queries that utilize the SQL tables and views. Staff rely on these reports to flag items they need to review or utilize for reconciliation.
3. Refresh \~40 Power BI reports via Power BI gateway on the same VM at \~3 AM. Same as Excel. Queries connect to my SQL database. Mix of staff and client reports that are again used for flags (staff) or reimbursement/analysis (clients).
4. Manually run Python script for weekly/monthly reports once I determine the data is clean. These scripts not only refreshes all queries across a hundred Excel reports but it also logs the script actions to a text file and emails me if there is an error running the script. Again, these reports are based on the SQL tables and views in my database.

I got my company to rent a VM so all these reports could be ready when everyone logs in in the morning. Budget is only about $500/month for ETL tools and I spend about $300 on renting the VM but everything else is minimal/free like Power BI/python/sql scripts running automatically. I run the VM 24/7 because we also have clients in London & the US connecting to these SQL views as well as running AdHoc reports during the day so we don't want to rely on putting this database on a computer that is not backed up and running 24/7.

Just not sure if there is a better ETL process that would be within the $500/month budget. Everyone talks about databricks, snowflake, dbt, etc. but I have a feeling since some of our system is so archaic I am going to have to run these Python and SQL scripts long-term as most modern architecture is designed for modern problems.

Everyone wants stuff in Excel on their computer so I had a hard enough time getting people to even use Power BI. Looks like I am stuck with Excel long-term with some end-users, whether they are clients or staff relying on my reports.",1769452933.0,19,22,https://www.reddit.com/r/dataengineering/comments/1qnorsn/how_to_improve_etl_pipeline/
1qnmyiz,BEST AI Newsletters?,"I've been mainly staying up to do date via youtube and podcasts (great for my daily walks) but I want to explore the current landscape of email newsletters for staying up to date with the AI space.

What are your favorite newsletter for staying up to date?

Asking here cause I mainly follow data engineering, so I want to know the newsletter other data engineers find useful.",1769449307.0,0,2,https://www.reddit.com/r/dataengineering/comments/1qnmyiz/best_ai_newsletters/
1qnlysg,How are you getting CDC into Iceberg?,"I've built CDC to Iceberg pipelines before (Debezium + Spark) and I'm curious, for anyone else running this in production, what are your biggest pain points? Is it deployment complexity? Operational overhead? Cost of managed solutions? ",1769447271.0,3,1,https://www.reddit.com/r/dataengineering/comments/1qnlysg/how_are_you_getting_cdc_into_iceberg/
1qnjx8b,Merging datasets with common keys,"Hi!

I've been tasked with merging two fairly large datasets. The issue is, that they don't have a single common key. Its auto data, specifically manufacturers and models of cars in Sweden for a marketplace.

The two datasets don't have a single common id between their datasets. But the vehicles should be present in both datasets. So things like the manufacturer will map 1:1 as its a smaller set. But the other fields like engine specifications and model namings vary. Sometimes a lot, but sometimes there are small tolerances like 0.5% on engine capacity. 

Previously they've had 'data analysts' creating mappings in a spreadsheet that then influences some typescript code to generate the links between them. Its super inefficient. I feel like there must be a better way to create a shared data model between them and merge them rather than attempting to join them. Maybe from the DS field. 

I've been an data engineer for a long time, this is the first I've seen something like this outside of medical data, which seems to be a bit easier. 

Any advice, strategies or software on how this could solved a better way? ",1769443065.0,1,5,https://www.reddit.com/r/dataengineering/comments/1qnjx8b/merging_datasets_with_common_keys/
1qnjeht,Is Spring Batch still relevant? Seeing it in my project but not on job boards,"â€™m currently working on a retail domain project that uses Spring Batch, Airflow, and Linux for our ETL (Extract, Transform, Load) pipelines.

However, when I search for ""Spring Batch"" on LinkedIn, I hardly see any job postings requiring it as a primary skill. This has me wondering: Is Spring Batch still widely used in the industry, or is it being phased out?",1769441954.0,3,1,https://www.reddit.com/r/dataengineering/comments/1qnjeht/is_spring_batch_still_relevant_seeing_it_in_my/
1qnjda0,Retrieve and Rerank: Personalized Search Without Leaving Postgres,,1769441884.0,1,0,https://www.reddit.com/r/dataengineering/comments/1qnjda0/retrieve_and_rerank_personalized_search_without/
1qnj3yv,cron update,"Hi,

On macOS what can the root that I updated my crontab with \`crontab -e\`, but the jobs that are executed does not change? Previously I added some env variables, but I donâ€™t get, why there is no action.

Thanks in advance!",1769441326.0,2,1,https://www.reddit.com/r/dataengineering/comments/1qnj3yv/cron_update/
1qnj202,OpenSheet: All in browser (local only) spreadsheet,"Hi! I'm trying to get some feedbacks on https://opensheet.app/. It's basically a spreadsheet with the core power of duckdb-wasm on the browser. I'm not trying to replace Excel or any formula heavy tool, its an experiment on how easy would it be to have the core power of sql and easy to use interface. I'd love to know what you think!",1769441210.0,1,0,https://www.reddit.com/r/dataengineering/comments/1qnj202/opensheet_all_in_browser_local_only_spreadsheet/
1qnispr,[Laid Off] Iâ€™m terrified. 4 years of experience but I feel like I know nothing.,"I was fired today (Data PM). Iâ€™m in total shock and I feel sick.

Because of constant restructuring (3 times in 1.5 years) and chaotic startup environments, I feel like I haven't actually learned the core skills of my job. Iâ€™ve just been winging it in unstructured backend teams for four years.

Now I have to find something again and I am petrified. I feel completely clueless about what a Data PM is actually supposed to do in a normal company. I feel unqualified.

Iâ€™m desperate. Can someone please, please help me understand how to prep for this role properly? I canâ€™t afford to be jobless for long and I donâ€™t know what to do.",1769440649.0,199,64,https://www.reddit.com/r/dataengineering/comments/1qnispr/laid_off_im_terrified_4_years_of_experience_but_i/
1qnh98r,Are there any analytics platform that also let you run custom executable functions?,For example something like Metabase but also gives you options to run custom executable functions in any language to get data from external APIs as well.,1769437143.0,3,0,https://www.reddit.com/r/dataengineering/comments/1qnh98r/are_there_any_analytics_platform_that_also_let/
1qnfgwo,Snowtree: Databend's Best Practices for AI-Native Development,"Snowtree codifies Databend Team's AI-native development workflow with isolated worktrees, line-by-line review, and native CLI integration.",1769432656.0,3,1,https://www.reddit.com/r/dataengineering/comments/1qnfgwo/snowtree_databends_best_practices_for_ainative/
1qnf69j,Built a new columnar storage system in C.,"Hi,i wanted to get rid of any abstraction and wanted to fetch data directly from disk,with this intuition i built a new columnar database in C,it has a new file format to store data.Zone-map pruning using min/max for each row group, includes SIMD.I ran a benchmark script against sqlite for 50k rows and got good metrics for simple where clauses scan. In future, i want to use direct memory access(DMA)/DPDK to skip all sys calls, and EBPF for observability. It also has a neural intent model (runs on CPU) inspired by BitNet that translates natural-language English queries into structured predicates. To maintain correctness, semantic operator classification is handled by the model while numeric extraction remains rule-based. It sends the output json to the storage engine method which then returns the resultant rows.

Github: [https://github.com/nightlog321/YodhaDB](https://github.com/nightlog321/YodhaDB)

This is a side project.

Give it a shot.Let me know what do you think!",1769431850.0,6,4,https://www.reddit.com/r/dataengineering/comments/1qnf69j/built_a_new_columnar_storage_system_in_c/
1qnf64h,What to learn next?,"I'm solid in traditional data modeling and getting pretty familiar with AWS and getting close to taking the DE cert. Now that I've filled that knowledge gap in debating on what's next. I'm deciding between DBT, snowflake or databricks? I'm pretty sure I'll need DBT regardless but wondering what people recommend. I do prefer visual based workflow orchestration, not sure if that comes into play at all. ",1769431838.0,11,6,https://www.reddit.com/r/dataengineering/comments/1qnf64h/what_to_learn_next/
1qnb2qk,DBT <-> Metabase Column Lineage VS Code extension,"We use dbt Cloud and Metabase at my company, and while Metabase is great, we've always had this annoying problem: it's hard to know which columns are actually being. This got even worse once we started doing more self-serve analytics.

So I built a super simple VSCode extension to solve this. It shows you which columns are being used and which Metabase questions they show up in. Now we know which columns we need to maintain and when we should be careful making changes.

I figured it might help other people too, so I decided to release it publicly as a little hobby project.

* Works with dbt Core, Fusion, and Cloud
* For Metabase, you'll need the serialization API enabled
* It works for native and SQL builder questions :) 

Would love to hear what you think if you end up trying it! Also happy to learn if you'd like me to build something similar for another BI tool.",1769418208.0,10,2,https://www.reddit.com/r/dataengineering/comments/1qnb2qk/dbt_metabase_column_lineage_vs_code_extension/
1qn92m3,How to move from mainframes to data engineering?,"I have 5+ years of experience in mainframe devlopment and modernization. During this time I was also involved in a project which was ETL using python primarily. 

Apart from this I also did ETL as part of modernization (simple stuff like cleaning legacy output, loading them to SQL server) and then readying that for PBI. I wonder if this would be enough for me to drift to a core data engineering career? 

I have done few projects on my own with Databricks, PSQL and a little bit of exposure on Azure Data Factory. ",1769411101.0,15,1,https://www.reddit.com/r/dataengineering/comments/1qn92m3/how_to_move_from_mainframes_to_data_engineering/
1qn8j0h,Ever had to clean up data after a â€œsafeâ€ SQL change?,"Iâ€™m not talking about disasters.

Just normal work:

\- UPDATE / DELETE with a WHERE

\- Backfills

\- Fixing bad records

Things that \*should\* be safe, but somehow still feel risky.

Iâ€™ve seen:

\- Manual backups before running SQL

\- People triple-checking queries

\- Teams banning direct DB writes entirely

Whatâ€™s your approach now?",1769409322.0,0,22,https://www.reddit.com/r/dataengineering/comments/1qn8j0h/ever_had_to_clean_up_data_after_a_safe_sql_change/
1qn37zp,MLB Data Engineer position - a joke?,I saw this job on LinkedIn for the MLB which for me would be a dream job since I group up playing and love baseball. However as you can see the job posting is for 23-30 per hour. Whatâ€™s the deal?,1769394005.0,115,61,https://www.reddit.com/r/dataengineering/comments/1qn37zp/mlb_data_engineer_position_a_joke/
1qn1vu8,When To Implement More Than One Data Warehouse,"I work for a healthcare organization with an existing data warehouse that stores client and medical/billing data. The corporate side now has a need to store finance and GL data.

In this scenario, is it more appropriate to stand up a separate warehouse to serve corporate data, or to use a federated model across domains? Given that these data sets will never be co-mingled, Iâ€™m leaning toward a separate warehouse, but Iâ€™d value input on best practice and trade-offs.

Additional Details: Data governance is relatively mature at this organization and architectural principles are in place to guide implementation and maintenance.

  
Edited: changed ""benefits/payroll data"" to ""GL data""",1769390476.0,5,11,https://www.reddit.com/r/dataengineering/comments/1qn1vu8/when_to_implement_more_than_one_data_warehouse/
1qn0e1i,"Darl: Incremental compute, scenario analysis, parallelization, static-ish typing, code replay & more","Hi everyone, I wanted to share a code execution framework/library that I recently published,Â Â called â€œdarlâ€.

**What my project does:**

Darl is a lightweight code execution framework that transparently provides incremental computations, caching, scenario/shock analysis, parallel/distributed execution and more. The code you write closely resembles standard python code with some structural conventions added to automatically unlock these abilities. Thereâ€™s too much to describe in just this post, so I ask that you check out the comprehensive README for a thorough description and explanation of all the features that I described above.

Darl only has python standard library dependencies. This library was not vibe-coded, every line and feature was thoughtfully considered and built on top a decade of experience in the quantitative modeling field. Darl is MIT licensed.

**Target Audience:**

The motivating use case for this library is computational modeling, so mainly data scientists/analysts/engineers, however the abilities provided by this library are broadly applicable across many different disciplines.

**Comparison**

The closest libraries to darl in look feel and functionality are fn\_graph (unmaintained) and Apache Hamilton (recently picked up by the apache foundation). However, darl offers several conveniences and capabilities over both, more of which are covered in the ""Alternatives"" section of the README.

**Quick Demo**

Here is a quick working snippet. This snippet on it's own doesn't describe much in terms of features (check our the README for that), it serves only to show the similarities between darl code and standard python code, however, these minor differences unlock powerful capabilities.

    from darl import Engine
    
    def Prediction(ngn, region):
    Â  Â Â model = ngn.FittedModel(region)
    Â  Â Â data = ngn.Data()Â  Â  Â  Â  Â  Â  Â  
    Â  Â Â ngn.collect()
    Â  Â Â return model + dataÂ  Â  Â  Â  Â  Â 
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
    def FittedModel(ngn, region):
    Â  Â Â data = ngn.Data()
    Â  Â Â ngn.collect()
    Â  Â Â adj = {'East': 0, 'West': 1}[region]
    Â  Â Â return data + 1 + adj Â  Â  Â  Â  Â  Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
    
    def Data(ngn):
    Â  Â Â return 1Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
    
    ngn = Engine.create([Prediction, FittedModel, Data])
    ngn.Prediction('West')Â Â # -> 4
    
    def FittedRandomForestModel(ngn, region):
    Â  Â Â data = ngn.Data()
    Â  Â Â ngn.collect()
    Â  Â Â return data + 99
    
    ngn2 = ngn.update({'FittedModel': FittedRandomForestModel})
    ngn2.Prediction('West')Â Â # -> 101Â Â # call to `Data` pulled from cache since not affectedÂ 
    
    ngn.Prediction('West')Â Â # -> 4Â Â # Pulled from cache, not rerun
    ngn.trace().from_cacheÂ Â # -> True",1769386644.0,3,0,https://www.reddit.com/r/dataengineering/comments/1qn0e1i/darl_incremental_compute_scenario_analysis/
1qmvc79,Cloud storage with a folder structure like on a phone,"First of all, I apologize for my English. My question is what kind of cloud storage is available so that when copying to the storage, the folder structure is saved as on the phone. I have an android",1769374973.0,0,0,https://www.reddit.com/r/dataengineering/comments/1qmvc79/cloud_storage_with_a_folder_structure_like_on_a/
1qmu3hh,I wanna make a data injector & schema orchestrator platform. Is it a good idea?,"I have come across websites such as [https://nifi.apache.org/](https://nifi.apache.org/) & [https://airbyte.com/](https://airbyte.com/) which pretty much try to do the same thing.  
But i want to create a simple, go-based, cli based data orchestrator. A backend that accepts untrusted, massive data; validates it, normalizes it, and safely injects it into any supported datastore while keeping the client informed.

I wanna make it open-source and completely free. Is it a good idea?? 

Would love to have suggestions if anything unique can be made to make this product stand out! ;)

first time here!!",1769372238.0,0,7,https://www.reddit.com/r/dataengineering/comments/1qmu3hh/i_wanna_make_a_data_injector_schema_orchestrator/
1qmseer,Looking for volunteers to try out a new CDC-based tool for tracking DB changes,"Hey all, I have recently started building a tool for time-travelling through DB changes based on Kafka and Debezium. I have written a full blog post providing details on what the tool is and the problems it solves as well as the architecture of the tool on a high-level. Feel free to have a read here - [https://blog.teonibyte.com/introducing-lambdora-a-tool-for-time-traveling-through-your-data](https://blog.teonibyte.com/introducing-lambdora-a-tool-for-time-traveling-through-your-data) The blog post also includes demo videos of the tool itself. 

  
At this stage I am looking for any feedback I can get as well as volunteers to try out the tool in their own projects or work environments for free. I will be happy to provide support on setting up the tool. In case this looks interesting to you, please do reach out!   ",1769368579.0,0,3,https://www.reddit.com/r/dataengineering/comments/1qmseer/looking_for_volunteers_to_try_out_a_new_cdcbased/
1qmqfw7,Why would I use DBT when Microsoft Fabrics exists ?,"Hello everyone,

I am a Analytics Engineer/PowerBI Consultant/Whatever-You-Call-It . I do all my ETL through dataflows, Power Query and SQL. I'm seeking to upgrade my data stack, maybe move into a data engineering role.

I have been looking into DBT, since it seems to be a very useful transformation tool, and kind of the new standard in the modern data stack. However I can't help to think that datasets/dataflows and the other tools in the Fabrics ecosystem already adress all the issues DBT solves.

So my question is : Is it relevant to learn DBT coming from Power BI ? Or should I focus on learning Fabrics first ?

Thank you.

\- A man looking to explore new horizons.

EDIT: Please don't give in to temptation to share your sunday evenning bad mood, it's really not needed. I'm just a mere human looking for simple info. Good for you if you are a superior omniscient being :)",1769364404.0,0,48,https://www.reddit.com/r/dataengineering/comments/1qmqfw7/why_would_i_use_dbt_when_microsoft_fabrics_exists/
1qmmenb,AWS Solutions Architect Associate,"I have 3 years of experience in data engineering and have not done any AWS fundamental certification before, should I directly go for Solutions Architect? I checked the syllabus and it's quite intimidating. 

  
FYI, I have the Azure DP900 and Snowflake SnowPRo Core certifications.",1769355684.0,13,12,https://www.reddit.com/r/dataengineering/comments/1qmmenb/aws_solutions_architect_associate/
1qmlfg6,How did you guys get data modeling experience?,"Hey y'all! So as the title suggests, I'm kind of curious how everyone managed to get proper hands on experience with data modeling 

From my own experience and from some of the discussion threads, it seems like the common denominator and a lot of companies is ship first, model later 

I'm curious if any of you guys stuck around long enough for the model later part to come around, or how you managed to get some mentorship or at least hands-on projects early in your career where you got to sit down and actually design a data model and implement it 

I've read Kimball and plan to read more, and try to do as much as I can to sort of model things where I'm at, but with everything always being urgent you have to compromise. So I'm curious how it went for everyone throughout their careers",1769353464.0,89,78,https://www.reddit.com/r/dataengineering/comments/1qmlfg6/how_did_you_guys_get_data_modeling_experience/
1qml84c,Near real-time data processing / feature engineering tools,"What are the popular or tried and true tools for processing streams of kafka events?

I have a real-time application where I need to pre-compute features for a basic ML model. Currently I'm using flink to process the kafka events and push the values to redis, but the development process is a pain. Replicating data lake sql queries into production flink code is annoying and can be tricky to get right. I'm wondering, are there any better tools on the market to do this? Maybe my flink development set up is bad right now? I'm new to the tool. Thanks everyone.",1769353002.0,10,5,https://www.reddit.com/r/dataengineering/comments/1qml84c/near_realtime_data_processing_feature_engineering/
1qmkh3e,Need suggestions for version control for our set up,"Hi there,

Our is MS Sql based ware house and all the transformations and ingestions happen through packages and T-sql jobs. We use SSIS and SSMS.

We want to implement version control for the codes that are being used in these jobs. Could someone here please suggest the best tool that can be leveraged here and the process of doing it.

Going forward after this we want to implement CI CD process as well.

Thanks in Advance.

(We also got a Development server recently, so we need to sync the Prod Server with the Development server).",1769351189.0,6,4,https://www.reddit.com/r/dataengineering/comments/1qmkh3e/need_suggestions_for_version_control_for_our_set/
1qmheqn,"Survey-heavy analytics engineer trying to move into commercial roles, can you please review my dbt Snowflake project.","As the title says, Iâ€™m trying to move from NGO / survey-heavy analytics work into a more commercial analytics engineering role, and Iâ€™d really value honest feedback on what I should improve to make that transition smoother.


A few people have asked me what I actually did day-to-day in a survey-heavy AE setting, so I built this project to make that work visible.

In practice, itâ€™s been a mix of running KPI definition sessions with programme teams, writing and maintaining a data contract, then encoding those rules in dbt across staging, intermediate and marts. Iâ€™ve focused heavily on data quality: DQ flags, quarantine patterns for bad rows, repeatable tests, and monitoring tables (including late-arrival tracking).

I also wired in CI on PRs and automated docs publishing on merge, so changes are reviewable and the project stays easy to navigate.

This week Iâ€™m extending the pipeline â€œupstreamâ€: pulling from Kobo servers to S3, then using SNS + SQS to trigger Snowpipe so RAW loads happen event-based.

Thanks in advance for any feedback and genuinely, thank you to everyone whoâ€™s helped me along the way so far. Iâ€™ve learned a lot from this community and really appreciate it.",1769342722.0,2,2,https://www.reddit.com/r/dataengineering/comments/1qmheqn/surveyheavy_analytics_engineer_trying_to_move/
1qmgzam,Is multidimensional data query still relevant today? (and Microsoft SQL Server Analysis Services),"Coming into the data engineering world fairly recently. Microsoft SQL Server Analysis Services (SSAS) offers multidimensional data query for easier slice-and-dice analytics. To run such query, unlike SQL that most people know about, you will need to write MDX (Multidimensional Expressions).

Many popular BI platforms, such as Power BI, Azure Analysis Services, seem to be the alternatives that replace SSAS. Yet they don't support multidimensional mode. Only tabular mode is available.

Even all by Microsoft, is multidimensional data modeling getting retired? (and so with the concept of 'cube'?)",1769341322.0,7,11,https://www.reddit.com/r/dataengineering/comments/1qmgzam/is_multidimensional_data_query_still_relevant/
1qmgqpn,"[Learning Project] Crypto data platform with Rust, Airflow, dbt & Kafka - feedback welcome","https://preview.redd.it/zustnm8hdhfg1.png?width=1656&format=png&auto=webp&s=e6e6018f2b31ee67158047b278e89c115227d1cf

Built a data platform template to learn data engineering (inspired by an AWS course with Joe Reis):   
  
\- Dual ingestion: Batch (CSV) or Real-time (Kafka)   
\- Rust for fast data ingestion - Airflow + dbt + PostgreSQL   
\- Medallion architecture (Bronze/Silver/Gold)   
\- Full CI/CD with tests GitHub: [https://github.com/gregadc/cookiecutter-data-platform](https://github.com/gregadc/cookiecutter-data-platform)   
  
Looking for feedback on architecture and best practices I might be missing!  
",1769340519.0,9,6,https://www.reddit.com/r/dataengineering/comments/1qmgqpn/learning_project_crypto_data_platform_with_rust/
1qmfyr1,Pandas 3.0 vs pandas 1.0 what's the difference?,"hey guys, I never really migrated from 1 to 2 either as all the code didn't work. now open to writing new stuff in pandas 3.0. What's the practical difference over pandas 1 in pandas 3.0? Is the performance boosts anything major? I work with large dfs often 20m+ and have lot of ram. 256gb+. 

Also, on another note I have never used polars. Is it good and just better than pandas even with pandas 3.0. and can handle most of what pandas does? So maybe instead of going from pandas 1 to pandas 3 I can just jump straight to polars? 

I read somewhere it has worse gis support. I do work with geopandas often. Not sure if it's gonna be a problem. Let me know what you guys think. thanks.",1769337872.0,47,38,https://www.reddit.com/r/dataengineering/comments/1qmfyr1/pandas_30_vs_pandas_10_whats_the_difference/
1qma558,Right way to use dlthub for extracting to target Postgres,"Currently I extract data from excel files using dlthub to raw layer of my data warehouse. All rows are extracted to a single json column in raw layer, and unpacked later using dbt. Is this the best way to do this? Or should I let dlt just unpack the columns to raw layer? Anyone has experience with this and the pros and cons? I understand that if I do this I stop dlthub from handling schema drift automatically.",1769318223.0,5,2,https://www.reddit.com/r/dataengineering/comments/1qma558/right_way_to_use_dlthub_for_extracting_to_target/
1qm4vul,Built a CSV to SQL converter that validates data - feedback from data engineers?,"Working data engineer here. Got tired of CSV imports corrupting data at work.

Decided to build a tool that validates your CSV before generating SQL:

\- Catches ZIP codes losing leading zeros

\- Finds invalid dates before they crash imports

\- Detects mixed types

\- 7 validation checks total

Supports PostgreSQL, MySQL, SQL Server, SQLite, Oracle.

Give it a try: [CSV-to-SQL-Tool](https://csv-to-sql-converter-tau.vercel.app/)



Looking for feedback from people who actually deal with this. What validations am I missing? Any suggestions on what features to add? ",1769303391.0,0,6,https://www.reddit.com/r/dataengineering/comments/1qm4vul/built_a_csv_to_sql_converter_that_validates_data/
1qm3nnr,Upskilling beyond SQL,"
Iâ€™ve been working with SQL Server for about 10 years, initially in more analytical roles, then over time into more ETL development and now ended up in a data engineer role but aware I need to broaden my technical skills to do a better job and open up more opportunities.

I can spare about half a day a week to focus on training, and have a limited budget. I would love some pointers about where to focus my efforts, as well as any courses or tutorials youâ€™d recommend. 

I have a strong grounding in SQL/ database fundamentals such as advanced SQL queries, data models like data warehouse and data vault, testing & validation, error handling and alerts, logging, parameterising and performance tuningâ€¦but all within the context of SQL Server.

Iâ€™m confident with CI/CD in terms of developing SQL Projects with VS Code integrated with GitHub.

Iâ€™ve used and amended Python scripts for ingesting data from APIs and scraping from web pages but am not confident with Python generally.

Some of the things Iâ€™m aware of that I think would be useful are:
 - Python fundamentals
 - Data bricks
- Handling JSON
 - Docker
 - Azure cloud tooling (eg data factory)/ other cloud platforms
 - Orchestration and workflow tools
 - Other databases (not SQL Server)
 - Using APIs for advanced queries
 - Spark

Where should I start?
What am I missing? 
What resources have you found useful?
",1769300219.0,36,15,https://www.reddit.com/r/dataengineering/comments/1qm3nnr/upskilling_beyond_sql/
1qlxojt,"Looking for feedback after 5 years building a data stack for .NET (dataframe, columnar storage, connectors)","Most of you work with Python and SQL exclusively, but thatâ€™s okay your feedback is still important. Almost 10 years ago I started working for a .NET embedded BI reporting tool. This kind of tool typically sits after ETL process, however a lot of  businesses donâ€™t follow that and simply want ETL with their dashboards all in one. The problem with that is thereâ€™s a huge gap in .NET for that functionality. Since that wasnâ€™t the product direction, 5 years ago I started letting out that frustration by starting to code a framework that was extendable with connectors, sinks, and at the center of it was an in-memory analytical database.

Over the years I continued to work on it off and on, finally to the point where I am now. I have several connectors, an in-memory analytical database (DataBlock), my own columnar file format and engine that I nicknamed Velocity (VelocityDataBlock), ML (VectorBlock), and other various libraries for UI (UIBlock). The last two havenâ€™t been publicly exposed yet. Hereâ€™s some code snippets of how the code looks.

### DataBlock

    var processed = await DataBlock.Connector.LoadCsvAsync(""raw_data.csv"")
        .Select(""id"", ""name"", ""value"", ""category"")
        .Where(""value"", 0, ComparisonOperator.GreaterThan)
        .Compute(""processed_value"", ""value * 1.1"")
        .Sort(SortDirection.Descending, ""processed_value"")
        .Head(1000);

### VelocityDataBlock

The snippet below materializes a DataBlock after Execute() is called.

    var topCategories = velocityBlock
        .Where(""Year"", 2024)
        .Pivot(""Category"", ""Region"", ""Sales"", AggregationType.Sum)
        .Sort(SortDirection.Descending, ""East_Sales"")
        .Head(5)
        .Execute();

Itâ€™s also possible to do something similar to what Polars does by using AsResult().

    var result = velocityBlock
        .Where(""OrderDate"", DateTime.Today.AddYears(-1), ComparisonOperator.GreaterThan)
        .AsResult();
    
    // Stream result rows
    long totalRevenue = 0;
    foreach (var row in result.EnumerateRows())
    {
        totalRevenue += row.GetValue<long>(""Revenue"");
    }
    
    // Materialize the result
    var data = result.ToDataBlock();

### Sinks

PDF export of a DataBlock.

    var pdfSink = new PdfSink
    {
        Title = ""Data Export Report"",
        Author = ""Datafication System"",
        Description = ""Automated data extraction from web sources"",
        RowLimit = 1000,
        LandscapeOrientation = true
    };
    
    using var pdfStream = await pdfSink.Transform(dataBlock);
    using var fileStream = File.Create(""report.pdf"");
    await pdfStream.CopyToAsync(fileStream);

Many of the connectors also have their sink output too, for example CSV.

    using Datafication.Sinks.Connectors.CsvConnector;
    var csvOutput = dataBlock.CsvStringSink();

Thereâ€™s honestly too much I could write about, but before I ask the feedback questions, Iâ€™ll throw in that the VelocityDataBlock can typically achieve 60M rows/sec on my 2020 iMac. However, Iâ€™ve learned that .NET isnâ€™t optimized the best on MacOS and if you try on Windows (which Iâ€™ve verified) you can easily get 100M rows a second. At some point Iâ€™ll put up a benchmark and the results. If youâ€™re curious to test on your end, try the QueryPerformance sample from the Datafication.Storage.Velocity repo. Use â€œdotnet run -c Release sf50â€ for testing with 50M rows of data.

### Feedback Questions

1. In a data world predominantly using Python and tools like dbt is there a place for an SDK like this?
2. Was the syntax easy to understand based on the tools youâ€™ve used?
3. Was there any functionality that you frequently use that wasnâ€™t available?

If you have any other thoughts or questions, let me know.

## GitHub Organization

[https://github.com/DataficationSDK](https://github.com/DataficationSDK)",1769285960.0,7,0,https://www.reddit.com/r/dataengineering/comments/1qlxojt/looking_for_feedback_after_5_years_building_a/
1qlv4el,Project Help,"Hi! Im working on a project on gcp which is fetching data via cloud run function, pushing it to pubsub which sends to dataflow and using the job builder i used a sql merge with a csv to enroch the data and eventually it will be in bigquery.

However right now the pipeline isng working and i suspect its smth to so with pubsub. When i run the function once, and run a pull on my subscriptipn, it shows the data which is unacknowledged. When i send the data again ans run a pull, the new messages dont appear. However if i manually key in a message and pull, it appears. 

How do i solve this, thanks!",1769280272.0,4,1,https://www.reddit.com/r/dataengineering/comments/1qlv4el/project_help/
1qlv3bm,Stakeholders Overengineering Solutions,"So translating stakeholder needs into specs into solutions is obviously a big part of the job. One specific aspect of this that I've been struggling with lately is that at least in our organization, there's a tendency for the stakeholders to try and directly give us what the solution should be, and it tends to be insanely complex. I often feel like it would be easier to just listen and understand the problem, propose a solution with a mockup or simplified prototype, and go from there.

The 'higher ups' like VPs and C-suite tend to be the ones who send us very complex requirements that look pretty AI-generated. It feels like it's mostly driven by them not wanting to spend time discussing or going back and forth with us. Does anyone else deal with this and if so how have you handled it?",1769280206.0,5,12,https://www.reddit.com/r/dataengineering/comments/1qlv3bm/stakeholders_overengineering_solutions/
1qltru0,Is there more to DE than this? Are their jobs out there for feeling like you actually matter?,"I'm pretty sure this isn't the norm, but I feel so exhausted with my job. The main problem is my team is not good and the company, while overall is very good/well regarded globally/competitive, is very pretentious. 

My job is not very challenging. In fact, it's the mundanity of it that exhausts me. Everything is the same over and over to process client data. There's almost 0 feedback/appreciation for it too, and I'm pretty sure the successor of the data basically passes off my work to others and pretends he did it. I of course could push myself more, learn new tech, and integrate it. But I just don't feel motivated. I still plan to do some online free courses about AI/ML soon.

But longterm, I'm not sure this job will sustain me. I have a noticeably worse mindsets (become a bit egotistical and negative myself, snarky, etc.). 

I'm in my early 30s, but about 10 years ago I switched from pre-medicine to tech. I was heavily invested in the the med route and had already applied to medical school. Then I spent too much time around some people I shouldn't have, and changed paths. Looking back, I probably wish I stayed in healthcare, though it has its own challenges and of course worse lifestyle usually. That said, there's no way I would want to switch back now.

All that to say, for so much of my life, I was looking at a career that directly helped people...so maybe that is part of my problem now. I could very well be in a 1/3 life crisis now, but overall okay (so dont worry :)). 

I'm debating a small to significant career change.

I think at the least, to save my sanity, I absolutely need to change jobs in the next year or two, if even that. Otherwise, I fear what my mind and personality will become. 

Things I've considered:
- online courses (for sure will do in the interim)

- a second masters (in tech related or business related field - my undergrad was economics, which I enjoyed)

- no further education (besides online stuff), but just change jobs

- stay at current job and do bare minimum BUT try to create a startup on the side (one big goal of mine is to be an entrepreneur some day).

Anyways, I digress. One reason for this post is because (1) I want to know if others feel this way in DE and (2) I am wondering if there are more rewarding industries out there. Something I've always been interested in is sustainability or in general, somehow making the world a better place. As cliche as it sounds, it'd be awesome to use my skills to improve the planet/humanity. I'm still looking at jobs and applying, but wanted to see if anyone here has recommendations. 


My fear is that whatever company I transition to will just be more of the same. I've been at a number of companies and in different job roles over the years, but there has not been a single one that I just loved (or even half loved). Everything felt like a cog in the wheel, like it doesn't help humanity, and just puts money in someone else's pocket.

Curious to hear if others feel this and if this is just the way of life. Or if there is something more out there that just needs to be found.",1769277340.0,29,47,https://www.reddit.com/r/dataengineering/comments/1qltru0/is_there_more_to_de_than_this_are_their_jobs_out/
1qlt6fk,"Automatically deriving data model metadata from source code (no runtime data), has anyone done this?","Hi all,

Iâ€™m looking for prior art, tools, or experiences around deriving structured metadata about data models purely from source code, without access to actual input/output data.

Concretely: imagine you have source code (functions, type declarations, assertions, library calls, etc.), but you cannot execute it and donâ€™t see real datasets. Still, youâ€™d like to extract as much structured information as possible about the data being processed, e.g.:

â€¢	data types (scalar, array, table, dataframe, tensor, â€¦)

â€¢	shapes / dimensions (where inferable)

â€¢	constraints (ranges, required fields, checks in code)

â€¢	formats (CSV, JSON, NetCDF, pandas, etc.)

â€¢	input vs output roles

A rough mental model is something like the RStudio environment pane (showing object types, dimensions, ranges), but inferred statically from code only.

Iâ€™m aware this will always be partial and heuristic, the goal is best-effort structured metadata (e.g. JSON), not perfect reconstruction.

My question:

Have you seen frameworks, pipelines, or research/tools that tackle this kind of problem?

(e.g. static analysis, AST-based approaches, schema inference, type systems, code-to-metadata, etc.)

I have worked so far asking code authors to annotate their interface functions using the python typing.annotated framework, but I want to start taking as much documentation work of them as possible.

I know itâ€™s mostly a crystal sphere task.

For deduktive reasoning, llms are also possible as parts of the pipeline.

Language-agnostic answers welcome (Python/R/Julia/C++/â€¦), as are pointers to papers, tools, or even â€œthis is a bad idea because Xâ€ takes.",1769276028.0,5,9,https://www.reddit.com/r/dataengineering/comments/1qlt6fk/automatically_deriving_data_model_metadata_from/
1qlsox2,Working from Asia making European wage,"I am in Asia, my goal is to work remotely for a European company while earning a European wage. I'd love to hear from anyone who is already doing this.

A bit about my background:

Â· I have almost 2 years of experience as a data engineer consultant.

Â· My core tech stack is Snowflake, Databricks, and Informatica.

Â· I've already worked with clients globally in my current role.

Questions:

1. Is this dream realistic?

2. What should I focus on? With my specific skills and experience level, where should I be looking and how should I position myself?

Any success stories, or words of caution would be incredibly helpful.",1769274938.0,0,5,https://www.reddit.com/r/dataengineering/comments/1qlsox2/working_from_asia_making_european_wage/
1qls0kd,Roast my junior data engineer onboarding repo,"Just want a sanity check if this is the good foundation for the company.

[https://github.com/dheerapat/pg-sqlmesh-metabase-bi](https://github.com/dheerapat/pg-sqlmesh-metabase-bi)

",1769273454.0,5,11,https://www.reddit.com/r/dataengineering/comments/1qls0kd/roast_my_junior_data_engineer_onboarding_repo/
1qlrhw7,Icebase: PostgreSQL-Stored Iceberg Metadata,"Hi!  I forked this project from [https://github.com/udaysagar2177/apache-iceberg-fileio](https://github.com/udaysagar2177/apache-iceberg-fileio) in order to make it delegate between database and object store for data/metadata, as well as use reflection for use within other query engines outside of the java api.

While this isn't as good of a spec for database-metadata as something like ducklake would be, there are still many intrinsic benefits to avoiding object store for these many small writes.

  
See the repo: [https://github.com/jordepic/icebase](https://github.com/jordepic/icebase)",1769272261.0,1,0,https://www.reddit.com/r/dataengineering/comments/1qlrhw7/icebase_postgresqlstored_iceberg_metadata/
1qlqhic,Azure Data Factory,"Need to move 200,000 records on a monthly basis out of dataverse into SQL. I currently use ADF copy activity for this.

There is then some validation etc.

Once completed I need to update the same dataverse records with the same data. 

Best way to do this? It needs  to be robust (retry no failures), performant, scalable.

ADF has the upsetting in the copy activity, but should a record not exist it will create one..(not that this should happen). Also I assume it would do this in a per record basis (not batch) so risk throttling / service limits for dataverse.

Alternate thoughts, send to msg queue in batches and have function app process using $batch.

Thoughts please?",1769269984.0,2,9,https://www.reddit.com/r/dataengineering/comments/1qlqhic/azure_data_factory/
1qlosqx,Etl pipelines testing using Python,"I need to find a course of Etl pipeline testing using Python. Common ETL Test Scenarios like Schema Validation, Duplicate Checks, NULL Checks, Data Completeness. Where can I find a course where I can learn that stuff ? I need to study asap =)",1769265979.0,10,4,https://www.reddit.com/r/dataengineering/comments/1qlosqx/etl_pipelines_testing_using_python/
1qllhix,Filemaker to Postgres Best Practise,"Hey hey fellow data engineers,

i just started a new job some months ago for a company which has a pretty wild 'organically grown' data landscape to put it mildly. The two main data storages are Filemaker applications built by non-IT consultants a while ago. They got their tweaks but for the most part serve the purpose. I was hired to kind of consolidate the data into future structure, to connect it and prepare the introduction of more serious data analysis aside of crappy excel exports.

As Filemaker JDBC routine and possibilities are rather limited and slow i wish to pull together both databases into respective schemes in one postgres db via python as this is my most convenient set-up. Therefore i got my own server with a bit of space. 

I've written some first scripts and tests and for the smaller one of the two databases this works pretty good and im quite satisfied with the result.

The problems start to occur with scaling as the other database is significantly larger and my (admittedly not very storage efficient) code would take way too long and consume way to much storage in the target structure, but as a one man DE team i just don't have sufficient time to plan a highly efficient, totally aligning target structure for the existing dbs.

So my question is if anybody has some best practises, ideas, experience, tips on how to tackle this problem. I am not fixed to the idea of that postgres db. The main goal would be to be able to systematically analyse that Filemaker DBs via SQL like queries (Joins, CTEs, Views, sophisticated reportings) and also to be able to connect the data of the two DBs. Right now i can satisfy basic analysis requirements via customly built python scripts which connect to both db's via JDBC but this is rather time consuming and far from the ideal state in my head. 

Thanks for your ideas!

  
tl;dr: I need to systematically connect and analyse two rather big Filemaker DBs with the goal of unifying them at some point in the future, ideally via python into a postgres db and im in need for tips, best practises, hints.",1769256905.0,4,4,https://www.reddit.com/r/dataengineering/comments/1qllhix/filemaker_to_postgres_best_practise/
1qlkijq,Upskill Career Advice,"Hello everybody,

Guys let me describe my situation. I'm unemployed since 1 month. I have accumulated resources to persist for 12 months being unemployed.


I'm seeking to confront my career upskill idea with real data people not only Gemini and Perplexity.
My destination market is Central European market.


I have 2 years of IT support, 1 year Elastic Stack ETL dev, 1 year Azure pipelines dev experience.
My skills are strongest within Linux, SQL, Python, Kubernetes, ELK, Azure.

My plan for 8 months:
* Az-104 - I have already learned 70%
* Databricks certified data engineer
* Certified Kubernetes Administrator - I have already learned 90%
* DP-300

Additionaly I have on-prem k8s cluster on top of Proxmox, and would like run following for hands on fee-less experience:
- Kibana k8s already have
- Grafana k8s
- ArgoCD k8s already have

- Wireguard lxc
- Bind9 lxc
- Postfix lxc
- Router vm

- Minio lxc
- Metabase lxc
- Apache hive metastore k8s
- DeltaLake lxc
- Airflow k8s
- Spark k8s
- Dbt
- Elasticsearch vm already have
- Sql server vm
- Prometheus vm already have

I would like to build and operate real world data architecture and real world ETL, ELT with Airflow.
After all I would like to be perceived as junior Data Platform Engineer.

Guys I would be grateful if you review/comment on my plan/give suggestions as you have more real world experience within this area.

My concerns are:
- is this feasible?
- are technologies and certs in synergy?
- isn't it overkill?
- is this enough to get the job?
- am I trying to bark too many trees?
- isn't it too niche?
- shouldn't I narrow the scope?

Kind regards
",1769253577.0,5,3,https://www.reddit.com/r/dataengineering/comments/1qlkijq/upskill_career_advice/
1qli8ca,Python topics for Data engineer,"Currently I'm learning data engineer tools spark, hadoop, sqoop and all. I'm confused which topics should we cover in python for Data engineering.

Need suggestions which python topics should I learn for this ",1769245388.0,24,19,https://www.reddit.com/r/dataengineering/comments/1qli8ca/python_topics_for_data_engineer/
1qlf7l8,Are there opportunities for Entry Level DE's in India?,"Well I see a lot of companies do have openings for DE's, but none for freshers. Can we (freshers) actually get into this field? 

Also need a no fluff skills required for this role. Some say you need deep understanding of all the things and others are like tools are sorted, so are you. Please help",1769234891.0,0,5,https://www.reddit.com/r/dataengineering/comments/1qlf7l8/are_there_opportunities_for_entry_level_des_in/
1qleiv1,Where Could I make networking data engineer,"I'm looking for experience as a data engineer, I know duckdb, polar, dataflow with python, and local AI Ollama to make automation, however I only made works with process data of Bank statements, I need more experience. Where Could I find communities of data engineer?",1769232723.0,3,2,https://www.reddit.com/r/dataengineering/comments/1qleiv1/where_could_i_make_networking_data_engineer/
1qld1id,Help me not try to solve everything,Got my first DE role out of school. I've noticed that for some of our A/B testing the analysts seem like they basically are just eyeballing results and comparing general trends. There's no real statistical comparison or analysis of revenue differences or churn as far as I can tell. I have a pretty good idea of how this could be improved both on a process level and on an analysis level but I obviously a) don't want to step on anyone's toes b) take on more ownership of work I'm not being paid for c) inevitably get blamed if something random happens further down the line. I know it could make a pretty big difference but maybe I'm just caring too much and should funnel that energy elsewhere for my own personal projects? I guess I'm hoping that maybe some more disgruntled senior DEs can talk some sense into me or impart some words of wisdom. Thanks for reading!,1769228317.0,10,11,https://www.reddit.com/r/dataengineering/comments/1qld1id/help_me_not_try_to_solve_everything/
1ql5y9s,What conferences do you all recommend?,"I am looking at conferences for the year and finding a LOT of high level AI ones that don't look amazing for someone who does development. Everything is do is on prem, so I try to avoid full cloud company focused conferences.

Please let me know what conferences you recommend and what you liked or didn't like.",1769209420.0,7,3,https://www.reddit.com/r/dataengineering/comments/1ql5y9s/what_conferences_do_you_all_recommend/
1ql5s1b,"Stuck in jupyter notebooks, how to get out?","Hey guys. I work at a small company and joined a data science team and started writing etl stuff in jupyter notebooks in jupyterlab. and then later even as the project grew I kept writing parameterised notebooks and ran them with papermill. But it's starting to get absurd and I think isn't really common practice at all. But I am not sure how to get out of this habit. 

I write data science style procedural code and like to inspect and muck with stuff every step along the way otherwise I feel blind. It feels as I am in a live debugger. Even when I write an api, I have to take that function in a jupyter notebook and run it there and copy paste and go back and forth etc. 

I personally dislike functions as well unless neccesary and there's reuse required. 

I am not sure what to do but seems like in real ides there's tools like debuggers and interactive window which helps with this. But just wanted to learn from others how can I write clean software style code where I can still not lose visibility of the data. Thanks guys.",1769209009.0,45,36,https://www.reddit.com/r/dataengineering/comments/1ql5s1b/stuck_in_jupyter_notebooks_how_to_get_out/
1ql54ba,A new tool for data engineering,"I am working as a data engineer for a hospital and most of our work is create data pipelines and maintain our data warehouse. I spend 90% of my time working in Airflow or SQL. Other than that we use open metadata as well. 

Now, my manager has mentioned that one of my goal for this year should be introducing a new tool which can help us in our work, it can be anything. I have looked at DBT and Iâ€™m not sure if itâ€™ll be much useful to us. Can you guys mention the tools you use often in data engineering work or recommend some tools that I should research?

Thank you.",1769207434.0,21,28,https://www.reddit.com/r/dataengineering/comments/1ql54ba/a_new_tool_for_data_engineering/
1ql3n7t,New Grad market for DE,"Hi all, I am an undergrad CS student contemplating taking a switch to data engineering by taking a data engineering internship over a general SWE internship for the junior year summer.

  
I am slightly worried that it seems like the new grad market is not so friendly for DE, as seen by the lack of ""new grad"" data engineer roles compared to Software engineer roles.

  
If anyone has recruited for new grad DE roles or knows about the market for new grads please give me some advice. I feel that coming out of college straight as a data engineer is not a path many take - I am wondering if it's because it's difficult to do so or some other reasons.

",1769203915.0,1,8,https://www.reddit.com/r/dataengineering/comments/1ql3n7t/new_grad_market_for_de/
1ql0r1w,Certs or tools? What should I learn next as a mid level DE?,"Iâ€™m trying to decide what to learn next to make myself more competitive in my job search and would love some feedback. After \~5 years of professional experience, I think there are two main areas where my background is weaker than what a lot of current data engineering roles expect:

1. Cloud â€“ I have some foundational certs in Snowflake and Azure, but no real hands on professional cloud experience. My previous roles were mostly on-prem.

2. Common industry standard tools â€“ Things like Spark, Airflow, and dbt, which show up constantly in job descriptions.

Iâ€™m looking at a couple of learning paths that would be pretty time-intensive, so Iâ€™m trying to pick what will give me the most ROI. Right now Iâ€™m debating between:

1. Going deeper on cloud with a data engineering focused cert (leaning toward the AWS Data Engineer cert to diversify beyond Azure/Snowflake).

2. Spending time learning Spark and Airflow (or similar other tools) and building a realistic ETL pipeline I can put in a public repoâ€”possibly even deploying it in the cloud with a real cluster as second step.

For a bit more context: Iâ€™m targeting mid level IC roles. Iâ€™m confident in my Python and SQL and feel good on data fundamentals (currently reading *Fundamentals of Data Engineering* as a refresh/gap fill). Iâ€™ve been getting some interviews, but mostly with companies that donâ€™t yet have data engineers or donâ€™t fully understand the role. Ideally, Iâ€™m trying to land somewhere with an established data team and the chance to learn from more senior engineers.

Which would you prioritize first? Or is there something else youâ€™d recommend focusing on instead?",1769197296.0,4,3,https://www.reddit.com/r/dataengineering/comments/1ql0r1w/certs_or_tools_what_should_i_learn_next_as_a_mid/
1qkxxls,Free Online Courses to take with Certifications,"Hi everyone! Iâ€™m looking for free online courses with certification related to dataâ€”yung reliable talaga and good for upskilling. Can you recommend some? Also, do you think itâ€™s worth getting paid certifications? Thank you",1769191187.0,1,1,https://www.reddit.com/r/dataengineering/comments/1qkxxls/free_online_courses_to_take_with_certifications/
1qkxr5q,Candidates using AI,"I am a data engineering manager and we are looking for a senior data engineer. So many times we see a candidate that looks perfect on paper, HR has a great conversation with them, then we do a technical Teams call and find that the candidate is using some kind of AI (or human) assistance - delayed responses, answers that are too perfect or very general, sometimes very obvious reading from the screen or listening through the headphones, and some (or complete) inability to write code during the test. 

Is there a way to filter out these candidates ahead of time, so we don't have to waste time on it? We don't mind that the team members use AI to be more productive and we even encourage it, but this is just pure manipulation, and definitely not what we are looking for.",1769190802.0,104,195,https://www.reddit.com/r/dataengineering/comments/1qkxr5q/candidates_using_ai/
1qkxlp5,Need ideas for personal project in non boring topics.,"For context : I graduated in June 2025 and been working since then in Company X . I have worked properly in a migration project which involved getting the clientâ€™s data from various sources and getting it in a single destination and making data marts for other users . My task here was connecting the data sources , getting the data and performing etl. Databricks was my main working platform with spark . IVE worked on this  for 4 months and then decided to opt out of the project hoping to find and learn to contribute more and make myself better but then I got assigned to a different project whicu deals with insurance company and ever since then IVE been performing , orchestrating etlâ€™s , cleaning data , debugging for this insurance project and honestly itâ€™s sickening me . The policies,claims,customers data is boring and it just feels mentally ill keeping in mind of all the relations between these entities and keep working on them . For refreshment I wanna build my own project which is a bit less boring than this and something which is actually being done in the industry, suggest me any project ideas which could be helpful for my future or just any real time working  ideas which are bit less boring than this insurance field . ",1769190473.0,5,3,https://www.reddit.com/r/dataengineering/comments/1qkxlp5/need_ideas_for_personal_project_in_non_boring/
1qkx5md,Good practices for flows where the origin file structure has no standard ?,"My current job has a heavy reliance on .csv files and we are creating workflows to make automation and other projects IN DATABRICKS

Though the issue is that the user's frequently change columns orders, they add extra columns, etc.

I was thinking of coding some railroads but it seems very troublesome to guarantee only specific columns exist in the files as i would have to check the columns and their contents them reorganize them to even start working.",1769189478.0,1,4,https://www.reddit.com/r/dataengineering/comments/1qkx5md/good_practices_for_flows_where_the_origin_file/
1qkw05o,Breaking Into the DE industry,"For those who have years working as a DE, when you first started it, how did you convince the company to hire you? 

I am feeling a little powerless right now as my github portofolio doesnt feel enough or recruiters probably dont even bother checking it. I would love to work as an intern but nobody taking interns unless its a company who urgently needs a recruit, but you have to be extra cautious and opportunistic. ",1769186982.0,13,28,https://www.reddit.com/r/dataengineering/comments/1qkw05o/breaking_into_the_de_industry/
1qkujmz,Question on Airflow,"We are setting up our data infrastructure, which includes Redshift, dbt Core for transformations, and Airflow for orchestration. We brought in a consultant who agreed with the use of Redshift and dbt; however, he was completely opposed to Airflow. He described it as an extremely complex tool that would drain our teamâ€™s time. Instead, he recommended using Lambda functions. I understand there are multiple ways to orchestrate Lambda, but it seems to me that these tools serve different purposes. Does he have a point? What are your thoughts on this?",1769183774.0,14,26,https://www.reddit.com/r/dataengineering/comments/1qkujmz/question_on_airflow/
1qkuho6,What to expect in System Design/Architecture/Data Modeling Round?,"First off (in a DE context), is 'system design' round or 'architecture' round the same thing/synonymous?

What is expected of a system design/architecture round?  
What is expected of a data modeling round?

",1769183650.0,3,0,https://www.reddit.com/r/dataengineering/comments/1qkuho6/what_to_expect_in_system_designarchitecturedata/
1qkr43c,How can an on prem engineer break into the cloud in this market?,"I have 10+ years total experience & 5-7 years of aws experience but have spent the last 3 at an on premise environment. I did this because they had a traditional Kimball warehouse and I really enjoy data modeling. I was also curious about shifting to more data pipeline type of environment. I was previously leading a team as an aws solution architect but felt I was leaning too much on star schema design and got the idea the leadership wanted pipelines. I made it work but constantly questioned how such an unconnected reporting layer could keep metrics consistent across company reporting. Because of this I took this job because they were planning to migrate to the cloud and my background would have helped. unfortunately shortly after I started my manager started butting heads with the consultant who was helping us reshare into a more current architecture. Because of that we were rebadged without getting any cloud training and I'm screwed. 

I'm working on the AWS data engineer certification, done with a class and working through the practice exams. I also feel like I'm under skilled when it comes to databricks and was going to be my next certification target. Do I have to get officially certified before I can start advertising these skills? any other general advice? I mainly don't want to put a lot of time or money into it only for it to not help and I end up getting pushed out anyway. ",1769175721.0,6,4,https://www.reddit.com/r/dataengineering/comments/1qkr43c/how_can_an_on_prem_engineer_break_into_the_cloud/
1qkqu2z,What is the future for dataengineering?,"I've just completed very first data project on one of the popular online learning platforms (I just don't want to mention its name here, so it is not a promotion). Now, basically that platform gives you access to their Jupeter Notebooks, and requirements. It is very simple project, where you need to load the .csv file, split it to different .csv files, do some cleaning and tranformations. All the requirements are there. AND, right to the notebook there is AI (LLM, I don't know. You name it.) I took the requirements, give it to AI and asked to write a promt. You see, I even didn't have to write the prompt. Now, next step is give the promt to the AI and ask him wirte python code. Now, it amaizing that the python code is correct. So, all I had to do is click 'Run', and that is it. I sucessfully submitted the project and earned some points. Done. 

Now, the question that bothers me is 'what is the future for dataengineering jobs?' Isn't it bothering you guys? How soon we will reach the point when you don't have to learn pandas and numpy and etc. All you have to do is ask AI to do it. Scary.  ",1769174986.0,24,40,https://www.reddit.com/r/dataengineering/comments/1qkqu2z/what_is_the_future_for_dataengineering/
1qkpue9,Career Advancement as a DE,"I'm a junior DE in a startup in EU. I'm kinda the black sheep for the data team when I got hired as a data analyst intern but after 3 days, I realized I needed to do data engineering. Though it is something I don't want to do, I can't help but to go with it since it pays. Fast forward, I'm in a permanent role in the same company and now the job scope is both engineering and analytics. I'm a one man team as a junior with a boss that came from SWE background and has little exp with data as a whole. 

I picked up python enough to complete one ETL pipeline. I learned everything on Youtube and I rely heavily on AI for almost everything. I make AI as my sparring partner to challenge my own ideas and understandings. I am burned out and I think I'm not cut out to even jump to another company. 

Can I get advices on how do I actually progress in this line of work? (I made peace with DE and I'm interested to do it further but I feel like my progress is very slow and stagnant. I also feel like I'm not doing what typical DE does in their day to day job) ",1769172326.0,12,4,https://www.reddit.com/r/dataengineering/comments/1qkpue9/career_advancement_as_a_de/
1qkovgd,Going insane trying to get Instagram performance data,"Hey folks

Need some help here since I'm going insane with this task that I thought it would be just a ""get api tokens and start working"". 

  
Context: My marketing colleagues wanted to get Instagram data into their brand performance reports (stuff like follower growth, reactions per post, etc). The company already has a business meta account for Instagram. 

Tried to  getting developer account using the same email used for the Instagram but no success. Then created meta business account with same Instagram and still no success. Creating a Facebook account is out of the picture. 

Has anyone else had any success trying to get this type of data to build a simple ETL? 

(I don't want to use third party connectors like fivetran btw) ",1769169422.0,7,3,https://www.reddit.com/r/dataengineering/comments/1qkovgd/going_insane_trying_to_get_instagram_performance/
1qkony4,Advice on query improvement/ clustering on this query in MS sql server,"```
SELECT DISTINCT
    ISNULL(A.Level1Code, '') + '|' +
    ISNULL(A.Level2Code, '') + '|' +
    ISNULL(A.Level3Code, '') AS CategoryPath,

    ISNULL(C1.Label, 'UNKNOWN') AS Level1Label,
    CAST(ISNULL(C1.Code, '') AS NVARCHAR(4)) AS Level1ID,

    ISNULL(C2.Label, 'UNKNOWN') AS Level2Label,
    CAST(ISNULL(C2.Code, '') AS NVARCHAR(4)) AS Level2ID,

    ISNULL(C3.Label, 'UNKNOWN') AS Level3Label,
    CAST(ISNULL(C3.Code, '') AS NVARCHAR(4)) AS Level3ID
FROM (
    SELECT DISTINCT
        Level1Code,
        Level2Code,
        Level3Code
    FROM AppData.ItemHeader
) A
LEFT JOIN Lookup.Category C1 ON A.Level1Code = C1.Code
LEFT JOIN Lookup.Category C2 ON A.Level2Code = C2.Code
LEFT JOIN Lookup.Category C3 ON A.Level3Code = C3.Code;
```
please see above as the query is taking a long time and could you please suggest what indexe(clustered or non clustered) in the tables AppData.ItemHeader and Lookup.Category? do we have to define index for each Level1Code, Level2Code and Level3Code or a combination?

",1769168736.0,0,5,https://www.reddit.com/r/dataengineering/comments/1qkony4/advice_on_query_improvement_clustering_on_this/
1qknt16,Annual/quarter corporate finance Vs stock tickers,"hi guys,   
  
i did try to apply data engineering standards as much as i can using databricks new free edition and AWS educate with their limitations of no iam role , now spark.set for serverless no dbfs and so on..  
to combine both sources reports and tickers to provide corporate real value calculating cashflow against risk ,   
the thing is i can't say what i did is ""the best"" or the ""truth""   
this is why guys i need your help help to assess brutally my working in terms of business understanding and technichal strategy and implementation  
 the goal is to know what is my position against data engineering levels.  
  
here's the medium article :  [corporate reports vs stock tickers](https://medium.com/@yahiachames/corporate-annual-reports-vs-corporate-stocks-a-lambda-architecture-for-dynamic-valuation-9e8d63e56818?source=friends_link&sk=a6faac988e23902e46e6b1bcf24366f5)  
or if you prefer code only : [financial cloud engine](https://github.com/yahiachames/Financial_cloud_engine.git)",1769165795.0,1,0,https://www.reddit.com/r/dataengineering/comments/1qknt16/annualquarter_corporate_finance_vs_stock_tickers/
1qklwjw,DataFrame or SparkSQL ? What do interviewers prefer ?,"I am learning spark. And i just needed clarity on what does interviewers prefer in interviews ? Irrespective of what is used in the companies while actual work. 

DataFrame or SparkSQL ?",1769158781.0,58,32,https://www.reddit.com/r/dataengineering/comments/1qklwjw/dataframe_or_sparksql_what_do_interviewers_prefer/
1qkkug6,Semantic views in snowflake,[https://peggie7191.medium.com/digging-into-semantic-views-in-snowflake-a391780d2938](https://peggie7191.medium.com/digging-into-semantic-views-in-snowflake-a391780d2938),1769154787.0,0,1,https://www.reddit.com/r/dataengineering/comments/1qkkug6/semantic_views_in_snowflake/
1qkikn4,What issues did users face with Cloudera platform apart from proprietary lock-ins? What are data users or enterprise data teams doing as an alternative to using Cloudera?,I was able to understand that Cooudera has paywalled their software where users require a private cloud subscription to even access to their downloads. In addition to the proprietary lock-ins what issues did users of Cloudera face? How can enterprises avoid being stuck in Clouderaâ€™s proprietary lock-ins? What alternatives can they look out for to manage their data workloads on both cloud and on-prem? Your take on it?,1769146947.0,3,1,https://www.reddit.com/r/dataengineering/comments/1qkikn4/what_issues_did_users_face_with_cloudera_platform/
1qkerci,How are you replicating your databases to the lake/warehouse in realtime?,"We use kafka connect to replicate 10-15 postgres databases but it's becoming a maintenance headache now.   
  
\- Schema evolution is running on separate airflow jobs.   
\- Teams have no control over which tables to (not) replicate.   
\- When a pipeline breaks, it creates a significant backlog on the database (increased storage). And DE has to do a full reload in most cases.  
  
Which managed solutions are you using? Please share your experiences.",1769136065.0,38,43,https://www.reddit.com/r/dataengineering/comments/1qkerci/how_are_you_replicating_your_databases_to_the/
1qkc7gg,Accounting to Data Engineering,"Is anyone here a career shifter from the field of accounting and finance? How did you do it? How did you prepare yourselves to make the switch? What do you wish you knew/learned sooner in your career?

  
",1769129201.0,7,12,https://www.reddit.com/r/dataengineering/comments/1qkc7gg/accounting_to_data_engineering/
1qkaaf3,Nvidia CEO Jensen Huang Mentions SQL @ Davos,"Jensen Huang mentioned SQL at word economic forum in Davos. He said the past was pre recorded structured data built on SQL. Now computers understand unstructured information. AI can take unstructured information and reason about its meaning to perform a task for you. 

Data pipelines retrieve data from a source, transform to tabular and load to database.

More data pipelines now will retrieve data from source then clean and prepare to load to AI models.",1769124358.0,0,11,https://www.reddit.com/r/dataengineering/comments/1qkaaf3/nvidia_ceo_jensen_huang_mentions_sql_davos/
1qk7xr1,Do you think AI Engineering is just hype or is it worth studying in depth?,I'm thinking about the future of data-related careers and how to stay relevant in the job market in the coming years,1769118612.0,19,35,https://www.reddit.com/r/dataengineering/comments/1qk7xr1/do_you_think_ai_engineering_is_just_hype_or_is_it/
1qk66m7,"Advice from a HM: If you're not getting called back, your CV isn't good. Or you didn't read the job post.","I see a lot of posts on here about people applying for jobs and not getting interviews. We put up a job post this week for a senior role and there are so many issues with so many of the applications that we're only reaching out to about 2% of them for a screening. That's not because that's our top 2%, but because there's so much spam that comes in.

\- Pay attention to location. If it says it's in office or hybrid and your CV says you live in a different state and doesn't mention either there or a cover letter that says willing to relocate, you're going to get rejected.

\- If you need visa sponsorship and the role is not providing that, you get auto filtered out.

\- If your CV is more than a page and a half, it's not getting read. We don't have time to thoroughly read size 10 font with 0.25 inch margins filled with the buzzwords of every single python library you've ever imported.

  
Here's what you can do:  
\- Spell out if you are willing to relocate. If you saw the job req shared on Linkedin, reach out to the person and make sure they know you are willing to relocate

  
\- Focus your CV on results. How much faster did pipelines run? how much were errors decreased? How much money did you save? We don't care about specifc technologies, if you can do it with one tool you can learn to do it with another

MOST IMPORTANTLY:

\- Make your CV shorter. The #1 issue with hiring DEs is that they cannot communicate clearly and effectively to non technical stakeholders. If your CV is 4 pages of technical terms, you're throwing everything at the wall and seeing what sticks.

\- Hiring managers want to see that you can communicate clearly, took ownership of projects, worked across orgs and made things run faster, cheaper, or more accurate.

  
Here's an example of a value driven result:

Maybe you wrote a quick script that took you 30 minutes. You didn't think of it being a huge deal. However, the process you put in place took the month close from 4 days to 2 days for your accounting team.

Most of the applications I see only focus on the BIG stuff, which is often some infrastructure project that is ongoing, and most people could do it if they were assigned to do so. If you want to stand out, saying you worked cross department to cut the month end close time in half, that's MASSIVE value.

  
IT's not about complexity. It's not about tools. It's about showing that you saw a need, and came up with a scalable solution to help everybody involved.",1769114612.0,0,2,https://www.reddit.com/r/dataengineering/comments/1qk66m7/advice_from_a_hm_if_youre_not_getting_called_back/
1qk53ak,How Meesho rebuilt their Real-Time Analytics Platform,"Interesting post from Rajat Rana on Meesho's technology blog. Meesho is a huge Indian e-commerce platform. He shares how, and why, they rebuilt their real-time analytics platform on Apache Pinot to support a wider variety of sophisticated features â€” such as identity merging, deep insights, funnel analysis, and complex user journeys. 

(I work at [StarTree](https://startree.ai/) \- a real-time analytics platform powered by Pinot - we helped them implement this)

",1769112149.0,2,0,https://www.reddit.com/r/dataengineering/comments/1qk53ak/how_meesho_rebuilt_their_realtime_analytics/
1qk4olu,OpenSheet: experimenting with how LLMs should work with spreadsheets,"Hi folks. I've been doing some experiments on how LLMs could get more handy in the day to day of working with files (CSV, Parquet, etc). Earlier last year, I built https://datakit.page and evolved it over and over into an all in-browser experience with help of duckdb-wasm. Got loads of feedbacks and I think it turned into a good shape with being an adhoc local data studio, but I kept hearing two main things/issues:

1. Why can't the AI also change cells in the file we give to it?
2. Why can't we modify this grid ourselves?

So besides the whole READ and text-to-SQL flows, what seemed to be really missing was giving the user a nice and easy way to ask AI to change the file without much hassle which seems to be a pretty good use case for LLMs.

DataKit fundamentally wasn't supposed to solve that and I want to keep its positioning as it is. So here we go. I want to see howÂ https://opensheet.app can solve this.

This is the very first iteration and I'd really love to see your thoughts and feedback on it. If you open the app, you can open up the sample files and just write down what you want with that file.",1769111265.0,0,0,https://www.reddit.com/r/dataengineering/comments/1qk4olu/opensheet_experimenting_with_how_llms_should_work/
1qk0djh,ClickHouse launches a native Postgres service,,1769102008.0,6,3,https://www.reddit.com/r/dataengineering/comments/1qk0djh/clickhouse_launches_a_native_postgres_service/
1qjzp7f,Seeking Data Folks to Help Test Our Free Database Edition,"Hey everyone!

Excited to be here! I work at a database company, and weâ€™ve just releasedÂ a free edition of our analytical database tool designed for individual developers and data enthusiasts. Weâ€™re looking for community members to test it out and help us make it even better with your hands-on feedback.

**What you can do:**

* Test with data at any scale, no limits.
* You can play around with enterprise features, including spinning up distributed clusters on your own hardware.
* Mix SQL with native code in Python, R, Java, or Lua, also supported out of the box.
* Distribute workloads across nodes for MPP.
* PS: Currently available on AWS, we will launch support for Azure and GCP as well soon.

**Quick Start:**

1. Make sure you have the ourÂ [LauncherÂ ](https://downloads.exasol.com/)installed and your AWS profile configured (see this [Quick Start Guide](https://docs.exasol.com/db/latest/get_started/exasol_personal.htm)Â for details).
2. Create a deployment directory:Â `mkdir deployment`
3. Enter the directory:Â `cd deployment`
4. Install the free edition:Â [here](https://www.exasol.com/downloads/for-individuals/exasol-personal/)
5. Work with your actual projects, test queries, or synthetic datasets, whatever fits your style!

**Weâ€™d love to hear about:**

* What works seamlessly, and what doesnâ€™t
* Any installation or usability hurdles
* Performance on your favorite queries and data volumes
* Integrations with tools like Python, VS Code, etc.
* Suggestions, bug reports, or feature requests

Please share your feedback, issues, or suggestions in this thread, or open an issue on GitHub.",1769100564.0,3,0,https://www.reddit.com/r/dataengineering/comments/1qjzp7f/seeking_data_folks_to_help_test_our_free_database/
1qjyglh,Made a dbt package for evaluating LLMs output without leaving your warehouse,"In our company, we've been building a lot of AI-powered analytics using data warehouse native AI functions. Realized we had no good way to monitor if our LLM outputs were actually any good without sending data to some external eval service.

Looked around for tools but everything wanted us to set up APIs, manage baselines manually, deal with data egress, etc. Just wanted something that worked with what we already had.

So we built this dbt package that does evals in your warehouse:

* Uses your warehouse's native AI functions
* Figures out baselines automatically
* Has monitoring/alerts built in
* Doesn't need any extra stuff running

Supports Snowflake Cortex, BigQuery Vertex, and Databricks.

Figured we open sourced it and share in case anyone else is dealing with the same problem - [https://github.com/paradime-io/dbt-llm-evals](https://github.com/paradime-io/dbt-llm-evals)

",1769097856.0,5,0,https://www.reddit.com/r/dataengineering/comments/1qjyglh/made_a_dbt_package_for_evaluating_llms_output/
1qjx3t1,Fivetran HVR Issues SAP,"We have set up fivetran HVR to replicate SAP data from S4 HANA to Databricks in real time.

It is fairly straight forward to use but we are regularly needing to do sliced refresh jobs as we find missing record changes (missed deleted, inserts or updates) in our bronze layer.

Fivetran support always tell us to update the agent but otherwise donâ€™t have much of an answer.

I am considering scheduling rolling refreshes and compare jobs during downtime.

Has anyone else experienced something similar? Is this just part of the fun?",1769094813.0,3,16,https://www.reddit.com/r/dataengineering/comments/1qjx3t1/fivetran_hvr_issues_sap/
1qjwtpf,Migrating or cloning a AWS glue Workflow,"Hi All..

I need to move a AWS glue workflow from one account to another aws account.  Is there a way to migrate it without manually creating the workflow again in the new account?",1769094166.0,2,2,https://www.reddit.com/r/dataengineering/comments/1qjwtpf/migrating_or_cloning_a_aws_glue_workflow/
1qjwlaz,Couchbase Users / Config Setup,"Hi All - planning a Couchbase setup for my HomeLab, want to spin up a bit of an algo trading bot... lots of real time ingress, and as fast as I can streaming messaging out to a few services to generate signals etc... Data will be mainly financial inputs / calculations, thinking long, flat and normalized, I can model it but who has the time.

Shooting for 4TB of usable storage, given rough estimate of 3GB a day for like... 20 Tickers and then some other random stuff? (Retention set at monthly, 30 days x 20 Tickers x 3GB/day = 1.8 TB. 20% empty to keep the hard drive gods happy = \~2.2TB, + other random buffer = 3TB. 4 TB should be plenty. For now?

I've got a bunch of hardware, just wanted to bounce the config off of this group to see what y'all think.

The relevant static portion of the hardware I have stands as:

* 5950x (16c/32t) - 128GB DDR4 - 2TB NVMe Boot Drive - 8 SATA ports - AMD 7900x GPU
* 5950x (16c/32t) - 64GB DDR4 - 2TB NVMe Boot Drive - 8 SATA ports
* 4x EliteDesk MiniPC - ONE of those handy NVME > 6x SATA cards that works, OKish
* 4x RPi

I've also got the below which can be configured to the above as I see fit.

* 4x 6TB HDD
* 4x 4TB HDD
* 8x 2TB HDD

This is where I could use some help, I've got a few thoughts on how to set it up.... but any advice here is welcome. Using proxmox / VMs to differentiate ""machines""

*Option 1 - Single Machine DB / 3 Node Deployment*

*Will allow me to ringfence the database compute needed to a single machine - but leave single point of failure.*

Machine 1: 5950x (16c/32t) - 128GB DDR4 - 2TB NVMe Boot Drive - 8 SATA ports

Disk Setup:

* 2x 2TB HDD (Raid0) - 4TB Storage Pool
* 2x 2TB HDD (Raid0) - 4TB Storage Pool
* 2x 2TB HDD (Raid0) - 4TB Storage Pool
* 2x 6TB HDD (Raid0) - 12TB Storage Pool

Node Setup:

* Node 1 - 5 Core / 10 Thread - 32GB Memory - 4TB Storage Pool
* Node 2 - 5 Core / 10 Thread - 32GB Memory - 4TB Storage Pool
* Node 3 - 5 Core / 10 Thread - 32GB Memory - 4TB Storage Pool

Snapshots run daily off market hours to the 12TB Drive.

*Option 2 - Multiple Machine / 6 Node Deployment*

*Will allow me to survive failure of a machine, but will need to share compute. I'll be eating drive space with this as well which I'm ok with... sorta.*

Machine 1: 5950x (16c/32t) - 128GB DDR4 - 2TB NVMe Boot Drive - 8 SATA ports

Disk Setup:

* 2x 2TB HDD (Raid0) - 4TB Storage Pool
* 2x 2TB HDD (Raid0) - 4TB Storage Pool
* 2x 2TB HDD (Raid0) - 4TB Storage Pool
* 2x 6TB HDD (Raid0) - 12TB Storage Pool

Node Setup:

* Node 1 - 4 Core / 8 Thread - 16GB Memory - 4TB Storage Pool
* Node 2 - 4 Core / 8 Thread - 16GB Memory - 4TB Storage Pool
* Node 3 - 4 Core / 8 Thread - 16GB Memory - 4TB Storage Pool

Snapshots run daily off market hours to the 12TB Drive. Leaves me with 4 cores of compute / 16GB memory for processing.

Machine 2: 5950x (16c/32t) - 64GB DDR4 - 2TB NVMe Boot Drive - 8 SATA ports

Disk Setup:

* 2x 2TB HDD (Raid0) - 4TB Storage Pool
* 2x 4TB HDD (Raid0) - 8TB Storage Pool
* 2x 4TB HDD (Raid0) - 8TB Storage Pool
* 2x 6TB HDD (Raid0) - 12TB Storage Pool

Node Setup:

* Node 1 - 4 Core / 8 Thread - 16GB Memory - 4TB Storage Pool
* Node 2 - 4 Core / 8 Thread - 16GB Memory - 8TB Storage Pool
* Node 2 - 4 Core / 8 Thread - 16GB Memory - 8TB Storage Pool

Any thoughts welcome to folks who have done this / have experience. I think I may be over provisioning the compute / memory needed? But not sure. If there is an entirely different permutation of the above... I'd be more than open to hearing it :)",1769093632.0,3,1,https://www.reddit.com/r/dataengineering/comments/1qjwlaz/couchbase_users_config_setup/
1qjvrps,Pricing BigQuery VS Self-hosted ClickHouse,"Hello. We use BigQuery now (no reserved slots). Pricing-wise, would it be cheaper to host ClikHouse on a GKE cluster? Not taking into account the challenges of managing a K8s cluster or how much it cost to have a person to work on that.",1769091651.0,8,9,https://www.reddit.com/r/dataengineering/comments/1qjvrps/pricing_bigquery_vs_selfhosted_clickhouse/
1qjv1ja,Any European Alternatives to Databricks/Snowflake??,"Curious to see what's out there from Europe?

Edit: options are open source route or exasol/dremio which are not in the same league as Databricks/Snowflake.",1769089842.0,93,95,https://www.reddit.com/r/dataengineering/comments/1qjv1ja/any_european_alternatives_to_databrickssnowflake/
1qjqt68,Fabric's Copy Data's Table Action (UPSERT),"I'm copying a table from a oracle's on-prem database to fabric's lakehouse as a table   
and in the copy data activity, I have set the copy data activity's table Action to **UPSERT**.

I have captured the updated records and have checked the **change data feed**, instead of showing *update\_preimage* and *update\_postimage* as a status, I'm having the combination of *insert* and *update\_preimage.*

Is this mal-functionality is of because, the UPSERT functionality is still in preview status from Fabric?",1769076745.0,2,1,https://www.reddit.com/r/dataengineering/comments/1qjqt68/fabrics_copy_datas_table_action_upsert/
1qjo19g,Purview - DGPU pricing,"I'm testing Purview for Data Governance - PoC before some serious workfloads. I deployed sample Adventure Works LT in Azure SQL and scanned it. Then created Data Product over it. I though cost will be neglible, but for these 19 assets it charged 4.72 EUR on Data Management Basic Data Governance Processing Unit itself. I know it's not much, but if we're going to have like 300 hunders tables per CRM database, and have few such sources, it's gonna be 10k EUR...

https://preview.redd.it/6ihy64hohceg1.png?width=1622&format=png&auto=webp&s=cad4258f7420397b62682d93b57d4fefe6f8aebc

As Far as I understand these DGPU are for Data Quality and Data Health Management as per MS docs.

And there are indeed some default Data Health Management rules running by default (under Health Management -> Controls). Which are enabled by default btw...

https://preview.redd.it/wuikep1wpueg1.png?width=1489&format=png&auto=webp&s=d625209fb1f6b72752af443fa9120eb424e2aa3b

I figured out, that to disable I need to go into Schedule Refresh and then disable it (lovely UI)... Not to mention I am able only to limit these controls per domain, not even per data product.

It all seems to be crazy complicated... Do you guys have any experience with this purview pricing?",1769066414.0,3,1,https://www.reddit.com/r/dataengineering/comments/1qjo19g/purview_dgpu_pricing/
1qjntkx,Insights by Snowflake data superhero: What breaks first as Snowflake scales and how to prepare for it.,"Hi everyone, We're hosting a live session with Snowflake superhero on what actually breaks first as Snowflake environments scale and how teams prepare for it before things spiral.

You can register [here](https://hevodata.com/webinar/what-breaks-first-when-snowflake-scales-and-how-to-prepare-for-it/) for free!

Open to answering qs if you have any, see you there!",1769065663.0,0,0,https://www.reddit.com/r/dataengineering/comments/1qjntkx/insights_by_snowflake_data_superhero_what_breaks/
1qjmtcv,I am reading more about context engineering? What should data engineer know about context engineering and why is it important?,"I reached out to a data engineer. He said that context engineering is the only way we can ensure AI agents help us manage our data problems. 

Many some organizations like Informatica and Acceldata mention that they have an intelligent contextual layer that can help data teams manage the data with the right context. How does it add value to the context engineering part? 

I am reading more articles about context engineering recently. Whatâ€™s your take on it? How can I understand it better? Can agentic data management tools like Informatica or Acceldata help us in context engineering or should we use a simple cloud data management tool like Databricks or Snowflake to do it? Your take in it? ",1769062247.0,4,3,https://www.reddit.com/r/dataengineering/comments/1qjmtcv/i_am_reading_more_about_context_engineering_what/
1qjluof,Databricks | ELT Flow Design Considerations,"Hey Fellow Engineers

My organisation is preparing a shift from Synapse ADF pipelines to Databricks and I have some specific questions on how I can facilitate this transition.

Current General Design in Synapse ADF is pretty basic. Persist MetaData in one of the Azure SQL Databases and use Lookup+Foreach to iterate through a control table and pass metadata to child notebooks/activities etc.

Now here are some questions 

1) Does Databricks support this design right out of the box or do I have to write everything in Notebooks (ForEach iterator and basic functions) ?

2) What are the best practices from Databricks platform perspective where I can achieve similar arch without complete redesign ?

3) If a complete redesign is warranted, whatâ€™s the best way to achieve this in Databricks from efficiency and a cost perspective.

I understand the questions are too vague and it may appear as a half hearted attempt but I was just told about this shift 6 hours back and would honestly trust the veterans in the field rather than some LLM verbiage.

Thanks Folks!",1769059228.0,3,7,https://www.reddit.com/r/dataengineering/comments/1qjluof/databricks_elt_flow_design_considerations/
1qjido1,Iceberg Sucks - But You Knew That Already,"Obligatory: this is my article, but I'm happy to discuss/hear any thoughts below!",1769049407.0,0,5,https://www.reddit.com/r/dataengineering/comments/1qjido1/iceberg_sucks_but_you_knew_that_already/
1qjgfq6,Career help for Career after data analyst role,"I'm currently in school as a 3rd year for Management Information Systems concentrating on data and cloud with classes like *Advanced Database Systems, Data Warehousing and Cloud System Management*. My goal is to get a six figure job when im in my mid to late 20s. I want to know what i should do to reach that goal and how easy/hard would it be. I also looked at jobs like cloud analyst but i don't think i would do well in that has my projects are data focused apart from when i did a DE project using AZURE.",1769044245.0,2,2,https://www.reddit.com/r/dataengineering/comments/1qjgfq6/career_help_for_career_after_data_analyst_role/
1qjg1mp,Questions about best practices for data modeling on top of OBT,"For context, the starting point in our database for game analytics is an events table, which is really One Big Table. Every event is logged in a row along with event-related parameter columns as well as default general parameters.

That said, we're revamping our data modeling and we're starting to use dbt for this. There are some types of tables/views that I want to create and I've been trying to figure out the best way to go about this.

I want to create summary tables that are aggregated with different grains, e.g. purchase transaction, game match, session, user day summary, daily metrics, user metrics. I'm trying to answer some questions and would really appreciate your help.

1. I'm thinking of creating the user-day summary table first and building user metrics and daily metrics on top of that, all being incremental models. Is this a good approach?
2. I might need to add new metrics to the user-day summary down the line, and I want it to be easy to: a) add these metrics and apply them historically and b) apply them to dependencies along the DAG also historically (like the user\_metrics table). How would this be possible efficiently?
3. Is there some material I could read especially related to building models based on event-based data for product analytics?",1769043224.0,3,4,https://www.reddit.com/r/dataengineering/comments/1qjg1mp/questions_about_best_practices_for_data_modeling/
1qjbawr,Fivetran pricing spike,"Hi DEs,

And the people using Fivetran..

We are experiencing a huge spike (more than double) in monthly costs following the March 2025 changes, and now with the January 2026 pricing updates.

Previously, Fivetran calculated the cost per million Monthly Active Rows (MAR) at the account level. Now, it has shifted to the connector (or connection) level. This means costs increase significantly â€” often exponentially â€” for any connector handling no more than one million MAR per month. If a customer has multiple connectors below that threshold, the overall pricing shoots up dramatically.

What is Fivetran trying to achieve with this change?
Fivetran's official explanation (from their 2025 Pricing FAQ and documentation) is that moving tiered discounts (lower per-MAR rates for higher volumes) from account-wide to per-connector aligns pricing more closely with their actual infrastructure and operational costs. Low-volume connectors still require setup, ongoing maintenance, monitoring, support, and compute resources â€” the old model let them ""benefit"" from bulk discounts driven by larger connectors, effectively subsidizing them.

Will Fivetran survive this one? My customer is already thinking about alternatives.. what is your opinion?",1769031884.0,104,56,https://www.reddit.com/r/dataengineering/comments/1qjbawr/fivetran_pricing_spike/
1qj9gsv,How repartition helps in dealing with data skewed partitions?,"I am still learning the fundamentals. I have seen in many articles that if there is skewness in your data then repartition can solve it. But from my understanding, when we do repartition it shuffles the entire data. So, assuming I do **df\_repart = df.repartition(""id"")** wouldn't this again give skewed partitions?

",1769027801.0,2,6,https://www.reddit.com/r/dataengineering/comments/1qj9gsv/how_repartition_helps_in_dealing_with_data_skewed/
1qj7daf,I am so bad at off the cuff questions about process,"1 year on from a disastrous tech assessment ended up landing me the job, a recruiter reached out and offered me a chat for what is basically my dream roll. AWS Data Engineer to develop an ingestion and analytics pipelines from IoT devices. 

Pretty much owning the provisioning and pipeline process from the ground up supporting a team of data scientists and analysts. 

Every other chat I've been to in my past 3 jobs has been me battling with imposter syndrome. But today, I got this, I know this shiz. I've been shoehorning AWS into my workflow wherever I can, I built a simulated corporate VPC and production ML workloads, learned the CDK syntax, built an S3 lake house.

But I go to the chat and its really light on actual AWS stuff. They are more interested in my thought process and problem solving. Very refreshing, enjoyable even. 

So why am I falling over on the world's simplest pipelines. 10 million customer dataset, approx 10k product catalogue, product data in one table, transaction data captured from a streaming source daily. 

One of the bullet points is ""The marketing team are interested in tracking the number of times an item is bought for the first time each day"" explain how you would build this pipeline.

Already covered flattening the nested JSON data into a columner silver layer. I read how many times an item is bought the first time each day as ""how do you track first occurance of an item bought that day""

The other person in the chat had to correct my thinking and say no, what they mean is how do you track when the customer first purchased an item overall. 

But then Im reeling from the screw up. I talk about creating a staging table with the 1st occurance each day and then adding the output of this to a final table in the gold layer. She says so where would the intermediate table live, I say it wouldn't be a real table its an in memory transformation step (meaning Id use filter pushdown and schema inference of the parquet in silver to pull the distinct customerid, productid, min(timestamp) and merge into gold where customerid productid doesn't exist. 

she said that would be unworkable with data of this size to have an in memory table, and rather than explain I didnt mean that I would dump 100 million rows into EC2 RAM, I kind of just said ah yeah, it makes sense to realise this in its own bucket. 

But im already in a twist by this point. 

Then on the drive home I'm thinking that was so dumb, if I had read the question properly its so obvious that I should have just explained that I'd create a look up table 

with the pertinent columns, customerid, productid, firstpurchase date. 

the pipeline is new data, first purchase per customer of that days data, merge into where not exists (maybe a overwrite if new firstpurchasedate < current firstpurchasedate to handle late arrival). 

So this is eating away at me and I think screw it, im just going to email the other chat person and explain what I meant or how I would actually approach it. So i did. it was a long boring email (similar to this post). But rather than make me feel better about the screw up Im now just in full cringe mode about emailing the chatter. its not the done thing. 

Recruiter didn't even call for a debrief. 

fml 

chat = view of the int",1769023180.0,12,6,https://www.reddit.com/r/dataengineering/comments/1qj7daf/i_am_so_bad_at_off_the_cuff_questions_about/
1qj5y75,Setting Up Data Provider Platform: Clickhouse vs DuckDB vs Apache Doris,"Please read the whole thing before ignoring the post because in the start I am going to use word most people hate so pl3ase stick with me.

Hi, so I want to setup data provider platform to provide blockchain data to big 4 accounting firms & gov agencies looking for it. Currently we provide them with filtered data in parquet or format of their choice and they use it themselves.

I want to start providing the data via an API where we can charge premium for it. I want to understand how I can store data efficiently while keeping performance I am 500ms latencies on those searches.

Some blockchains will have raw data up to 15TB and I know for many of you guys building serious systems this won't be that much.

I want to understand what is the best solution which will scale in future. Things I should be able to do:
- search over given block number range for events
- search a single transaction and fetch details of it do same for a block too

I haven't thought it through but ask it here might be helpful.

Also, I do use duckdb on data that I have locally about 500GB so I know it somewhat that's qhy I added it at the top not sure if it a choice at all for something serious.",1769020178.0,1,25,https://www.reddit.com/r/dataengineering/comments/1qj5y75/setting_up_data_provider_platform_clickhouse_vs/
1qj1a6l,Databricks certificate discount,"I found this databricks event that says if you complete courses through their academy you will be eligible for 50% discount.

I wanted to share it here if its useful for anyone and to ask if someone else is joining or if someone maybe joined an older similar event that can explain how does this work exactly.

Link: https://community.databricks.com/t5/events/self-paced-learning-festival-09-january-30-january-2026/ec-p/141503/thread-id/5768",1769010157.0,34,16,https://www.reddit.com/r/dataengineering/comments/1qj1a6l/databricks_certificate_discount/
1qj0hi3,Is Moving Data OLAP to OLAP an Anti Pattern?,Recently saw a comment on a post about ADBC that said moving data from OLAP to OLAP is an anti pattern. I get the argument but realized I am way less dogmatic about this. I could absolutely see a pragmatic reason you would need to do move data/tables between DW's. And that doesn't even account for the Data Warehouse to DuckDB pattern. Wouldn't that technically be OLAP to OLAP?,1769008419.0,0,11,https://www.reddit.com/r/dataengineering/comments/1qj0hi3/is_moving_data_olap_to_olap_an_anti_pattern/
1qizifw,Found a Issue in Production while using Databricks Autoloader,"Hi DE's,

recently one of our pipeline had failed due to very abnormal issue.

  
upstream: json files

downstream : databricks

the issue is with the schema evolution. during the job execution. the first file which was present after the checkpoint file. is completely had a new schema ( a colunm addition) after the activity og DDL from source side we have extratced all the changes before. after the DDL while starting the file we faced the issue .

ERROR :

\[UNKNOWN\_FIELD\_EXCEPTION.NEW\_FIELDS\_IN\_RECORD\_WITH\_FILE\_PATH\]

  
We have used this option in read stream:

    .option(""cloudFiles.schemaEvolutionMode"", ""addNewColumns"")

  
in write stream.

    .option(""mergeSchema"",""true"")

  
as a work arround we removed a colunm of the first record which was added and we started the it started to read and pusing it to the delta tables and schema also evolued.

Any idea about this behaviour ?

",1769006163.0,1,4,https://www.reddit.com/r/dataengineering/comments/1qizifw/found_a_issue_in_production_while_using/
1qizd0c,Data from production machine to the cloud,"The company I work for has machines all over the world. Now we want to gain insight into the machines. We have done this by having a Windows IPC retrieve the data from the various PLCs and then process and visualize it. The data is stored in an on-prem database, but we want to move it to the cloud. How can we get the data to the cloud in a secure way? Customers are reluctant and do not want to connect the machine to the internet (which I understand), but we would like to have the data in the cloud so that we can monitor the machines remotely and share the visualizations more easily. What is a good architecture for this and what are the dos and don'ts?

",1769005802.0,3,3,https://www.reddit.com/r/dataengineering/comments/1qizd0c/data_from_production_machine_to_the_cloud/
1qiylvo,Interesting Links in Data Engineering - January 2026,"Here's January's edition of Interesting Links: https://rmoff.net/2026/01/20/interesting-links-january-2026/

It's a bumper set of links with which to kick off 2026.
There's lots of data engineering, CDC, Icebergâ€¦and even _whisper_ some quality AI links in there tooâ€¦but ones that *I* found interesting with a data-engineering lens on the world. 
See what you think and lmk.",1769003978.0,31,2,https://www.reddit.com/r/dataengineering/comments/1qiylvo/interesting_links_in_data_engineering_january_2026/
1qiyh0w,Cloud Data Engineer (4â€“5 YOE) â€“ Company-wise Fixed CTC (India),"Letâ€™s build a salary reference to help all of us benchmark compensation for Cloud/Data Engineers with 4â€“5 YOE in India.

Please share real numbers (current salary, recent offers, or verified peer data) in this format only:
Copy code

Company:
Role:
YOE:
Fixed CTC (â‚¹ LPA):
Bonus/RSUs/Variable (â‚¹ LPA):

Well-known companies only.

If everyone contributes honestly, this thread can help the entire community make better career decisions.",1769003629.0,0,2,https://www.reddit.com/r/dataengineering/comments/1qiyh0w/cloud_data_engineer_45_yoe_companywise_fixed_ctc/
1qixwtz,Informatica deployment woes,"I'm new to Informatica so apologies if the questions are a bit noddy.

I'm using the Application Integration module.

There is a hierarchy of objects where you have a service connector at the bottom that is used by an application connector.  The app connector is used by a process object.

If the process object is ""published"" then to edit it I 1st have to unpublished it. But that takes it offline which is not good for a thing in production. This seems to be a major blocker to development. There doesn't seem to be the concept of versioning. V1 is in production, but there seems to be no concept of V1.0.1 or any other semantic versioning capability.

Worst still, it seems I have to unpublish the hierarchy of objects to make basic changes as published objects block changes in the dependency tree.

I must be approaching this the wrong way and should be grateful for any advice.",1769002174.0,1,1,https://www.reddit.com/r/dataengineering/comments/1qixwtz/informatica_deployment_woes/
1qixl62,Logging and Alert,"How you guys will do logging and Alert in Azure Data Factory and in databricks?? 

What you will follow log analytics or do you use any other ways ?? 

Did anyone suggest good resources for logging and alert for both services! ",1769001337.0,4,4,https://www.reddit.com/r/dataengineering/comments/1qixl62/logging_and_alert/
1qivynv,The Call for Papers for J On The Beach 26 is OPEN!,"Hello Data Lovers!

Next [J On The Beach](http://www.jonthebeach.com) will take place in Torremolinos, Malaga, Spain in October 29-30, 2026.

The Call for Papers for this year's edition is **OPEN** until **March 31st**.

Weâ€™re looking for practical, experience-driven talks about building and operating software systems.

Our audience is especially interested in:

# Software & Architecture

* Distributed Systems
* Software Architecture & Design
* Microservices, Cloud & Platform Engineering
* System Resilience, Observability & Reliability
* Scaling Systems (and Scaling Teams)

# Data & AI

* Data Engineering & Data Platforms
* Streaming & Event-Driven Architectures
* AI & ML in Production
* Data Systems in the Real World

# Engineering Practices

* DevOps & DevSecOps
* Testing Strategies & Quality at Scale
* Performance, Profiling & Optimization
* Engineering Culture & Team Practices
* Lessons Learned from Failures

ğŸ‘‰ If your talk doesnâ€™t fit neatly into these categories but clearly belongs on a serious engineering stage, submit it anyway.

This year, we are also enjoying another 2 international conferences together: [Lambda World](https://lambda.world/) and [Wey Wey Web](http://www.weyweyweb.com).

**Link for the CFP:** [**www.confeti.app**](http://www.confeti.app)

",1768996600.0,1,0,https://www.reddit.com/r/dataengineering/comments/1qivynv/the_call_for_papers_for_j_on_the_beach_26_is_open/
1qivk0x,"This will work, yes??",did i get it right?,1768995280.0,231,8,https://www.reddit.com/r/dataengineering/comments/1qivk0x/this_will_work_yes/
1qisvk1,Help me pick my free cert please!,"Hey everyone, aspiring data engineer here. I wanted to ask you guys for advice here. I get 1 free cert through this veteran program and wanted to see what yall thought I should pick? (This is for extra/foundational knowledge, not to get me a job!) 

Out of the options, the ones I thought were most interesting were: 

\*\*CompTIA Data+\*\*

\*\*CCNA\*\* 

\*\*CompTIA Security+\*\*

\*\*PCAP OR PCEP\*\* 

I know they arenâ€™t all related to my goal, but figured the extra knowledge wouldnâ€™t hurt? 

Current plan: CS Major, trying to stay internal at current company by transitioning to Business Analyst/DA -> BI Engineer then after obtaining experience -> Data Engineer 

I was recommended this path by few Data Engineers Iâ€™ve spoke to that did a similar path, and I also plan to do the Google DA course and Data Camp SQL/Python to get my feet wet!

So knowing my plan, which free cert should I do? Thereâ€™s also a few AWS certification options if yall think those to be beneficial.

(Sorry if I babbled too much!)",1768985635.0,4,7,https://www.reddit.com/r/dataengineering/comments/1qisvk1/help_me_pick_my_free_cert_please/
1qisbbq,How did you land your first Data Engineer role when they all require 2-3 years of experience?,"For those who made it - did you just apply anyway? Do internships or certs actually help? Where did you even find jobs that would hire you?

Appreciate any tips.",1768983496.0,71,87,https://www.reddit.com/r/dataengineering/comments/1qisbbq/how_did_you_land_your_first_data_engineer_role/
1qirtu3,What resources or tutorials helped you get the most advanced knowledge of Polars?,Title says it allâ€¦ i am struggling with Polars and trying to up my game. TIA.,1768981730.0,10,9,https://www.reddit.com/r/dataengineering/comments/1qirtu3/what_resources_or_tutorials_helped_you_get_the/
1qilwgb,Airflow Best Practice Reality?,"Curious for some feedback. I am a senior level data engineer, just joining a new company. They are looking to rebuild their platform and modernize. I brought up the idea that we should really be separating the orchestration from the actual pipelines.  I suggested that we use the KubernetesOperator to run containerized Python code instead of using the PythonOperator. People looked at me like I was crazy, and there are some seasoned seniors on the team. In reality, is this a common practice? I know a lot of people talk about using Airflow purely as an orchestration tool and running things via ECS or EKS, but how common is this in the real world. ",1768963518.0,55,36,https://www.reddit.com/r/dataengineering/comments/1qilwgb/airflow_best_practice_reality/
1qig8ea,Senior DE on on-prem + SQL only â€” how bad is that?,"Hey all,

Iâ€™m a senior data engineer but at my company we donâ€™t use cloud stuff or Python, basically everything is on-prem and SQL heavy. I do loads of APIs, file stuff, DB work, bulk inserts, merges, stored procedures, orchestration with drivers etc. So Iâ€™m not new to data engineering by any means, but whenever I look at other jobs they all want Python, AWS/GCP, Kafka, Airflow, and I start feeling like Iâ€™m way behind.

Am I actually behind? Do I need to learn all this stuff before I can get a job thatâ€™s â€œequivalentâ€? Or does having solid experience with ETL, pipelines, orchestration, DBs etc still count for a lot? Feels like Iâ€™ve been doing the same kind of work but on the â€œwrongâ€ tech stack and now Iâ€™m worried.

Would love to hear from anyone whoâ€™s made the jump or recruiters, like how much not having cloud/Python really matters.",1768949227.0,64,32,https://www.reddit.com/r/dataengineering/comments/1qig8ea/senior_de_on_onprem_sql_only_how_bad_is_that/
1qifo47,How do teams handle environments and schema changes across multiple data teams?,"I work at a company with a fairly mature data stack, but we still struggle with environment management and upstream dependency changes.

Our data engineering team builds foundational warehouse tables from upstream business systems using a standard dev/test/prod setup. That part works as expected: they iterate in dev, validate in test with stakeholders, and deploy to prod.

My team sits downstream as analytics engineers. We build data marts and models for reporting, and we also have our own dev/test/prod environments. The problem is that our environments point directly at the upstream teamsâ€™ dev/test/prod assets. In practice, this means our dev and test environments are very unstable because upstream dev/test is constantly changing. That is expected behavior, but it makes downstream development painful.

As a result:

* We rarely see â€œrealityâ€ until we deploy to prod.
* People often develop against prod data just to get stability (which goes against CI/CD)
* Dev ends up running on full datasets, which is slow and expensive.
* Issues only fully surface in prod.

Iâ€™m considering proposing the following:

* **Dev:** Use a small, representative slice of upstream data (e.g., â‰¤10k rows per table) that we own as stable dev views/tables.
* **Test:** A direct copy of prod to validate that everything truly works, including edge cases.
* **Prod:** Point to upstream prod as usual.

Does this approach make sense? How do teams typically handle downstream dev/test when upstream data is constantly changing?

Related question: schema changes. Upstream tables arenâ€™t versioned, and schema changes arenâ€™t always communicated. When that happens, our pipelines either silently miss new fields or break outright. Is this common? Whatâ€™s considered best practice for handling schema evolution and communication between upstream and downstream data teams?",1768947930.0,9,13,https://www.reddit.com/r/dataengineering/comments/1qifo47/how_do_teams_handle_environments_and_schema/
1qie1ih,Would you recommend running airflow in Kubernetes (Spot),"is anyone actually running Airflow on K8s using only spot instances? Iâ€™m thinking about going full spot (or maybe keeping just a tiny bit of on-demand for backup). If youâ€™ve tried this in prod, did it actually work out?

I understand that spot instances aren't ideal for production environments, but I'm interested to know if anyone has experience with this configuration and whether it proved successful for them.",1768944287.0,3,1,https://www.reddit.com/r/dataengineering/comments/1qie1ih/would_you_recommend_running_airflow_in_kubernetes/
1qidvaz,3yoe SAS-based DE experience - how to position myself for modern DE roles? (EU),"Some context:  
I have 3 years of exp, across a few projects as:  
\- Data Engineer / ETL dev  
\- Data Platform Admin

but most of my commercial work has been on SAS-based platforms. Ik this stack is often considered legacy, and honestly, the vendor locked nature of SAS is starting to frustrate me.

In parallel, I've developed ""modern"" DE skills through a CS degree and 1+ year of 1:1 mentoring under a Senior DE, combining hands-on work in Python, SQL, GCP, Airflow and Databricks/PySpark with coverage of DE theory and I also built a cloud-native end-to-end project.  
So... conceptually, I feel solid in DE fundamentals.

I've read quite a few posts on reddit, about legacy-heavy backgrounds (SAS) beign a disadvantage, which doesn't inspire optimism. I'm struggling to get interviews for DE roles - even at the Junior level, so I'm trying to understand what I'm missing.

Questions:  
\- is the DE market in EU just very tight now?  
\- How is SAS exp actually perceived for modern DE roles?  
\- How would you position this background on a CV/interviews?  
\- Which stack should I realistically double down on for the EU market - should I go allin on one setup (eg. GCP + Databricks), or keep a broader skill set across multiple tools, and are certifications worth it at this stage?

Any feedback is appreciated, especially from people who moved from legacy/enterprise stacks into modern data platforms.",1768943915.0,3,2,https://www.reddit.com/r/dataengineering/comments/1qidvaz/3yoe_sasbased_de_experience_how_to_position/
1qicj6k,I am a data engineer with 2+ years of experience making 63k a year. What are my options?,"I wanted some input regarding my options. My fuck stick employer was supposed to give me my yearly performance review in the later part of last year, but seems to be pushing it off. They gave me a 5% raise from 60k after the first year. I am not happy with how much I am being paid and have been on the look out for something else for quite some time now. However, it seems there are barely any postings on the job boards I am looking at. I live in the US and I currently work remotely. I look for jobs in my city as well as remote opportunities. My current tech stack is Databricks, Pyspark, SQL, AWS and some R.  My experience is mostly characterized by converting SAS code and pipelines to Databricks. I feel like my tech stack and years of experience is too limited for most job posts. I currently just feel very stuck.

  
I have a few questions.

1. How badly am I being underpaid?

2. How much can I reasonably expect to be paid if I were to move to a different position?

3. What should I seek out opportunity wise? Is it worth staying in DE? Should I continue to also search for SWE positions? Is there any other option that's substantially better than what I am doing right now?

  
Thank you for any appropriate answers in advance",1768940981.0,23,39,https://www.reddit.com/r/dataengineering/comments/1qicj6k/i_am_a_data_engineer_with_2_years_of_experience/
1qichn4,What degree should I pursue in college? If Iâ€™m interested in â€œoneâ€ day becoming a data engineer,"Iâ€™m curious: what degree did you guys pursue in college? Since Iâ€™m planning on going back to school. I know itâ€™s discouraging to see the trend of people saying the CS degree is dead, but I think I might pursue it regardless. Should I consider a math, statistics, or data science degree? Also, should I consider grad school? If things donâ€™t work out it doesnâ€™t work out. Iâ€™m just going to pivot. Any advice would help. ",1768940885.0,5,11,https://www.reddit.com/r/dataengineering/comments/1qichn4/what_degree_should_i_pursue_in_college_if_im/
1qib5fr,Hardware engineering for Data Eng,"So a few days ago I watched an interesting article about how to productionise a hardware product.

Then I thought hang on, a LOT of this applies to what we do!

Hence:

Predictable Designs in Data Engineering

https://www.linkedin.com/pulse/predictable-designs-data-engineering-dan-keeley-9vnze?utm_source=share&utm_medium=member_android&utm_campaign=share_via

Worth watching the og (who doesn't love some hardware playing) and would love to know your thoughts!",1768937965.0,1,2,https://www.reddit.com/r/dataengineering/comments/1qib5fr/hardware_engineering_for_data_eng/
1qi7quj,Spending >70% of my time not coding/building - is this the norm at big corps?,"I'm currently a ""Senior"" data engineer at a large insurance company (Fortune 100, US).

Prior to this role, I worked for a healthcare start up and a medium size retailer, and before that, another huge US company, but in manufacturing (relatively fast paced). Various data engineer, analytics engineer, senior analyst, BI, etc roles.

This is my first time working on a team of just data engineers, in a department which is just data engineering teams.

In all my other roles, even ones which had a ton of meetings or stakeholder management or project management responsibilities, I still feel like the majority of what I did was technical work. 

In my current role, we follow Devops and Agile practices to a T, and it's translating to a **single pipeline being about 5-10 hours of data analysis and coding and about 30 hours of submitting tickets to IT requesting 1000 little changes to configurations, permissions, etc and managing Jenkins and GitHub** deployments from unit>integration>acceptance>QA>production>reporting

Is this the norm at big companies? if you're at a large corp, I'm curious what ratio you have between technical and administrative work.",1768930810.0,136,36,https://www.reddit.com/r/dataengineering/comments/1qi7quj/spending_70_of_my_time_not_codingbuilding_is_this/
1qi76hy,Feel too old for a career change to DE,"Hi all - new to the sub as for the last 12 months I've been working towards transitioning from my current job as a project manager/business analyst to data engineering but I feel like a boomer learning how the TV remote works (I'm 38 for reference). I have a built a solid grasp of Python, I'm currently going full force at data architectures and database solutions etc but it feels like when I learn one thing it opens up a whole new set of tech so getting a bit overwhelmed. Not sure what the point of this post is really - anyone else out there who pivoted to data engineering at a similar point in life that can offer some advice?",1768929614.0,30,33,https://www.reddit.com/r/dataengineering/comments/1qi76hy/feel_too_old_for_a_career_change_to_de/
1qi6yc8,Crit cloud native data ingestion diagram,"Can you please crit my data ingestion model? Is it garbage? I'm designing a cloud native data ingestion solution (covering data ingestion only at this stage) and want to combine data from AWS and Azure to manage cloud costs for an organisation. They have legacy data in SharePoint, and can also make use of financial data collected and stored in Oracle Cloud. Having not drawn up one of these before, is there anything major I'm missing or others would do differently?

The solution will continue in Azure only so I am wondering whether an AWS Athena layer is even necessary here as a pre-processing step. Could the data be taken out of the data lake and queried using SQL afterwards? I'm unsure on best practice.

Any advice, crit, tips?

https://preview.redd.it/bufxmm3kfjeg1.jpg?width=889&format=pjpg&auto=webp&s=cbef1cc4f0977a57d42d99ab29447c2820329f15",1768929121.0,3,2,https://www.reddit.com/r/dataengineering/comments/1qi6yc8/crit_cloud_native_data_ingestion_diagram/
1qi0z60,How to prevent spark dataset long running loops from stopping (Spark 3.5+)," anyone run Spark Dataset jobs as long running loops on YARN with Spark 3.5+?

Batch jobs run fine standalone, but wrapping the same logic in while(true) with a short sleep works for 8-12 iterations and then silently exits. No JVM crash, no OOM, no executor lost messages. Spark UI shows healthy executors until gone. YARN reports exit code 0. Logs are empty.

Setup: Spark 3.5.1 on YARN 3.4, 2 executors u/16GB, driver 8GB, S3A Parquet, Java 21, G1GC. Tried unpersist, clearCache, checkpoint, extended heartbeats, GC monitoring. Memory stays stable.

Suspect Dataset lineage or plan metadata accumulates across iterations and triggers silent termination.

Is the recommended approach now structured streaming micro-batches or restarting batch jobs each loop? Any tips for safely running Dataset workloads in infinite loops?",1768915394.0,13,6,https://www.reddit.com/r/dataengineering/comments/1qi0z60/how_to_prevent_spark_dataset_long_running_loops/
1qhzxaw,Anybody using Hex / Omni / Sigma / Evidence?,"Evaluating between these.  
Would love to know what works well and what doesn't while using these tools. ",1768912460.0,6,15,https://www.reddit.com/r/dataengineering/comments/1qhzxaw/anybody_using_hex_omni_sigma_evidence/
1qhyv0a,Load data from S3 to Postgres,"Hello,

Goal:  
I need to reliably and quickly load files from S3 to a Postgres RDS instance.

Background:  
1. I have an ETL pipeline where data is produced to sent to S3 landing directory and stored under customer\_id directories with a timestamp prefix.  
2. A Glue job (yes I know you hate it) is scheduled every hour, discovers the timestamp directories, writes them to a manifest and fans out transform workers per directory (customer\_id/system/11-11-2011-08-19-19/ for example). transform workers make the transformation and upload to s3://staging/customer\_id/...  
3. Another Glue job scans this directory every 15 minutes, picks up staged transformations and writes them to the database   
  


Details:  
1. The files are currently with Parquet format.  
2. Size varies. ranges from 1KB to 10-15MB where medial is around 100KB  
3. Number of files is at the range of 30-120 at most.

State:  
1. Currently doing delete-overwrite because it's fast and convenient, but I want something faster, more reliable (this is currently not in a transaction and can cause some sort of an inconsistent state) and more convenient.  
2. No need for columnar database, overall data size is around 100GB and Postgres handles it easily.



I am currently considering two different approached:  
1. Spark -> staging table -> transactional swap  
Pros: the simpler of the two, not changing data format, no dependencies  
Cons: Lower throughput than the other solution.

2. CSV to S3 --> aws\_s3.table\_import\_from\_s3  
Pros: Faster and safer.  
Cons: Requires switching from Parquet to CSV at least in the transformation phase (and even then I will have a mix of Parquet and CSV, which is not the end of the world, but still), requires IAM access (barely worth mentioning).

  
Which would you choose? is there an option 3?",1768909185.0,3,6,https://www.reddit.com/r/dataengineering/comments/1qhyv0a/load_data_from_s3_to_postgres/
1qhycmc,Airflow 3.0.6 fails task after ~10mins,"Hi guys, I recently installed Airflow 3.0.6 (prod currently uses 2.7.2) in my companyâ€™s test environment for a POC and tasks are marked as failed after ~10mins of running. Doesnâ€™t matter what type of job, whether Spark or pure Python jobs all fail. Jobs that run seamlessly on prod (2.7.2) are marked as failed here. 
Another thing I noticed about the spark jobs is that even when it marks it as failed, on the Spark UI the job would still be running and will eventually be successful. 
Any suggestions or advice on how to resolve this annoying bug? ",1768907481.0,8,5,https://www.reddit.com/r/dataengineering/comments/1qhycmc/airflow_306_fails_task_after_10mins/
1qhu7di,Switch domain to data engineering,"I am currently working as an embedded/automotive software engineer and have been thinking seriously about switching to data engineering. Iâ€™ve been reading mixed opinions online, so I wanted to hear from people who are actually in the field.

My main questions are:

1.How are job opportunities right now for data engineers, especially for someone switching domains?

2.What does the salary progression realistically look like (not the inflated YouTube numbers)?

3.Is data engineering still expected to have long-term demand, or is the market getting saturated?

I am already comfortable with programming and system-level thinking, and Iâ€™m starting to learn Python.

Would really appreciate honest advice from people working as data engineers or who have made a similar switch",1768892610.0,2,4,https://www.reddit.com/r/dataengineering/comments/1qhu7di/switch_domain_to_data_engineering/
1qhpdfc,Need Guidance,"I am currently working at TCS and have completed one year in a Production Support role. My day-to-day work mainly involves resolving tickets and generating reports using PL/SQL, including procedures, functions, cursors, and debugging existing code.

However, after spending more than a year in this role, I genuinely feel stuck. There has been very little growth in my career, my financial savings have not improved, and over time it has started affecting my health as well. This situation has been mentally exhausting, and I often feel uncertain about where my career is heading.

Because of this, I am now thinking seriously about switching to a different role or moving into a new domain. I am interested in the data field, especially Data Engineering, but at the same time, I am scared of the current job market and worried about making the wrong decision. I constantly find myself overthinking whether this switch is right for me or whether I should continue in my current role.

At this point, I feel confused and stuck, and I truly need guidance. If anyone has been in a similar situation or has experience in this field, I would really appreciate your advice on whether transitioning into Data Engineering would be a good choice for someone with my background and how I should approach this change.

Thank you for taking the time to read this.",1768878067.0,3,1,https://www.reddit.com/r/dataengineering/comments/1qhpdfc/need_guidance/
1qhp36r,Databricks Lakeflow,"Anyone mind explaining where Lakeflow comes into play and how the Databricks' architecture works?

I've been reading articles online and this is my understanding so far, though not sure if correct \~

\- Lakehouse is a traditional data warehouse  
\- Lakebase is an OLTP database that can be combined with lakehouse to give databases functionality for both OLTP and data analytics (among other things as well that you'd get in a normal data warehouse)  
\- Lakeflow has to do something with data pipelines and governance, but trying to understand Lakeflow is where I've gotten confused.

Any help is appreciated, thanks!",1768877300.0,5,5,https://www.reddit.com/r/dataengineering/comments/1qhp36r/databricks_lakeflow/
1qho5x2,School Project for Beginner DE,"Hello everyone,   
I am currently going to college and doing a capstone project this semester. I am currently pursuing a Junior DE roles, therefore I want to take the role of Data Engineering in this group project as an opportunity to work on the skills. I can write Python, SQL and also taking a 9-week Data Engineering course on the side (not this capstone course) to build up more skills and tool using.   
I am writing this post to ask any project ideas that I should do for the capstone project where I can work on DE part. I am willing to do as I learn from the project since I understand that my DE skills is at the beginning phase, but want to take this opportunity to strengthen the DE knowledge and logics. ",1768874806.0,5,14,https://www.reddit.com/r/dataengineering/comments/1qho5x2/school_project_for_beginner_de/
1qhk6rc,Is shifting to data engineering really a good choice in this market.,"Hi, I am a CS graduate of 2023, Iâ€™ve worked as a data analyst intern for about 8 months and rest 4 months got barely any pay. The only good part about that was I got learn and have a good hands on experience in python and little bit of sql.

After that I switched to Digital Marketing along with Data Analysis and worked here for a year too.

Now, I have been laid off a month ago due to AI, and I thought Iâ€™ll take my time to study and prepare for GCP Professional Data Engineering certification.

Right now I am very confused and cannot decide if doing this is actually a good move and a good choice for my career specially in this current job market.

Right now I have started preparing for this certification through Googleâ€™s materials and udemy course and other materials. I plan to take the test in the next 3 months.

Would genuinely appreciate some guidance, opinions and advice on this.

Would also appreciate guidance for the gcp pde test.",1768864644.0,26,20,https://www.reddit.com/r/dataengineering/comments/1qhk6rc/is_shifting_to_data_engineering_really_a_good/
1qhjnv4,"Anyone else going to Data Day Texas, want to meet up?",Anyone else going to Data Day Texas 2026? Can you explain what the Sunday Sessions thing is about?,1768863405.0,3,2,https://www.reddit.com/r/dataengineering/comments/1qhjnv4/anyone_else_going_to_data_day_texas_want_to_meet/
1qhjb83,Transition from SDET role to Entry Data Engineer,"Disclaimer: I know there are a few of these ""transition"" posts, but I could never find anything on the Software Development Engineer in Test (SDET) transition experience.

I have been stuck in SDET style roles with attempts to transition into Data Engineering roles from within organizations. The moment I have a potential spot open to transition to, I am laid off. I am on unemployment now and likely going to be focusing on some training before submitting applications for entry level data engineering roles. I have touched some data warehousing and data orchestration tools while in my SDET role.

**Experience:** 

6 YOE in Test Automation

Bachelor of Science in Computer Science

**DE related experience I had were:**

Snowflake - Used to query test result data from a data lake we had, but the columns seemed to already be established by the data engineers. So it was mostly just SQL and working in worksheets

Airflow - Used as an orchestrator for our test execution and data provisioning environments

I found that I was most excited about this kind of work, I understand completely that the role involves much more than that. Should I start with some certifications, projects, or some formal training? Any help is welcome!

  
Edit: Added Experience",1768862578.0,3,2,https://www.reddit.com/r/dataengineering/comments/1qhjb83/transition_from_sdet_role_to_entry_data_engineer/
1qhi4lr,Designing Data-Intensive Applications,"First off, shoutout to the guys on the Book Overflow podcast. They got me back into reading, mostly technical books, which has turned into a surprisingly useful hobby.

Lately Iâ€™ve been making a more intentional effort to level up as a software engineer by reading and then trying to apply what I learn directly in my day-to-day work.

The next book on my list is Designing Data-Intensive Applications. Iâ€™ve heard nothing but great things, but I know an updated edition is coming at some point.

For those whoâ€™ve read it: would you recommend diving in now, or holding off and picking something else in the meantime?",1768859892.0,64,16,https://www.reddit.com/r/dataengineering/comments/1qhi4lr/designing_dataintensive_applications/
1qhcekr,Any data engineers here with ADHD? What do you struggle with the most?,"Iâ€™m a data/analytics engineer with ADHD and Iâ€™m honestly trying to figure out if other people deal with the same stuff.

My biggest problems

\- I keep forgetting config details. YAML for Docker, dbt configs, random CI settings. I have done it before, but when I need it again my brain is blank.

\- I get overwhelmed by a small list of fixes. Even when itâ€™s like 5 â€œeasyâ€ things, I freeze and canâ€™t decide what to start with.

\- I ask for validation way too much. Like Iâ€™ll finish something and still feel the urge to ask â€œis this right?â€ even when nothing is on fire. Feels kinda toddler-ish. 

\- If I stop using a tool for even a week, I forget it. Then Iâ€™m digging through old PRs and docs like I never learned it in the first place.

\- Switching context messes me up hard. One interruption and it takes forever to get my mental picture back.

Iâ€™m not posting this to be dramatic, I just want to know if this is common and what people do about it.

If youâ€™re a data engineer (or similar) with ADHD, what do you struggle with the most? 

Any coping systems that actually worked for you? Or do you also feel like youâ€™re constantly re-learning the same tools?

Would love to hear how other people handle it.",1768847490.0,149,86,https://www.reddit.com/r/dataengineering/comments/1qhcekr/any_data_engineers_here_with_adhd_what_do_you/
1qhb7xp,AI Landscape Visualization,"Hi, I'm a enterprise data architect with a an large government organization.  We have many isolated projects all pursuing AI capabilities of some sort, each of which using a different tool or platform.  This lead to a request to show a depiction of how all of our AI tools overlap with the AI capabilities, with the idea of show all the redundancy. I typically call this a capabilities map or a landscape visualization that shows many of the tools that are perform that capability.  Usually I'm able to find a generic one from a 3rd party analyst like Gardener but I have been unable to find one for AI that isn't focused on AI categories.  I'm posting to see if anyone has seen anything like this for AI and can maybe point in the right direction. 

This the the type of visualization I'm looking for, this one is focused on data tools.

https://preview.redd.it/nofa8kcybceg1.png?width=1586&format=png&auto=webp&s=3d121eded977b0c2f03388d819a13ab2d93dbb05

Here are some the tools we're looking to put on the diagram, it isn't limited to these but these are some of the overlaps we know of.

* Databricks
* AWS Bedrock
* AWS Sagemaker
* OpenAI
* ChatGPT
* CoPilot
* Sharepoint (it's our content repository)",1768845027.0,0,1,https://www.reddit.com/r/dataengineering/comments/1qhb7xp/ai_landscape_visualization/
1qhawfu,Apache Arrow for the Database,It's super cool to see the Apache Arrow world coming into the database world!,1768844369.0,6,0,https://www.reddit.com/r/dataengineering/comments/1qhawfu/apache_arrow_for_the_database/
1qha2l9,"Context graphs: buzzword, or is there real juice here?",,1768842657.0,56,16,https://www.reddit.com/r/dataengineering/comments/1qha2l9/context_graphs_buzzword_or_is_there_real_juice/
1qh9fwt,Export to Excel Song,"[https://www.youtube.com/watch?v=AeMSMvqkI2Y](https://www.youtube.com/watch?v=AeMSMvqkI2Y)

We now have a hit song that describes much of the reality of the data engineering profession.",1768841353.0,0,1,https://www.reddit.com/r/dataengineering/comments/1qh9fwt/export_to_excel_song/
1qh9d0d,Designing a data lake,"Hi everyone,

Iâ€™m a junior ML engineer, I have 2 years experience so Iâ€™m not THAT experienced and especially not in this.

Iâ€™ve been asked in my current job to design some sort of data lake to make the data independent from our main system and to be able to use this data for future projects such as ML and whatnot. 

To give a little context, we already have a whole IT department working with the â€œmainâ€ company architecture. We have a very centralized system with one guy supervising every in and out, heâ€™s the one who designed it and he gives little to no access to other teams like mine in R&D. Itâ€™s a mix of AWS and on-prem.

Everytime we need to access data, we either have to export them manually via the software (like a client would do) or if we are lucky and there is already an API that is setup we get to use it too. 

So my manager gave me the task to try to create a data lake (or whatever the correct term might be for this) to make a copy of the data that already exists in prod and also start to pump data from the sources used by the other software. And by doing so, weâ€™ll have the same data but weâ€™ll have it independently whenever we want.

The thing is I know that this is not a simple task and other than the courses I took on DBs at school, I never designed or even thought about anything like this. I donâ€™t know what would be the best strategy, the technologies to use, how to do effective logsâ€¦.

The data is basically a fleet management, there are equipment data with gps positions and equipment details,  there are also events like if equipment are grouped together then they form a â€œjobâ€ with ids, start date, locationâ€¦ so itâ€™s a very structured data so I believe a simple sql db would suffice but Iâ€™m not sure if itâ€™s scalable.

I would appreciate it if I could get some kind of books to read or leads that I should follow to at least build something that might not break after two days.",1768841183.0,2,1,https://www.reddit.com/r/dataengineering/comments/1qh9d0d/designing_a_data_lake/
1qh6wo1,Crippling your Data Engineers,"I'm working as a contractor for a client where I have to log onto a GDE terminal. The window size is fixed and the resolution is probably 800x600.
You can't copy/paste between your host and the GDE so be prepared to type a 24character strong password. Session time outs are aggressive so expect to type this a lot.

GDEs are notoriously slow. This one sets a new record. The last time I saw something this slow was when I had to use an early Amstrad laptop with dial up modem to connect to an HP3000 mini computer. In 2026, I've been assigned kit that wasn't impressive in 1989.

I'd love to know the justification for this fetid turd of an environment.",1768835914.0,37,10,https://www.reddit.com/r/dataengineering/comments/1qh6wo1/crippling_your_data_engineers/
1qh5bo7,How Vinted standardizes large-scale decentralized data pipelines,,1768832282.0,14,2,https://www.reddit.com/r/dataengineering/comments/1qh5bo7/how_vinted_standardizes_largescale_decentralized/
1qh4sp0,Databricks vs AWS self made,"I am working for a small business with quite a lot of transactional data (around 1 billion lines a day). We are 2-3 data devs.
Currently we only have a data lake on s3 and transform data with spark on emr. Now we are reaching limits of this architecture and we want to build a data lakehouse. We are thinking about these 2 options:

- Option 1: Databricks 
- Option 2: connect AWS tools like S3, EMR, Glue, Athena, Lake Formation, Data Zone, Sage Maker, Redshift, airflow, quick sight,...

What we want to do:
- Orchestration
- Connect to multiple different data sources, mainly APIs
- Cataloging with good exploration 
- governance incl fine grained access control and approval flows
- Reporting
- self service reporting
- Ad hoc SQL queries 
- self service SQL
- Posgres for Website (or any other OLTP DB)
- ML
- Gen Ai (eg RAG, talk to data use cases)
- share data externally 

Any experiences here? Opinions? Recommendations?",1768831018.0,31,64,https://www.reddit.com/r/dataengineering/comments/1qh4sp0/databricks_vs_aws_self_made/
1qh4esu,Cloudflare Pipelines + Iceberg on R2 + Example Open Source Project,"Afternoon folks, long time lurker, first time poster.    I have spent some time recently getting up to speed with different ways to work with data in/out of Apache Iceberg and exploring different analytics tools / visualisation options.  I use Cloudflare a lot for my side projects, and have recently seen the 'Beta' data platform products incl. the idea of https://developers.cloudflare.com/r2/data-catalog/.    
  
So, I decided to give it a go and see if I can build a real end to end data pipeline (the example is product analytics in this case but you could use it for other purposes of course).  I hope the link to my own project is OK, but it's MIT / open source: https://github.com/cliftonc/icelight.

My reflections / how it works:

\- Its definitely a beta, as I had to re-create the pipelines once or twice to get it all to actually sync through to R2 ... but it really works!  
\- There is a bit of work to get it all wired up, hence why I created the above project to try and automate it.  
\- You can run analytics tools (in this example DuckDB - https://duckdb.org/) in containers now and use these to analyse data on R2.  
\- Workers are what you use to bind it all together, and they work great.  
\- I think (given zero egress fees in R2) you could run this at very low cost overall (perhaps even inside the free tier if you don't have a lot of data or workload).   No infrastructure at all to manage, just 2 workers and a container (if you want DuckDB).  
\- I ran into quite a few issues with DuckDB as I didn't fully appreciate that its single process constraints - I had always assumed it was actually a real server - but actually it seems to now work very well with a bit of tweaking, and the fact it is near Postgres capable but running on parquet files on R2 is nothing short of amazing.  
\- I have it flushing every minute at the moment to R2, not sure what this means longer term but will send a lot more data at it over coming weeks and see how it goes. 

Happy to talk more about it if anyone is interested in this, esp. given Cloudflare is very early into the data engineering world. I am in no way affiliated with Cloudflare, though if anyone from Cloudflare is listening I would be more than happy to chat about my experiences :D 



",1768830029.0,7,2,https://www.reddit.com/r/dataengineering/comments/1qh4esu/cloudflare_pipelines_iceberg_on_r2_example_open/
1qh2653,Did anyone use Strategy One (previously known as MicroStrategy) in building a Semantic Layer (Mosaic Model),"Hello guys, sorry in advance for the long post. 

I am currently trying Strategy One to build a Semantic Layer, I got the 30 days free trial and I was testing the tool.

I am facing a very weird situation with connecting to DBeaver and Query my data.  
I have generated some random data with 1,000 Customers and 3,000 Bills (Telecom Data),  
Not all the Customers have bills (only 948 have bills)

I have created 2 models, 1st one using some of the data on a SQL Server Database and the rest using CSV, and the 2nd model only the data from SQL Server.

# 1st model (SQL + CSV):

  
\- total records = 3,000  
\- count(distinct customer\_id) returns 1,000 HOWEVER when you check the data manually there is no 1,000 distinct customer\_id  
\- select distinct customer\_id will return 1,000 IDs (which is not the case as there is only 948 distinct ID)

# 2nd model (SQL only):

\- total records = 3,052  
\- count(distinct bill\_id) returns 3,000  
\- count(distinct customer\_id) returns 1000  
\- count of duplicated bills return 0  
\- count of records with NULL bill\_id returns 0 HOWEVER when I checked the data manually I found 52 records with NULL bill\_id 

My main 2 Questions are:  
1- How to select the joining behavior between the tables (inner join, left join,..)  
2- Why are the Queries acting that weird? ",1768823582.0,3,8,https://www.reddit.com/r/dataengineering/comments/1qh2653/did_anyone_use_strategy_one_previously_known_as/
1qgzdju,Leveraging Legacy Microsoft BI Stack Exp.,"I have experience with out cloud platforms, but not Azure. Despite this I have experience with SSRS, SSIS, SSAS, SSRS, and some Power BI.

Azure would be a nice feather to add to my bow given my past experience with Microsoft BI stack, but my question is what are some good resources to learn for data services in Azure?",1768813758.0,3,4,https://www.reddit.com/r/dataengineering/comments/1qgzdju/leveraging_legacy_microsoft_bi_stack_exp/
1qgyrpt,Understanding schema from DE rather than DA,"I am a DA and am fairly confident in SQL. 

I can happily write long queries with CTEs, debug models up and downstream and I am comfortable with window functions. 

I am working in DE as my work as afforded me the flexibility to learn some new skills. I am onboarding 25 tables from a system into snowflake. 

Everything is working as intended but I am confused about how to truncate and insert daily loads. 

I have a schema for the 25 tables and how they fit together but I'm unsure how to read it from an ingestion standpoint. 

It works by the main tables loading 45 past dates and 45 future dates every day. So I can remove that time chunk in the truncate task and then reinsert it with the merge task using streams for each. The other tables ""fork"" out from here with extra details to do with those events.

What I'm unsure of is how to interact with data that is removed from the system, since this doesn't show up in the streams. Ie, a staff time sheet from 2 weeks ago gets changed to +30 minutes or something. 

In the extremity tables, there is no dates held within.

If I run a merge task using the stream from that days ingestion, what do I use as the target for truncate tasks?

What is the general thought process when trying to understand how the schema fits together in regards to inserting and streaming the data rather than just making models?

Thanks for the help!",1768811539.0,4,1,https://www.reddit.com/r/dataengineering/comments/1qgyrpt/understanding_schema_from_de_rather_than_da/
1qgy9rx,Validating a 30Bn row table migration.,"Iâ€™m migrating a table from one catalog into another in Databricks.

I will have a validation workspace which will have access to both catalogs where I can run my validation notebook.

Beyond row count and schema checks, how can I ensure the target table is the exact same as source post migration?

I donâ€™t own this table and it doesnâ€™t have partitions.

If we wanna chunk by date, each chunk would have about 2-3.5Bn rows.",1768809707.0,18,21,https://www.reddit.com/r/dataengineering/comments/1qgy9rx/validating_a_30bn_row_table_migration/
1qgv0fv,Advice for an open-source tech stack,"Hi everyone, Im working on a personal project with the idea of â€‹â€‹analyzing data from core systems including MES, ERP, internal app, each system having its own users and databases. The problem is how to consolidate data from these systems' databases into one place to generate reports, ensuring that users from each system can only view data from that system, as before. I'm considering using: Airbyte, MinIO, Iceberg, Trino, OpenMetadata, Metabase, Dagster.

However, I find these techstacks quite complex to manage and set up. Are there any simpler stacks that can still be applied to businesses?",1768798900.0,5,8,https://www.reddit.com/r/dataengineering/comments/1qgv0fv/advice_for_an_opensource_tech_stack/
1qguef9,Powerbi data gateway,"I know this may be a stupid question, but my skillset mainly is in serverless architecture. I am trying to create a bootstrap for an ec2 instance to download the AWS Athena odbc 2 connector as well as the Microsoft on premise data gateway. I am trying to find a way to reliable have this bootstrap work (for example, what if the link itâ€™s downloading from changes). Iâ€™m thinking of having a script that runs in GitHub on a schedule to pull the installers and upload them into s3 for the bootstrap to reference. That way even if a link changes I have versioned installers I can use. What do you think? Is there a better way? Am I over engineering this? Maybe the links are constant and I just download it directly in the bootstrap code. ",1768797081.0,2,2,https://www.reddit.com/r/dataengineering/comments/1qguef9/powerbi_data_gateway/
1qgl0av,Shall I move into Data Engineering at the age of 38,"Hello All. Need advice on my carrier switch plan. I am 38 currently and have 14 years of experience as a QA including close to 2 years of experience as a Manual ETL tester/QA. I know Python programming and I am very drawn to programming. I am considering learning and switching to become a Data Engineer (Developer). My question is, is it a good decision to make this carrier move at the age of 38. Also please suggest what kind of roles should I target ? Should I target beginner level or Mid Seniour Levle or Lead level considering my previous 14 years of experience. Please suggest.",1768772248.0,31,54,https://www.reddit.com/r/dataengineering/comments/1qgl0av/shall_i_move_into_data_engineering_at_the_age_of/
1qgdliv,Data streaming project pipeline,"Hi!

I'm getting into my first data engineering project. I picked google as a provider and the project is using realtime carpark data api (fetching via python) to then visualise it on a frontend. The data will be needed to be processed as well. Im not too sure what the whole data piepline will look like for streaming data so im looking for some advice. Particulary on the whole flow and what each step does. Thanks!",1768755026.0,9,4,https://www.reddit.com/r/dataengineering/comments/1qgdliv/data_streaming_project_pipeline/
1qgcog8,"Order of Books to Read: (a) The Data Warehouse Toolkit, (b) Designing Data-Intensive Applications, (c) Fundamentals of Data-Engineering","For someone who wants to enter the field and work as a data engineer this year, whose skills include basic SQL and (watched some) Python (tutorials), in what order should I read the books stated in the title (and why)? Should I read them from cover to cover? If there are better books/resources to learn from, please state those as well. Also, I got accepted in the DE Zoomcamp but I still have not started on it yet since I got so busy.

Thanks in advance!",1768752927.0,38,14,https://www.reddit.com/r/dataengineering/comments/1qgcog8/order_of_books_to_read_a_the_data_warehouse/
1qgbu67,Do you guys use vs code or jupyterlab for jupyter notebooks?,"hey guys. I used jupyterlab a lot. But trying to migrate to more standard ide. but seems like vs code is too verbose for jupyter. Even if I try to zoom out, output stays same size. So it seems as if I can see a lot less in one frame in vsc compared to jupyterlab. it adds so much random padding below output and inside cells too. 

I generally stay at 90% zoom in jupyter in my browser. but with vsc the amount I see is close 110% zoom in jupyterlab. and I can't find a way to customise it. anyone knows any solution or has faced this problem.",1768751045.0,17,26,https://www.reddit.com/r/dataengineering/comments/1qgbu67/do_you_guys_use_vs_code_or_jupyterlab_for_jupyter/
1qg4ve4,"Solo devs making apps and senior devs of reddit, what to learn as an intern in the age of vibe coding for career progression???","Onto making projects, prompting, system design and dsa already...

Open to all kinda thoughts opinions...",1768730701.0,1,6,https://www.reddit.com/r/dataengineering/comments/1qg4ve4/solo_devs_making_apps_and_senior_devs_of_reddit/
1qg2t2x,Help with Lakehouse POC data,"I've built a homelab lakehouse: MinIO (S3), Apache Iceberg, Hive Metastore, Spark, Airflow DAGs. And I need sample data to practice.

* Where to grab free datasets <100GB (Parquet/CSV ideal) for medallion practice? Tried NYC Taxi subsets/TPC-DS gens?
* Is medallion (bronze/silver/gold) the only layering, or will you have something else? Monitoring tools for pipelines/data quality (beyond Netdata)? Costs/scaling pains?
*  Best practices welcome!

Thanks.",1768723412.0,6,9,https://www.reddit.com/r/dataengineering/comments/1qg2t2x/help_with_lakehouse_poc_data/
1qg2qf4,MapReduce on Spark. Smooth transition available?,"My team took over some projects recently. Many things need an upgrade. One of those is moving MapReduce jobs to run on Spark. However, the compute platform team tells us that classic MapReduce is not available. Only modern compute engines like Spark, Flink, etc. are supported.

Is there a way to run classic Hadoop MapReduce jobs on Spark? Without any code changes? My understanding is that the Map->Shuffle&Sort->Reduce is just a special case for Spark to do a batch.

Most of the MapReduce jobs are just pulling data from HDFS (which are tied to a Hive table indivdually), doing some aggregation (e.g. summing up the cost & revenue of a day), then writing back to HDFS for another Hive table to consume. Data are encoded in Protobuf, not Parquet, yet.",1768723171.0,2,5,https://www.reddit.com/r/dataengineering/comments/1qg2qf4/mapreduce_on_spark_smooth_transition_available/
1qg2hus,Data Engineering Guidance Help,"So lately i had like ups and downs choosing a career path for me. Tried different things for like good time to get my hands on it, but nothing suited me well enough to pursue it.  
I am new to this DE. Have seen a lot of posts and videos about it. Doing self learning, and have not enrolled in any courses. I have a guide like which is generated by Claude based on what i know. I am not assuming the guide to be a launch pod for me, just a direction of what to do next.   
I am really confused that whether DE is the right for me or not.  
Your feedback would be greatly appreciated

",1768722312.0,13,14,https://www.reddit.com/r/dataengineering/comments/1qg2hus/data_engineering_guidance_help/
1qg2fz2,dc-input: turn any dataclass schema into a robust interactive input session,"Hi all! I wanted to share a Python library Iâ€™ve been working on. Feedback is very welcome, especially on UX, edge cases or missing features.

[https://github.com/jdvanwijk/dc-input](https://github.com/jdvanwijk/dc-input)

**What my project does**

I often end up writing small scripts or internal tools that need structured user input. â€‹This gets tedious (and brittle) faâ€‹stâ€‹, especiallyâ€‹ once you add nesting, optional sections, repetition, â€‹etc.

This â€‹library walks aâ€‹â€‹ dataclass schema insteadâ€‹ and derives an interactive input session from it (nested dataclasses, optional fields, repeatable containers, defaults, undo support, etc.).

For an interactive session example, see:Â [https://asciinema.org/a/767996](https://asciinema.org/a/767996)

â€‹This has been mostly been useful for me in internal scripts and small tools where I want structured input without turning the whole thing into a CLI framework.

\------------------------

For anyone curious how this works under the hood, here's a technical overview (happy to answer questions or hear thoughts on this approach):

The pipeline I use is: schema validation -> schema normalization -> build a session graph -> walk the graph and ask user for input -> reconstruct schema. In some respects, it's actually quite similar to how a compiler works.

**Validation**

The program should crash instantly when the schema is invalid: when this happens during data input, that's poor UX (and hard to debug!) I enforce three main rules:

* Reject ambiguous types (example:Â str | intÂ -> is the parser supposed to chooseÂ strÂ orÂ int?)
* Reject types that cause the end user to input nested parentheses: this (imo) causes a poor UX (example:Â list\[list\[list\[str\]\]\]Â would require the user to typeÂ ((str, ...), ...)Â )
* Reject types that cause the end user to lose their orientation within the graph (example: nested schemas asÂ dictÂ values)

None of the following steps should have to question the validity of schemas that get past this point.

Normalization

This step is there so that further steps don't have to do further type introspection and don't have to refer back to the original schema, as those things are often a source of bugs. Two main goals:

* Extract relevant metadata from the original schema (defaults for example)
* Abstract the field types into shapes that are relevant to the further steps in the pipeline. Take for example aÂ ContainerShape, which I define as ""Shape representing a homogeneous container of terminal elements"". The session graph further up in the pipeline does not care if the underlying type isÂ list\[str\],Â set\[str\]Â orÂ tuple\[str, ...\]: all it needs to know is ""ask the user for any number of values of type T, and don't expand into a new context"".

**Build session graph**

This step builds a graph that answers some of the following questions:

* Is this field a new context or an input step?
* Is this step optional (ie, can I jump ahead in the graph)?
* Can the user loop back to a point earlier in the graph? (Example: after the last entry ofÂ list\[T\]Â where T is a schema)

**User session**

Here we walk the graph and collect input: this is the user-facing part. The session should be able to switch solely on the shapes and graph we defined before (mainly for bug prevention).

The input is stored in an array ofÂ UserInputÂ objects: these are simple structs that hold the input and a pointer to the matching step on the graph. I constructed it like this, so that undoing an input is as simple as popping off the last index of that array, regardless of which context that value came from. Undo functionality was very important to me: as I make quite a lot of typos myself, I'm always annoyed when I have to redo an entire form because of a typo in a previous entry!

Input validation and parsing is done in a helper module (\_parse\_input).

**Schema reconstruction**

Take the original schema and the result of the session, and return an instance.",1768722117.0,2,3,https://www.reddit.com/r/dataengineering/comments/1qg2fz2/dcinput_turn_any_dataclass_schema_into_a_robust/
1qfznzw,Is Data Engineering the next logical step for me as an Insights Analyst,"I had a read over the excellent wiki maintained here and I'm definitely leaning towards ""yes"" as the answer to this, but wanted to hear some first-hand experience about my specific situation.

**TLDR Summary:**

* Getting burnt out/bored of data analysis after 13 years (especially stakeholder management).
* Am comfortable on my current pay grade and don't need high vertical movement.
* Want to learn new skills that will be useful going into the agentic future.
* Enjoy building data models & working with Streamlit/Python to create small applications.
* No experience in ITIL structure as I've always been in a merchandising/marketing team.

**Longer Details**

I've been working in the customer intelligence/insights field in FMCG for over 13 years now. I've been in my most recent role for 3 years and I realised at about the 2 year mark that I was about as senior as I could get in my team and was not learning anything new - simply applying my extensive experience/knowledge on the field to providing solutions/analyses for stakeholders.

That realisation combined with a live demonstration from Snowflake on the agentic future of analysis got me looking for the next logical career step. There is a data engineering secondment opportunity in my org that will likely be made permanent, so I really need to knuckle down and decide.

Over the last year, the most enjoyment that I've had has not been when providing any insights, but in building data models for reports and in creating small tools using Streamlit/Python to help stakeholders/team members self-serve and reduce friction. The tool-building component is what I found the most enjoyment in because it was all new with a steep/rapid learning curve. Creating the actual permanent BI reports is also interesting and rewarding, but less engaging because there's generally less of a problem to solve there. What does everyone find the most engaging about their role in data engineering?

I don't actually have any formal training or experience in working in an ITIL system as I've always operated under a merchandising/marketing team structure, so I'm not sure at all how I'd go with a more rigid structure and more rigid processes. Has anyone moved in a similar fashion from a more ""fast and loose"" team to a more structured/process-heavy team? What are the pros/cons there?

And finally, what does everyone find the most frustating about data engineering? From my brief exposure to the teams that handle it, I imagine that it would be stuff like; undocumented data sources and trying to find the correct joins, validation and constant data quality checks, getting clear answers from a BA on the requirements so that you can create an efficient schema/model.",1768712975.0,4,3,https://www.reddit.com/r/dataengineering/comments/1qfznzw/is_data_engineering_the_next_logical_step_for_me/
1qfxoqp,Need some suggestions,"If you have experience in Azure cloud and there are openings for your same experience but in AWS and GCP, will they consider your profile and shortlist you?

",1768707121.0,2,3,https://www.reddit.com/r/dataengineering/comments/1qfxoqp/need_some_suggestions/
1qfv0iu,Data solutions analyst,"A recruiter reached back out to me for a data solutions analyst and Iâ€™m trying to prepare. What are some technical or behavioral questions they ask about this role. Also to my understanding it seems like a data solutions is something similar to a data analyst and a data engineer or a business analyst? 

Tbh I kind of got lucky they hit me back up cuz I feel a little under qualified but I really want a chance to land the job. The post also mentioned an assessment test. Any advice would be appreciated 

This is the job description if it helps:

Demonstrate understanding of data and KPIâ€™s while holding oneself and others accountable to Golden 1 principles of member commitment to service excellence.

Understands and utilizes data warehouse model to write, maintain, and deploy custom T-SQL scripts primarily, but not exclusively, via Microsoft SSMS.

Conduct exploratory data investigation resulting of the interplay between operational process and source data representation, implementing code changes and quality controls as necessary.

Interpret and synthesize ad hoc data requests originated from business partners, teasing out precise verbiage and root cause to provide accurate and tailored quantitative guidance.

Develop and curate custom data sets for inspired and poignant Power BI data visualizations that are effective at every level of the organization.

Build relationships and think critically about people, process, and technology to identify areas for streamlining and improvement.

Embody and promote integrity of the data governance strategy, partner with our IT and Data Warehouse teams to provide research and support at each level of the ETL process.",1768699650.0,5,4,https://www.reddit.com/r/dataengineering/comments/1qfv0iu/data_solutions_analyst/
1qfumps,Building portfolio projects in azure.,"I was thinking on building two or three projects to practice my cloud skills and understand some system designs. But not sure if I will just be wasting my money away without getting any actual benefit. I could build everything locally ofc, but the idea is to get familiar with cloud systems. Any piece of advice? Has anyone tried this before? ",1768698621.0,14,4,https://www.reddit.com/r/dataengineering/comments/1qfumps/building_portfolio_projects_in_azure/
1qfu4xb,First project for a government use case,"In terms of use cases, itâ€™s already defined and approved by project sponsor.

The team will have frontend engineers, devops, devsecops engineers. What are the clarifying questions that I should ask them in the early to mid term stage?

I am curious how to collaborate with people outside of DE, or does our work not overlap with each other?",1768697294.0,3,1,https://www.reddit.com/r/dataengineering/comments/1qfu4xb/first_project_for_a_government_use_case/
1qfrqq5,DBT Platinum Models,"I know medallion arch is kinda in the hands of the beholder at any given company but I am thinking through a reporting layer on top of some gold fact tables and cleaned silver layer models that are built in top of some Salesforce objects. 

The straight up question I have is the reporting level or platinum level models (at least that is what my company calls this layer lol) okay to have static table references with the DBT pointers to source tables?",1768691092.0,2,2,https://www.reddit.com/r/dataengineering/comments/1qfrqq5/dbt_platinum_models/
1qfpglp,"Do you use a dedicated Landing layer, or dump straight into Bronze?","Settling a debate at work: Are you guys still maintaining a Landing/Copper layer (raw files), or are you dumping everything straight into Bronze tables?

Also, how are you handling idempotency at the landing or bronze layers? Is your Bronze append-only, or do you use logic to prevent double-dipping raw data?",1768685448.0,29,25,https://www.reddit.com/r/dataengineering/comments/1qfpglp/do_you_use_a_dedicated_landing_layer_or_dump/
1qfoxst,How many meetings / ad-hoc calls do you have per week in your role,"Iâ€™m trying to get a realistic picture of what the day-to-day looks like. Iâ€™m mostly interested in:

1. number of scheduled meetings per week
2. how often you get ad-hoc calls or â€œcan you jump on a call now?â€ interruptions
3. how often you have to explain your work to non-technical stakeholders?
4. how often you lose half a day due to meetings / interruptions

how many hours per week are spent in meetings or calls?",1768684175.0,4,2,https://www.reddit.com/r/dataengineering/comments/1qfoxst/how_many_meetings_adhoc_calls_do_you_have_per/
1qfnogc,Azure Certs as Data Engineer - which non Data Engineer certs are useful?,"Heyo,

  
quick questions: doing plan for 2026 certs and wanted to pass AZ-203 and DP-700 Fabric DE. I'm working with Azure so it's not my first touch with this cloud platfrom but need 'papers' and something to show to my current client that I'm worth investing in and talking about raise. Long story short: client needs from consultant with papers so can charge more because 'company has experts in DE field'.

First: would you recommend doing AZ-203 while doing DP-700? 

Second: if you have any other cert and you are DE, did it help you anyhow with you tasks/projects? Been thinking about DevOps or maybe AI but would like to hear sth from people who already have other certs but work as DE. 

  
Thanks in advance and have a great weekend :) ",1768681152.0,8,2,https://www.reddit.com/r/dataengineering/comments/1qfnogc/azure_certs_as_data_engineer_which_non_data/
1qfmrd6,Clickhouse launches managed PostgreSQL,,1768678944.0,48,11,https://www.reddit.com/r/dataengineering/comments/1qfmrd6/clickhouse_launches_managed_postgresql/
1qfjxea,A guide to writing/scripting DBT models.,Can anyone suggest any comprehensive guide to writing DBT models like I have learned how to build models with DBT but thatâ€™s only on a practice level. I wish to understand and do what actually happens in a work environment.,1768672502.0,0,7,https://www.reddit.com/r/dataengineering/comments/1qfjxea/a_guide_to_writingscripting_dbt_models/
1qfj8hg,Looking for a Switch - Need Advice,"Iâ€™m 23, working as a Software Engineer with 15 months of experience in Mainframe development. Iâ€™ve realized I lack passion for this area and want to transition into Data Engineering. Working with data feels more impactful to me and I am eager to explore its opportunities.

What skills, initiatives, or actions should I focus on to update my profile and improve my chances of making the switch? Any guidance or resources would be greatly appreciated.",1768670924.0,6,9,https://www.reddit.com/r/dataengineering/comments/1qfj8hg/looking_for_a_switch_need_advice/
1qfir7d,"Why are nearly all Python based openings, based on Data Engg. only ?","As a 5+ years exp. , I have started applying for open positions. In my current company, for a client, we have worked on API creation using Flask , ETL workflow in AWS Glue using Python, and Lambda functions/other such functions using Python. All of these (except ETL) are not Data Engg. related 

But, in all job portals, like Naukri, LinkedIn, I only get the openings for Data Engineering roles.

Why is that ? I have worked in ETL workflow, but Data Engg. needs more than that like being strong in SQL. I do have experience in SQL, and Data warehouses, but only from Development standpoint. Not as a purely Data Engineer.

**How do I manage this ?**",1768669834.0,22,9,https://www.reddit.com/r/dataengineering/comments/1qfir7d/why_are_nearly_all_python_based_openings_based_on/
1qfijmf,Advice for navigating smaller companies,"Hi everyone ! I'll try to keep it short. I started my career in data at a pretty large company. 

It had a lot of the cliche pitfalls but they had leadership in place and processes and roles and responsibilities squared away to a degree 

I am almost a year into working at a smaller firm. We are missing many key leadership roles on the org chart relating to data and all basically roll up to one person where there should be about 3 layers of leadership 

We divide up our responsibilities by business verticals and a couple of us support diff ones 

I am struggling to find my place here. It seems like the ones succeeding are always proposing initiatives,  meddling in other verticals, and doing every project that comes their way at top speed

I like the exposure I am getting to high level conversations for my vertical, but I feel like there's too much going on for me to comfortably maintain some semblance of work life balance and do deep work 

How do you survive these sorts of environments and are they worth staying in to learn/grow?

I'd like to have optionality to freelance one day and I feel like this type of environment is relatively common in companies that might be hiring me down the road so I wanna stick it out ",1768669375.0,2,2,https://www.reddit.com/r/dataengineering/comments/1qfijmf/advice_for_navigating_smaller_companies/
1qfgxsh,Managers: what would make you actually read/respond to external emails?,"Iâ€™m in a role where I get a lot of stuff from outside the org - vendors, â€œquick advice?â€ emails, random Linkedin followsâ€‘up, that kinda thing. A lot of it dies in my inbox if Iâ€™m honest.

If you put aÂ *number*Â on it:

* Whatâ€™s the minimum youâ€™d need to justify spending 10-15 mins on a thoughtful reply to a stranger?
* Would you ever think of it as â€œIâ€™ll do 3-4 of these if thereâ€™s at least $X on the tableâ€ vs â€œno amount is worth the context switchingâ€?
* Does it change if itâ€™s a founder vs a random sales pitch vs a student vs a recent grad?

Genuinely curious how other managers value that incoming attention drain especially with all the AI outreach bots. I feel like Iâ€™m either being too niceâ€¦ or too grumpy.",1768665713.0,0,5,https://www.reddit.com/r/dataengineering/comments/1qfgxsh/managers_what_would_make_you_actually_readrespond/
1qfghbk,Need Guidance,"Hi , I am currently working as a Power bi developer. Now I am preparing for AWS Data Engineering. Anyone can guide me on the progress and insights. I am totally in a confused state. Really inneed of the help. 

Thanks ",1768664649.0,9,25,https://www.reddit.com/r/dataengineering/comments/1qfghbk/need_guidance/
1qfg4ey,Help with Restructuring Glue Jobs,"Hi Everyone, I got into a new company where they use one glue job for one customer ( around 300 customers that send us files daily). Orchestrator handles the file copies into s3. 

The problem now is that, there is no configuration setup for a customer, each Glue job needs to be developed/modified manually. The source data is structured and the transformations are mostly simple one like adding columns, header mapping, setting default values and so. There are 3 sets of files and 2 lookups from Databases, along the processing these are joined and finally output into another Database. Most values including the customer names in the transformations are hardcoded.

Whats the best way/pattern/architecture to restructure these Glue jobs? The transformations needed may vary Cutomer to Cutomer.",1768663811.0,1,2,https://www.reddit.com/r/dataengineering/comments/1qfg4ey/help_with_restructuring_glue_jobs/
1qfg04z,European Travel Data (Beyond Google),"Hello everybody,

I am consolidating data from all around Europe regarding travel specifically. Anything from organic wine, specialty coffee, small vinil stores etc. I am unable to normally find those things on google and such would like to create it.

if you are familier with something like this please share.v",1768663528.0,0,1,https://www.reddit.com/r/dataengineering/comments/1qfg04z/european_travel_data_beyond_google/
1qfepkd,At a crossroads as a data engineer trying to change job,"Hi everyone,

I am a data engineer with 11 years of experience looking for a change. Need your input on how to proceed further.

So before going in i would give a brief overview of things i have worked on my career. I started with traditional ETL development. Worked on Ibm datastage with unix as scripting language for almost 8 years. Post that i moved entirely to Snowflake. For storage and transformation as well just tws as scheduling tool.

My problem started when i looked at the job openings. Almost all openings have spark,pyspark and python as bare minimum with snowflake. On top of that some included azure data factory and kafka as well.

So how do i approach this? I dont see anything solely for snowflake. 

Do i have to learn spark or pyspark as bare minimum for going forward?

If yes is there any problem statement with dataset that i can design/develop to get an idea of thing.

Any help/input is appreciated 

 ",1768660412.0,8,16,https://www.reddit.com/r/dataengineering/comments/1qfepkd/at_a_crossroads_as_a_data_engineer_trying_to/
1qfannn,Internship project Suggestion,"Hello everyone!

I hope u r doing fantastic.

I have currently secured an internship opportunity. I would like to know whether there are any subjects that could be worked on well, on my level for the internship and thank you.

The duration of the internship is 4 months.

Anything that links Data Engineering and Data Analysis, why not a bit of ML. But let's not make it too complicated xD.

I am en engineering student, last year, we study for 5 years here.

Anyone got any good recommendations? and thank you.

I don't mind new tech to learn but just don't make it too complicated.",1768648481.0,5,2,https://www.reddit.com/r/dataengineering/comments/1qfannn/internship_project_suggestion/
1qfadud,"Thereâ€™s no column or even combination of columns that can be considered as a pk, what would your approach be?","Hey guys, itâ€™s my first day of work as an intern and I was tasked with finding the pk but the data seems to be not proper I tried finding the pk by using a single column all the way to 4-5 combinations of columns but all I got are 85% distinct not fully distinct which can be considered as a pk, since group of columns approach is also not working I was wondering how would yâ€™all approach this problem ",1768647538.0,37,47,https://www.reddit.com/r/dataengineering/comments/1qfadud/theres_no_column_or_even_combination_of_columns/
1qfa0ii,Amazon Data Engineer I,"Hello everyone! Did anyone in here get their first DE role? Or even first job in data/tech all? Iâ€™d love to get some advice from you! 

The attached snip is for an L4 role - however I am already an L5: so I would have to be internal transfer; and down level  well as internal transfer ",1768646225.0,91,23,https://www.reddit.com/r/dataengineering/comments/1qfa0ii/amazon_data_engineer_i/
1qf856d,System Design For Data Engineering,"Hello Everyone. What should i prepare for system design round for Data Engineering to be taken by Director of software engineering. 
I'm comfortable designing Big Data systems but do not know much on software engineering side designs. Can you please share your experiences how system design round goes for Data Engineers. ",1768639275.0,21,4,https://www.reddit.com/r/dataengineering/comments/1qf856d/system_design_for_data_engineering/
1qf7eoh,"How big of an issue is ""AI slop"" in data engineering currently?","I know many industries are having issues now with AI generated slop, but data engineering should in *theory* consist of people who are a bit more critical and at least question the AI results to some extent before implementing. How is it at your work? Do people actually vet the information given and critically assess it, or do they just plug it into whatever pipeline that exists and call it a day?

I have seen a lot of questionable DAX queries from people I assume have very little to no clue as to why they have made it like that. The complexity of the queries are often worrying as it displays a very high level of trust in the result that has been given to them. Stuff that ""works"" in the moment, but can easily break in the future.

What are your experiences? Have you seen anything in production that made you go ""oh, this is BAD!""?",1768636605.0,38,43,https://www.reddit.com/r/dataengineering/comments/1qf7eoh/how_big_of_an_issue_is_ai_slop_in_data/
1qf6xgx,I have a problem statement and I'm thinking of a design. I would like to hear other's opinions as well.,"Hi everyone, Iâ€™m stuck on a data modeling / pipeline design problem and would really appreciate guidance.

Current architecture

We have a layered warehouse with SCD-style pipelines:

Raw / Landing layer

Data arrives from multiple sources at different frequencies. All rows have some As of date value.

We snapshot it and store delta with valid\_from / valid\_to. If there is no change in the columns we are checking, the older asofdate row stays valid. 

Transformation layer

We transform raw data and store snapshots (still SCD-style).

Presentation layer

We do as-of-date reporting.

Each presentation view maps to a specific snapshot via metadata tables. 

For any requested as-of date, we pick the correct chunk using a framework we have created. What it does is it simply provides us with a timestamp that we can get records from the snapshot that were valid at that particular time. 

So far, all transformations in lower layers always run on the latest available data, and only at the final presentation layer do we resolve which snapshot chunk to use. This has worked well until now.

New problem

Now we have reporting use cases where even on the lower level, calculation won't happen on the latest data but the business will specify the dates. 

Example:

For a report as of Aug 2025:

Dataset A should use its Dec 2025 snapshot (business rule).

Dataset B should use its Aug 2025 snapshot.

I was just thinking that every time a snapshot runs, I will store the timestamp and the asofdate this snapshot date corresponds to in a metadata table and in this way, we will have a way to get a timestamp. And I will parameterise the date picking in each logic sitting in the transformation layer instead of just using valid\_to equals NULL. 

Is there anything else that I should think about ? Is there a better way to approach it ? And I will also love to have any book recommendations related to DE System Design. ",1768634928.0,1,5,https://www.reddit.com/r/dataengineering/comments/1qf6xgx/i_have_a_problem_statement_and_im_thinking_of_a/
1qf5k36,Need guidance for small company big data project,"Recently found out (as a SWE) that our ML team of 3 members uses email and file sharing to transfer 70GB+ each month for training purposes.   
The source systems for these are either files in the shared drive, our SQL server or just drive links

Not really have any data exp. Was wondering if a simple python script running on a server cron job could do the trick to keep all data in sql? been tasked with centralizing it. 

Our company is ML dependent and data quality >> data freshness.

Any suggestions?  
Thanks in advance

",1768630303.0,8,17,https://www.reddit.com/r/dataengineering/comments/1qf5k36/need_guidance_for_small_company_big_data_project/
1qf5488,Data Management project for my work,"Hi Everyone,

I'm a male nurse who loves tech and AI. I'm currently trying to create a knowledge database for my work (the ""lung league"", or ""la ligue pulmonaire"" in Switzerland.) The first goal is to extract the text from a lot of documents (.docx, .pdf, .txt, .xlsx, pptx) and put it into .md files. Finally, I need to chunk all the .md files correctly so that they can be used with our future chatbot.

I've created a Python script with Claude to chunk several files into a doc; it works on my local LLM and LanceDB but... I don't know if what I'm doing is correct or if it respects standard layouts (is my YAML correct, things like that). I want my data base to can be ""futur proof"" and completely standard for later use.

I'm not sure if my question is appropriate here, but I would be grateful for any tips to help with this kind of data management. Itâ€™s more about knowing where to start than having a complete solution at the moment.

Thanks ! :)

  
EDIT : Â For the moment, it's only for theoretical knowledge; there is no mention of our client info. Everything is done locally on my computer currently. My goal is to better understand data management and to better orientate our future decisions with our IT partner.Â I will never use vibecoded things on critical data or for production.",1768628922.0,3,4,https://www.reddit.com/r/dataengineering/comments/1qf5488/data_management_project_for_my_work/
1qf0y19,Lack of Network Connectivity in Fabric!,"I have built data engineering solutions (with spark) in HDInsight, Azure Synapse, Databricks, and Fabric.

Sometimes building a solution will go smoothly; and other times I cannot even connect to my remote resources.  In Fabric the connectivity can be very frustrating.  They have a home-grown networking technology that lets spark notebooks connect to Azure resources.  The interface is called ""Managed Private Endpoints"" (MPE).  It is quite different than connecting via normal service endpoints (within a VNET).  This home-grown technology used to be very unreliable and buggy; but about a year ago it finally became about as reliable as normal TCP/IP (albeit there is still a non-zero SLA for this technology, that you can find in their docs.)

The main complaint I have with MPE's is that Microsoft is required to make them available on a ""onesie-twosie"" basis for each and every distinct azure resource that you want to connect to!  The virtualized networking software seems like it must be written in resource-dependent way.

Microsoft had asked Synapse customers to move to Fabric a couple years ago, before introducing many of the critical MPE's.  The missing MPE's have been a show-stopper, since we had previously relied on them in Synapse.  About a month ago they FINALLY introduce a way to use an MPE to connect our spark workloads to our private REST APIs (HTTP with FQDN host names).  That is a step forward, although the timing leaves a lot to be desired.

There are other MPE's that are still not available.  Is anyone aware why network connectivity doesn't get prioritized at Microsoft?  It seems like such a critical requirement for data engineers to connect to our data!!  If I had to make guess, these delays are probably for non-technical reasons.  In this SaaS platform Microsoft is accustomed to making a large profit on their so-called ""gateways"" that move data to ADF and Dataflows (putting it into Fabric storage).  Those data-movement activities will burn thru a ton of our CU credits ... whereas making a direct connection to MPE resources is going to have a much lower cost to customers.  As always, it is frustrating to use a SaaS where the vendor puts their own interests far above those of the customer.   
  
Is there another explanation for the lack of MPE network connectivity into our azure tenant? ",1768617496.0,3,7,https://www.reddit.com/r/dataengineering/comments/1qf0y19/lack_of_network_connectivity_in_fabric/
1qf0iqk,"How can you cheaply write OpenLineage events to S3, emitted by Glue 5 Spark DataFrame?","Hello,

What would be the most cost effective way to process OpenLineage events from Spark into S3, as well as custom events I produce via Pythonâ€˜s OpenLineage client package?

I considered managed Flink or Kafka, but these seem like overkill. I want the events emitted from Glue ETL jobs during regular pollung operations. We only have about 500 jobs running a day, and so Iâ€™m not sure large, expensive tooling is justified.

I also considered using lambda to write these events to S3. This seems like overkill too, because itâ€™s a whole lambda boot and process per event. Not sure if this is unsafe for some reason as well, or if it risks corruption due to (e.g.,) non-serialized event processing?

What have you done in the past? Should I just bite the bullet and introduce Flink to the ecosystem? Should I just accept Lambda as a solution? Is there something Iâ€™m missing, instead?

Ive considered Marquez as well, but I donâ€™t want to host the service just yet. Right now, I want to start preserving events so that I have the history available for once Iâ€™m ready to consume them.",1768616470.0,3,2,https://www.reddit.com/r/dataengineering/comments/1qf0iqk/how_can_you_cheaply_write_openlineage_events_to/
1qeyych,Not a single dbt adapter has worked with our s3 tables. Any suggestions?,"Sup guys, I am working on implementing dbt at our company. Our Iceberg tables are configured as s3 tables, however, I havenâ€™t been able to make most adapters work because of the following:

\- dbt-glue: Loading all dependencies (dbt core and dbt glue) takes around 50s

\- dbt-Athena: their api call doesnâ€™t go well with s3 tables

are there any other options? Should I just abandon dbt?

Thanks!",1768612750.0,2,16,https://www.reddit.com/r/dataengineering/comments/1qeyych/not_a_single_dbt_adapter_has_worked_with_our_s3/
1qexmyu,"""semantic join"" problems","I know this subreddit kinda hates LLM solutions, but I think there is an undeniable and underappreciated fact about this. If you search on various forums like SO, reddit, community forums of various data platforms etc. for terms like (would link them, but can't here):

1. fuzzy matching
2. string distance
3. CRM contact matching
4. software list matching
5. cross-referencing \[spreadsheets\]
6. ...

and so on, you find hundreds or thousands of posts dealing with seemingly easy issues where you have the classic example of not having your join keys exactly matching, and having to do some preprocessing or softening of the keys on which to match. This problem is usually trivial for humans, but very hard to generically solve. Solutions range from stuff like fuzzy string matching, levenshtein distance, word2vec/embedding to custom ML approaches. I personally have spent hundreds of hours over the course of my career putting together horrendous regexes (with various degrees of success). I do think there is still use for these techniques in some relatively specific cases, such as when we are talking about big data and stuff, but for all those CRMs systems that need to match customers to companies that are under 100k rows of rows and so on, it's IMHO solved for negligible cost (like dollars compared to hundreds or thousands of hours of human labour). 

There are different shades of ""matching"" - I think most of the readers imagine something like a pure ""join"" with matching keys, a pretty rare case in the world of messy spreadsheets or outside of RDBMs. Then there are some trivial cases of transformation like capitalization of strings where you can pretty easily get to a canonical form and match on that. Then there are those cases that you still can get quite far with some kind of ""statistic"" distance. And finally there are scenarios where you need some kind of ""semantic distance"". The latter, IMHO the hardest, is something like matching list of S&P500 companies, where you can't really get the results correct unless you do some kind of (web)search. Example is e.g. a ticker change for Facebook in 2022 from FB to META. I believe today LLMs opened the door to solving all of those.

For example, a classic issue companies have is matching all the used software by anyone in the company to licenses or whitelisted providers. This can be now done by something like this python-pseudocode:

    software = pd.read_csv(""software.csv"", columns=[""name""])
    suppliers = set(pd.read_csv(""suppliers.csv"", columns=[""company""]))
    
    def find_sw_supplier(software_name: str, suppliers: set[str]) -> str | None:
        return call_llm_agent(
            f""Find the supplier of {software_name}, try to match it to the name of a company from the following list: {suppliers}. If you can't find a match, return None."",
            tools=[WebSearch],
        )
    
    for software_name in software[""name""]:
        supplier = find_sw_supplier(software_name, suppliers)
        df.loc[idx, ""supplier""] = supplier

It is a bit tricky to run at scale, and can get pricey, but depending on a task it can be drawn down quite significantly depending on the usecase. For example, for our usecases we were able to trim down the cost and latency in our pipelines by doing some routing (like only sending to LLMs what isn't solved by local approaches like regexes) and by batching LLMs calls together and ultimately fit it into something like (disclosure: this is our implementation):

    from everyrow.ops import merge
    
    result = await merge(
        task=""Match trial sponsors with parent companies"",
        left_table=trial_data,
        right_table=pharma_companies,
        merge_on_left=""sponsor"",
        merge_on_right=""company"",
    )

and given these cases are basically embarrassingly parallel (in the stupidest way, you throw every row on all the options), the latency mostly boils down to the available throughput and longest-llm-agent-with-search, in our case we are running virtually arbitrary (publicly web-searchable) problems under 5 minutes and 2-5$/1k of rows to merge (trivial cases are of course for 0, most of the cost is eaten by LLMs generations and web search through things like serper and stuff). 

This is of course one of the few classes of problems that are possible now and weren't before. I don't know, but I find it fascinating - in my 10-year career, I haven't experienced such a shift. And unless I am blind, it seems like this still hasn't been picked up by some of the industries (judging based on the questions from the various sales forums and stuff). Are people just building this in-house and it's just not visible, or am I overestimating how common this pain point is?",1768609762.0,19,10,https://www.reddit.com/r/dataengineering/comments/1qexmyu/semantic_join_problems/
1qetgdg,Which system would you trust to run a business you canâ€™t afford to lose?,"A) A system that summarizes operational signals into health scores, flags issues, and recommends actions



B) A system that preserves raw operational reality over time and requires humans to explicitly recognize state



Why?",1768600485.0,0,3,https://www.reddit.com/r/dataengineering/comments/1qetgdg/which_system_would_you_trust_to_run_a_business/
1qes5tx,Anyone else losing their touch?,"Iâ€™ve been working at my company for 3+ years and canâ€™t really remember the last time I didnâ€™t use AI to power through my work. 

If I were to go elsewhere, I have no idea if I could answer some SQL and Python questions to even break into another company. 

It doesnâ€™t even feel worth practicing regularly since AI can help me do everything I need regarding code changes and I understand how all the systems tie together. 

Do companies still ask raw problems without letting you use AI? 

I guess after writing this post out, I can already tell itâ€™s just going to take raw willpower and discipline to keep myself sharp. But Iâ€™d like to hear how everyone is battling this feeling. ",1768597462.0,261,135,https://www.reddit.com/r/dataengineering/comments/1qes5tx/anyone_else_losing_their_touch/
1qericu,Data science student looking to enhance his engineering skills,"Hello everyone, Iâ€™m currently a masterâ€™s student in Data Science at a French engineering school. Before this, I completed a degree in Actuarial Science. Thanks to that background, my skills in statistics, probability, and linear algebra transfer very well, and Iâ€™m comfortable with the theoretical aspects of machine learning, deep learning, time series and so on.

However, through discussions on Reddit and LinkedIn about the job market (both in France and internationally), I keep hearing the same feedback. That is engineering skills and computer science skills is what make the difference. It makes sense for companies as they are first looking for money and not taking time into solving the problem by reading scientific papers and working out the maths.

At school, Iâ€™ve had courses on Spark, Hadoop, some cloud basics, and Dask. I can code in Python without major issues, and Iâ€™m comfortable completing notebooks for academic projects. I can also push projects to GitHub. But beyond that, I feel quite lost when it comes to:

\- Good engineering practices

\- Creating efficient data pipelines

\- Industrialization of a solution

\- Understanding tools used by developers (Docker, CI/CD, deployment, etc.)

I realize that companies increasingly look for data scientists or ML engineers who can deliver end-to-end solutions, not just models. Thatâ€™s exactly the type of profile Iâ€™d like to grow into. Iâ€™ve recently secured a 6-month internship on a strong topic, and I want to use this time not only to perform well at work, but also to systematically fill these engineering gaps.

The problem is I donâ€™t know where to start, which resources to trust, or how to structure my learning. What Iâ€™m looking for:

\- A clear roadmap in order to master essentials for my career

\- An estimation of the needed work time in parallel of the internship

\- Suggestion of resources (books, papers, videos) for a structured learning path

If youâ€™ve been in a similar situation, or if youâ€™re working as a ML Engineer / Data Engineer, Iâ€™d really appreciate your advice about what really matters to know in these fields and how to learn them.",1768595964.0,2,2,https://www.reddit.com/r/dataengineering/comments/1qericu/data_science_student_looking_to_enhance_his/
1qeqfk7,What data should a semantic layer track?,"We often see things like schema, DDL, metric name, created/updated dates, etc. tracked in different Semantic Layer solutions.

What else do you think should be tracked by a Semantic Layer, and how should that semantic layer be packaging that data for an Agentic AI tool.",1768593498.0,1,5,https://www.reddit.com/r/dataengineering/comments/1qeqfk7/what_data_should_a_semantic_layer_track/
1qeq2d3,"Ideas needed for handling logging in a ""realtime"" spark pipeline","Hey everyone! Looking for some ideas/resources on how to handle a system like this. I'm fairly new to realtime pipelines so any help is appreciated.

The existing infrastructure: We have a workflow that consists of some spark streaming jobs and some batch processing jobs that run once every few hours. The existing logging approach is to write the logs from all of these jobs to a continuous text file (one log file for each job, for each day) and a different batch job also inserts the logs into a MySQL table for ease of querying and auditing. Debugging is done through either reading the log files on the server, or the YARN logs for any failed instances, or the MySQL table. 

This approach has a few problems, mainly that the debugging is kinda tedious and the logs are very fragmented. I'm wondering if there's a better way to design this. All I need is a few high level ideas or resources where I can learn more. Or if you've worked on a system like this, how does your company handle the logging?

Thanks all the help!",1768592655.0,2,2,https://www.reddit.com/r/dataengineering/comments/1qeq2d3/ideas_needed_for_handling_logging_in_a_realtime/
1qeoqfo,First time leading a large data project. Any advice?,"Hi everyone,

Iâ€™m a Data Engineer currently working in the banking sector from Brazil ğŸ‡§ğŸ‡· and Iâ€™m about to lead my first end-to-end data integration project inside a regulated enterprise environment.

The project involves building everything from scratch on AWS, enriching data stored in S3, and distributing it to multiple downstream platforms (Snowflake, GCP, and SQL Server). Iâ€™ll be the main engineer responsible for the architecture, implementation, and technical decisions, working closely with security, governance, and infrastructure teams.

Iâ€™ve been working as a data engineer for some time now, but this is the first time Iâ€™ll be building an entire banking infrastructure with my name on it. Iâ€™m not looking for â€œperfectâ€ solutions, but rather practical lessons learned from real-world experience.

Thanks in advance, community!",1768589687.0,23,14,https://www.reddit.com/r/dataengineering/comments/1qeoqfo/first_time_leading_a_large_data_project_any_advice/
1qeo2nu,Red flags for contract extension,"My internship is ending soon, and there is an opportunity to extend as a contractor. From discussion, my manager said he would try to get me closer to market rate, and mentioned a possible 2nd extension with the same period once this extension ends.

News came a while ago that HR pushed back on the expected salary. They only counted my experience in this field (just the internship) and wanted to pay junior market rate. This eventually got resolved, which I suspected to be because:

1. They already tried hiring externally, could not find anyone suitable, and wanted someone to fill in the gaps.
2. The budget has always been there. My manager's willingness to raise the expected salary suggested they had more budget than HR initially wanted to use.

I accepted it. Pay bump is decent and the work seems challenging & interesting enough to me. The ideal scenario is that I do this for a year, and gain enough experience to either convert or find another place.  
  
Any blind spots that I missed, or concerns/issues with the contract that you think I need to be aware of? General advice probably works best, as I am not US-based.",1768588253.0,1,2,https://www.reddit.com/r/dataengineering/comments/1qeo2nu/red_flags_for_contract_extension/
1qemxue,Germany DE market for someone with around 1 YOE?,"Hey all,  
I have about 1 year of experience as a Data Engineer (Python/SQL, AWS Glue/Lambda/S3, Databricks/Spark, Postgres). Planning a Masterâ€™s in Germany (Winter 2026).

Howâ€™s the DE job market there for juniors? And besides German, what skills should I focus on to actually land a role (Werkstudent/internship/junior)? Also, which cities would you recommend for universities if I want better job opportunities during/after my Masterâ€™s?

Also wondering if my certs help at all:

AWS Certified Data Engineer (Associate), Databricks DE (Associate)

Thanks!",1768585841.0,12,3,https://www.reddit.com/r/dataengineering/comments/1qemxue/germany_de_market_for_someone_with_around_1_yoe/
1qem905,API pulls to Power BI for Shopify / Amazon,"Hey guys, I am a data analyst at a mid-sized CPG company and wear a few hats, but I do not have much engineering or ETL experience. I currently pull reports into Excel weekly to update a few Power BI dashboards that I built. I know the basics of Python, R, and SQL, but mainly do all of my analysis in Excel.

In short, my boss would like to see a combined Power BI dashboard of our Amazon and Shopify data that updates weekly. I am researching which software would be best for automatic API pulls from Seller Central and Shopify with low code and minimal manual work. So far, I am leaning toward Airbyte because of the free trial and low cost, but I am also looking into Windsor.ai, Adzviser, and Portable.

We do not have much of a budget, so I was hoping to get some input on which service might be best for someone with limited coding skills. Any other suggestions or advice would be greatly appreciated! Thank you!

P.S. I love lurking in this sub. You guys are awesome.",1768584355.0,9,10,https://www.reddit.com/r/dataengineering/comments/1qem905/api_pulls_to_power_bi_for_shopify_amazon/
1qelsee,How to Keep Business Users Autonomous,"I'm a data engineer in a local government organization and we're clearly stuck in a strategic impasse with our data architecture.

We're building a classic data architecture: DataLake, DataWarehouse, ETL, DataViz. On-premise only due to sovereignty requirements and no Google/Microsoft. That's fine so far. The problem is we're removing old tools like Power BI and Business Objects that allowed business teams to transform their data autonomously and in a decentralized way.

Now everything goes through the warehouse, which is good in theory. But concretely, our data team manages the ETL for generic data, the business teams will have access to the warehouse plus a dataviz tool, and that's it. There's no tool to transform business-specific data outside of Python. And that's the real problem: 90% of business analysts will never learn Python. We just killed their autonomy without replacing it with anything.

I'm looking for an open-source, on-prem or self-hosted tool that would allow non-expert business users to continue transforming their data ergonomically. The business teams are starting to panic and honestly I'm pretty lost too.

Do you have any recommendations?",1768583377.0,2,7,https://www.reddit.com/r/dataengineering/comments/1qelsee/how_to_keep_business_users_autonomous/
1qelnbt,"Passed a DP-700, let me share my experience","Today I passed the DP-700: Implement data engineering solutions using Microsoft Fabric exam certification.

It was challenging, more complex than the DP-203 Data engineering on Azure, but still doable.

For preparation, I completed the full Microsoft learning course on the topic, but skipped most of the practice exercises.

I only explored a few to get a sense of them.

I also didnâ€™t use the Microsoft Fabric trial offer, but I did complete one of the Applied Skills exercises, where you get hands-on practice creating databases and tables directly within the Fabric interface.

That helped a lot for understanding the environment.



My main training point was the ""Practice for the exam"" section at the course page, which gives you 50 questions per attempt.

Some questions repeated, I suggest there are about 200 in the pool. These questions are easier than the actual exam ones, but they gave me the spirit.



The actual exam structure differs noticeably from whatâ€™s described on the official page. There are 51 questions instead of 50.

41 questions are in the first section, you can review them in random order or in a batch but before you go to the next part.

And 10 more are in a case study, which is reviewed in whole separately.



What I must say: do not be afraid of KQL. I knew almost nothing of it, but basic sense and logic were quite enough.

They don't ask you very complex questions on KQL.



I faced no occurences of Synapse, but Eventhouses and Eventstreams were frequent.

Familiarize yourself with the hierarchy of Fabric levels and what belongs to each.

Domains and subdomains didnâ€™t appear in the questions either, but organizing them mentally was worth it.

Use AI during preparation to structure your understanding of Fabric components: workspaces, eventhouses, pipelines, dataflows, databases and spark pools.



I have seen numerous pieces of advice on Aleksi Partanen Certiace, Fabricforge and similar resources, and I even looked into their videos, but did not use that much.

Yes, I know they say that the official Learn is not sufficient, but my case proves otherwise.



Use Microsoft Learn, this is allowed throughout the exam!

Moreover, for some questions it is essential to use the manuals.

There is zero value in memorizing the \`sys\_dm\_requests\_anything\` names, contents and uses.

During real work, you will definitely lookup the manpage for it. So the same applies to an exam as well.



Even better, MS Learn has an AI assistant builtin. And you actually can type exactly the question you see at the screen.

Again, this resembles the real work process so this is not just allowed, but asking AI is an important part of your expertise.

Because after that, you must extract meaningful parts from an AI response and use it accordingly.



There were a few what Iâ€™d call ""questionable"" items: overly wordy definitions leading to self-evident choices, but fewer than in the practice quizzes.



Some parts I still donâ€™t fully grasp, such as all features for Dataflow Gen2 versus Spark in complex scenarios.

Still, this is an intermediate-level exam, so I think that's just enough knowledge for now.",1768583073.0,2,1,https://www.reddit.com/r/dataengineering/comments/1qelnbt/passed_a_dp700_let_me_share_my_experience/
1qel3sz,Seeking advice,"Hello everyone, Iâ€™m a 2025 graduate in Big Data Analytics and currently looking for my first job. Itâ€™s been about 5.5 months since my internship ended, and during this time Iâ€™ve been doing a lot of reflection on my academic journey. The program was supposed to prepare us for roles like Data Analyst, Data Engineer, or Data Scientist, but honestly, I have mixed feelings about how effective it really was.

Over three years, we covered a huge range of topics: statistics, machine learning, big data, databases, networking, cybersecurity, embedded systems, image processing, mobile development, Java EE/ spring bot, SaaS development, ETL, data warehousing, Kafka, spark, and more. On paper, it sounds great. In practice, it often felt scattered and a bit inefficient.

We kept jumping between multiple languages (C, java, python, javascript) without enough time to truly master any of them. Many technical modules stayed very theoretical, with little connection to real-world use cases: real datasets, real production pipelines, proper data engineering workflows, or even how to debug a broken pipeline beyond adding print statements. Some courses were rushed, some poorly structured, and others lacked continuity or practical guidance.



I know university is meant to build foundations, not necessarily teach the latest trendy tools. Still, I feel the curriculum could have been more focused and better aligned with what data roles actually require today, such as:

* strong SQL and solid database design
* Strong python for data processing and pipelines 
* Real ETL and data modeling projects 
* Distributed systems with clear, practical applications
* A clear separation between web development tracks and data tracks
* Better guidance on choosing ML algorithms depending on the use case

Instead, everything was mixed together: web dev, mobile dev, low-level systems, data science, big data, and business, without a clear specialization path.

Now Iâ€™m trying to fill the gaps by self-studying and building real projects, mainly with a data engineering focus. For context, here are the main projects I worked on during my internships:

1. Machine test results dashboard  

* A web application to visualize machine test results.  
* Stack: Django REST Framework, MongoDB, React.

It was a 2-person project over 2 months. I was responsible for defining what should be displayed (failure rate, failure rate by machine/section, etc.) and implementing the calculation logic while making sure the results were accurate. I also helped with the frontend even though it was my first time using JavaScript. A lot of it was assisted by chatgpt and claude, then reviewed and corrected with my teammate.

2. Unix server resource monitoring system   

A server monitoring platform providing:

* Real-time monitoring of CPU, memory, disk, and network via websockets  
* Historical analysis with time-series visualization
* ML-based anomaly detection using Isolation Forest
* Server management (CRUD, grouping, health tracking)
* Scalable architecture with Kafka streaming and redis caching

Stack: Django REST Framework, PostgreSQL, redis, Kafka, Angular 15, all containerized with Docker.

I admit the stack is more â€œweb-heavyâ€ than â€œpure data engineering,â€ but it was chosen to match the companyâ€™s ecosystem and increase hiring chances (especially Angular, since most of their tech team were web developers). Unfortunately, it didnâ€™t lead to a position.

Now Iâ€™d really need advice from people already working in data engineering:

* What core skills should I prioritize first? 
* How deep should I go into SQL, Python, and system design?
* What kinds of projects best show readiness for a junior data engineer role(and where can i get the data like the millions of rows of data aside from web scraping)?
* How do you personally bridge the gap between academic knowledge and industry expectations?
*  What are your thoughts on certifications like those on Coursera?
* And for the love of god â€¦ how do you convince HR that even if youâ€™ve never used their exact stack, you have the fundamentals and can become operational quickly?

Any advice, feedback, or shared experience would be greatly appreciated.

\---

\*\*TL;DR\*\*  

My data program covered a lot but felt too scattered and too theoretical to fully prepare for real data engineering roles. Iâ€™m now self-learning, building projects, and looking for guidance on what skills to prioritize and how to position myself as a solid junior data engineer.",1768581923.0,5,4,https://www.reddit.com/r/dataengineering/comments/1qel3sz/seeking_advice/
1qekr0y,Fivetran experience,"Hi all, 

Iâ€™m entering a job which uses Fivetran. Generally Iâ€™ve rolled my own custom Pyspark jobs for ingestion or used custom ingestion via Apache Hudi/ Iceburg. Generally I do everything with Python if possible. 

Stack:  

cloud- AWS 

Infra - kubernetes/ terraform / datadog   

Streaming- Kafka 

Db - snowflake 

Orchestration - airflow 

Dq - saas product 

Analytics layer - DBT. 

Note: Iâ€™ve used all these tools and feel comfortable except Fivetran.  

Do you have any tips for using this tooling? While I have a lot of experience with custom programming Iâ€™m also a bit excited to focus on some other areas and let fivetran do some of the messy work.  

While I would be worried about losing some of my programming edge, this opportunity has a lot of areas for growth for me so I am viewing this opportunity with growth potential. Saying that I am happy to learn about downsides as well. ",1768581167.0,5,12,https://www.reddit.com/r/dataengineering/comments/1qekr0y/fivetran_experience/
1qekncd,"I created DAIS: A 'Data/AI Shell' that gives standard ls extra capabilities, instant for huge datasets","Want instant data of your huge folder structures, or need to know how many **millions of rows** does your data files have with just your standard 'ls' command, **in blink of an eye**, without lag, or just want to customize your terminal colors and ls output? I certainly did, so I created something to help scout out those unknown codebases. Here:

[mitro54/DAIS: < DATA / AI SHELL >](https://github.com/mitro54/DAIS)

Hi,

I created this open-source project/platform, Data/AI shell, or DAIS in short, to add capabilities to your favourite shell. Currently as MVP, it has the possibility to run python scripts as extensions to the core logic, however this is not fully implemented yet. At its core, it is a PTY Shell wrapper written in C++

Current ""big"" and only real feature is the ability to add some extra info to your standard ""ls"" command, the ""ls"" formatting, and your terminal colors are fully customizable. It is able to scan and output thousands of folders information in an instant. It is capable of scanning and estimating how many rows there are in you text files, without causing any delays, for example estimating and outputting info about .csv file with 21.5 million rows happens as fast as your standard 'ls' output would.

This is just the beginning, I will keep on updating and building this project along my B. Eng studies to become a Data/AI Engineer, as I notice more pain points. If you want to help, please do! Any suggestions and opinions of it are welcome.",1768580952.0,2,2,https://www.reddit.com/r/dataengineering/comments/1qekncd/i_created_dais_a_dataai_shell_that_gives_standard/
1qejnn6,Messy Data Problems: How to get Stakeholders on Board,"Hello! This is my first post in this sub. Iâ€™ve seen a lot of strong practical advice here and would like to get multiple perspectives on how to approach a messy data cleanup and modeling problem.

Until recently, I worked mostly at startups, so I never dealt with serious legacy data issues. I just joined a midsized private company as an â€œAnalyst.â€ During the hiring process, after hearing about their challenges, I told them it sounded like they really needed a data engineer or more specifically an analytics engineer. They said nope we just need an analyst, which i thought was odd. FYI: They already have an ERP system, but the data is fragmented, difficult to retrieve, and widely acknowledged across the company as dirty and hard to work with.

Once I joined, I got access to the tools I needed fairly quickly by befriending IT. However, once I started digging into the ERP backend, I found some fundamental problems. For example, there are duplicated primary keys in header tables. While this can be handled downstream, it highlights that even basic principles like first normal form were not followed. I understand ERPs are often denormalized, but this still feels extreme.

Some historical context that likely contributed to this:

* In the past, someone was directly injecting data via SQL
* The company later migrated to a cloud ERP
* Business processes have changed multiple times since then

As a result, naming conventions, supplier numbers, product numbers, and similar identifiers have all changed over time, often for the same logical entity. Sales data is especially messy. Some calculated fields do not align with what is defined in the ERPâ€™s data dictionary, and overall there is very little shared understanding or trust in the data across the company.

Constraints I am working under:

* I have read-only access to the ERP and cannot write data back, which is appropriate since it is the raw source
* of-course the ERP is not a read-optimized database, so querying it directly is painful
* There are over 20,000 tables in total, but after filtering out audit, temp, deprecated, and empty tables, I am down to roughly 500 tables
* Total row count across those tables is likely 40 to 50 million rows, though normalization makes that hard to reason about
* I am the first and only data-focused hire

The business context also matters. There are no real long-term data goals right now. Most work is short-term:

* One-week automations of existing manual processes
* One to two month dashboard and reporting projects

Stakeholders primarily want reports, dashboards, and automated spreadsheets. There is very little demand for deeper analysis, which makes sense given how unreliable the underlying data currently is. Most teams rely heavily on Excel and tribal knowledge, and there is effectively zero SQL experience among stakeholders.

My initial instinct was to stand up a SQL Server or PostgreSQL instance and start building cleaned, documented models or data marts by domain. However, I am not convinced that:

1. I will get buy-in for that approach
2. It is the right choice given maintainability and the short-term nature of most deliverables

As a fallback, I may end up pulling subsets of tables directly into Power BI and doing partial cleaning and reshaping using Power Query transformations just to get something usable in front of stakeholders.

So my core question is:  
**How would you approach cleaning, organizing, documenting, and storing this kind of historically inconsistent ERP data while still delivering short-term reports and dashboards that stakeholders are expecting?**

If I am misunderstanding anything about ERPs, analytics engineering, or data modeling in this context, I would appreciate being corrected.",1768578820.0,6,7,https://www.reddit.com/r/dataengineering/comments/1qejnn6/messy_data_problems_how_to_get_stakeholders_on/
1qei0lu,Feel like I'm falling behind. Now what?,"I've worked in databases for around 25 years, never attended any formal training. Started in data management building reports and data extracts, built up to SSIS ETL. Current job moved most work to cloud so learnt GCP BigQuery and Python for Airflow. Don't think of myself as top drawer developer but like to think I build clean efficient ETL's.

Problem I find now is that looking at the job market my experience is way behind. No Azure, no AWS, no Snowflake, no Databricks.. 

Current job is killing my drive, not got the experience to move. Any advice that doesn't involve a pricey course to upskill?",1768575123.0,72,32,https://www.reddit.com/r/dataengineering/comments/1qei0lu/feel_like_im_falling_behind_now_what/
1qehorv,Best way to learn fundamentals,"I'm currently trying to pivot from a BI analyst role to DE. What's the best way to learn the core principles and methodologies of DE during the transition?

I want to make it clear that I am NOT looking to learn tools end to end and work on certs but rather focus on the principles during each phase from ingestion to deployment.

Any books/YouTube/course recommendations?",1768574360.0,6,14,https://www.reddit.com/r/dataengineering/comments/1qehorv/best_way_to_learn_fundamentals/
1qecu5b,How to trace expensive operations in Spark UI back to specific parts of my PySpark code?,"Hey everyone,

I have a PySpark script with a ton of joins and aggregations. I've got the Spark UI up and running, and I've been looking at the event timeline, jobs, stages, and DAG visualization. I can spot the slow tasks by their task ID and executor ID.

The issue is the heavy shuffle read/write from all those joins is killing performance, but how do I figure out exactly which join (or aggregation) is the biggest culprit?

Is there a good way to link those expensive stages/tasks in the UI directly back to lines or sections in my PySpark code?

I've heard about caching intermediate DataFrames or forcing actions (like count() or write()) at different points to split the job into smaller observable parts in the UIâ€¦ has anyone done that effectively? 

",1768560459.0,5,5,https://www.reddit.com/r/dataengineering/comments/1qecu5b/how_to_trace_expensive_operations_in_spark_ui/
1qebwl9,"AI reasoning over Power BI models in workflow automation, would this help?","Curious about how teams handle automated insights from BI models: imagine a workflow (e.g., in n8n) that can query your Power BI model with AI reasoning. You could automatically:
	1.	Enrich leads with missing or inferred data.
	2.	Estimate ARR or deal potential from similar historical deals.
	3.	Identify geographic regions performing above or below expectations.

Would this type of automation fit into your pipelines or workflow automation?",1768557130.0,0,4,https://www.reddit.com/r/dataengineering/comments/1qebwl9/ai_reasoning_over_power_bi_models_in_workflow/
1qebb1m,AI on top of a 'broken' data stack is useless,"This is what I've noticed recently:

The more fragmented your data stack is, the higher the chance of breakage.

And now if you slap AI on top of it, it makes it worse.

I've come across many broken data systems where the team wanted to add AI on top of it thinking it will fix everything, and help them with decision making. But it didn't, it just exposed the flaws of their whole data stack.

I feel that many are jumping on the AI train without even thinking about if their data stack is 'able', otherwise it's pretty much pointless.

Fragmentation often fails because semantics are duplicated and unenforced.

This leaves me thinking that the only way to fix this is to find a way to fully unify everything(to avoid fragmentation) and avoid semantic duplication with platforms like Definite or any other all-in-one data platforms that will pretty much replace all you data stack.",1768554877.0,65,27,https://www.reddit.com/r/dataengineering/comments/1qebb1m/ai_on_top_of_a_broken_data_stack_is_useless/
1qeaukw,"Designed a data ingestion pipeline for my quant model, which automatically fetches Daily OHLCV bars, Macro (VIX) data, and fundamentals Data upto last 30 years for free. Should I opensource the code? Will that be any help to the community?","So I was working on myÂ Quant Beast Model, which I have presented to the community before and received much backlash.

I was auditing the model, I realized that the data ingestion engine I have designed is pretty robust. It is a multi-layered, robust system designed to provide high-fidelity financial data while strictly avoiding look-ahead bias and minimizing API overhead.

And it's free on top of that using intelligently polygon, Yfinance, and SEC EDGAR to fill the required Daily market data, macro data and fundamentals data for all tickers required.

[](https://preview.redd.it/designed-a-data-ingestion-pipeline-for-my-quant-model-which-v0-xgmm2px68odg1.png?width=958&format=png&auto=webp&s=bb6bac6075103e59fa1271ff37a584afb6d7cdd4)

[Data Ingestion Piepleine](https://preview.redd.it/z88w2tf9codg1.png?width=958&format=png&auto=webp&s=ea13e57526bf5dbb856e2f8d3f43918008a976eb)

Should I opensource it? Will that help the trading community? Or is everybody else have better ways to acquire data for their system?",1768553136.0,5,3,https://www.reddit.com/r/dataengineering/comments/1qeaukw/designed_a_data_ingestion_pipeline_for_my_quant/
1qea1l9,Medallion Architecture Explained in 4 Mins,Medallion Architecture: What It Means To The Business [https://medium.com/@sindu0090/medallion-architecture-what-it-means-to-the-business-d3035c73723c](https://medium.com/@sindu0090/medallion-architecture-what-it-means-to-the-business-d3035c73723c),1768550143.0,0,0,https://www.reddit.com/r/dataengineering/comments/1qea1l9/medallion_architecture_explained_in_4_mins/
1qe4fvt,Healthcare data insights?,"Hello all!

I have been looking to understand the healthcare data for data engineers. Anyone here please help me with giving overview on health information exchange forums, about HEDIS measures, cpt/loinc codes and everything around healthcare data. Any small insight from you will be helpful.

Thanks!",1768532568.0,1,1,https://www.reddit.com/r/dataengineering/comments/1qe4fvt/healthcare_data_insights/
1qe02ol,Pragmatism and best practice,"Disclaimer: I'm not a DE but a product manager who has been in my role managing our company's data platform for the last ten months. I come from a non-technical background and so it's been a steep learning curve for me. I've learnt a lot but I'm struggling to balance pragmatism and best practice. 

For context:

\- We are a small team on a central data platform 

\- We do not have any defined data modelling standards or governance standards that are implemented

\- The plan was to move away from our current implementation towards a data mart design. We have a DA but there's no alignment at the senior leadership level across product and architecture so their priorities are elsewhere

\- Analysts sit in another department 

The engineers on my team are understandably advocating for bringing in some foundational modelling, standards work but the company expects quick outputs. 

I want to avoid over-engineering but I'm concerned we will incur a lot of tech debt later on down the line that will need to be unpacked - that's on top of the company not getting the value it envisioned with a platform.

For anyone who has been in this situation do you have any guidance on whether you have:

\- Taken a step back to focus on foundational work? I know a full-scale enterprise data model is not happening at this point but is there something we can begin to bring into our sprints for our higher value use cases? 

\- Do you have a definition of 'good enough' to help keep you moving while minimising later pain? 

I really want to do the best for the team while bearing in mind the questions I know I'll get from leadership in the value of this kind of work. I've been collecting data around trust and in interpreting the data to help evidence this.

A huge thank you in advance .",1768521113.0,7,12,https://www.reddit.com/r/dataengineering/comments/1qe02ol/pragmatism_and_best_practice/
1qdya1r,How do you guys handle the tables and schemas versioning at your company?,"In our current data stack we mostly use AWS Athena for querying, AWS Glue as the data catalog (databases, tables, etc.), and S3 for storage. All the infra is managed with Terraform, that is S3 buckets, Glue databases, table definitions (Hive or Iceberg), table properties, the whole thing.

Lately Iâ€™ve been finding it pretty painful to define Glue tables via Terraform, especially for Iceberg tables with partitions. Iceberg tables with partitions just arenâ€™t properly supported by Terraform, so we ended up with a pretty ugly workaround thatâ€™s hard to read, reason about, and debug.

Iâ€™m curious: Do you run a similar setup? If so, how do you handle table creation? Do you bootstrap tables some other way (dbt, SQL, custom scripts, Glue jobs, etc.) and keep Terraform only for the â€œhardcore-infraâ€?

Would love to hear how others are approaching this and whatâ€™s worked (or not) for you. Thanks!",1768516810.0,9,5,https://www.reddit.com/r/dataengineering/comments/1qdya1r/how_do_you_guys_handle_the_tables_and_schemas/
1qdv9lx,Building my first data warehouse,"I am building the first data warehouse fpr our small company. I am thinking of wether I use Postgresql or Motherduck as data warehouse. What you think? 

The data stack I use in my first several projects will eventually be adopted by our small data team which I want to set up soon.

As I enjoy both Python and SQL, I would choose dbt for transformation. I am going to use Metabase for BI/Reporting.

We are just starting and so we are keeping our cost minimum.

Any recommendations about this data stack I am thinking of...",1768509945.0,8,4,https://www.reddit.com/r/dataengineering/comments/1qdv9lx/building_my_first_data_warehouse/
1qdv3wh,Getting off of Fabric.,"Just as the title says. Fabric has been a pretty rough experience.

I am a team of one in a company that has little data problems. Like, less than 1 TB of data that will be used for processing/analytics in the future with < 200 people with maybe \~20 utilizing data from Fabric. Most data sources (like 90 %) are from on-prem SQL server. The rest is CSVs, some APIs.

A little about my skillset - I came from a software engineering background (SQLite, SQL Server, C#, WinForms/Avalonia). Iâ€™m intermediate with Python and SQL now. The problem. Fabric hasnâ€™t been great, but Iâ€™ve learned it well enough to understand the business and their actual data needs.

The core issues:

* Random pipeline failures or hangs with very little actionable error output
* Ingestion from SQL Server relies heavily on Copy Data Activity, which is slow and compute-heavy
* ETL, refreshes, and BI all share the same capacity
* When a pipeline hangs or spikes usage, capacity shoots up and Power BI visuals become unusable
* Debugging is painful and opaque due to UI-driven workflows and preview features

The main priority right now is stable, reliable BI. I'm open to feedback on more things I need to learn. For instance, better data modeling.

Coming from SWE, I miss the control and being granular with execution and being able to reason about failures via logs and code.

I'm looking at Databricks and Snowflake as options (per the Architect that originally adopted Fabric) but I think since we are still in early phases of data, we may not need the price heavy SaaS.

DE royalty (lords, ladies, and everyone else), let me know your opinions.

  
EDITED: Because there was too much details and colleagues.",1768509582.0,92,106,https://www.reddit.com/r/dataengineering/comments/1qdv3wh/getting_off_of_fabric/
1qdtg0v,What is best System Design Course available on the internet with proper roadmap for absolute beginner ?,"Hello Everyone, 

I am a Software Engineer with experience around 1.6 years and I have been working in the small startup where coding is the most of the task I do. I have a very good background in backend development and strong DSA knowledge but now I feel I am stuck and I am at a very comfortable position but that is absolutely killing my growth and career opportunity and for past 2 months, have been giving interviews and they are brutal at system design. We never really scaled any application rather we downscaled due to churn rate as well as. I have a very good backend development knowledge but now I need to step and move far ahead and I want to push my limits than anything. 

I have been looking for some system design videos on internet, mostly they are a list of videos just creating system design for any application like amazon, tik tok, instagram and what not, but I want to understand everything from very basic, I don't know when to scale the number of microservices, what AWS instance to opt for, wheather to put on EC2 or EKS, when to go for  mongo and when for cassandra, what is read replica and what is quoroum and how to set that, when to use kafka, what is kafka.

Please can you share your best resources which can help me understand system design from core and absolutely bulldoze the interviews. 

All kinds of resources, paid and unpaid, both I can go for but for best.

Thanks.

",1768505836.0,13,1,https://www.reddit.com/r/dataengineering/comments/1qdtg0v/what_is_best_system_design_course_available_on/
1qdqe76,SAP ECC to Azure Using SHIR on VM,"So Here I need to get the data from SAP ECC systems to Azure Ecosystem using SHIR on Virtual Machine

Will be using Table/Odata connectors based on the volume

Here I need some leads/resources in order to do this achieve this

Need suggestions

  
",1768499296.0,1,5,https://www.reddit.com/r/dataengineering/comments/1qdqe76/sap_ecc_to_azure_using_shir_on_vm/
1qdq9bq,Where to go from here?,"Hi DEâ€™s!

Iâ€™m feeling lost about how I should go about my next step in my career, so I was hoping I could find some guidance here.

My story:

After serving 6 years in a technical role in the Unite States Navy, I went to school for compsci for a few years before Covid hit. I never finished school, but continued learning programming and whatnot through good olâ€™ YouTube University, docs, etc - primarily focused on web dev as it was the most accessible.

During school and self teaching, I was working in the service industry (\~6 years of bartending).

Around the middle of 2024, I finally landed my first job in tech in a contracted role as a DE. The contracting company had us train for a couple of months, and then sent us to a predetermined company where I worked primarily with Snowflake and PowerBI. I worked with SQL primarily, and because of my experience with scripting languages, was easily writing SPâ€™s in JS, Python, and even had some fun with Snowflakeâ€™s scripting language.

\*Small context of the company I was contracted to\*:

A brand new company that broke off of a very, very large company. This made working here feel somewhat like a startup, but also already had an insane net worth and company infrastructure/hierarchy. The people I get to work with here are amazing, and itâ€™s been a really amazing experience. Unfortunately, a lot of talent is being dropped from the US and moved to India.

So, to the reason for this post:

Does anyone have any guidance for where I should go from here? I have worked for 1.5 years in this role as a DE, but every entry level job posting I see seems to be looking for 1 of or a mix of:

\- Several years experience

\- Degree

Thank you very much to anyone that reads and responds, I seriously appreciate it!",1768499010.0,0,7,https://www.reddit.com/r/dataengineering/comments/1qdq9bq/where_to_go_from_here/
1qdp2sl,How do you test db consistency after a server migration?,"I'm at a new job and the data here is stored in 2 MSSQL tables, table\_1 is 1TB, table\_2 is 500GB. I'm tasked with ensuring the data is the same post migration as it is now. A 3rd party is responsible for the server upgrade and migration of the data. 

My first thought is to try and take some summary stats, but Select count(\*) from table\_1 takes 13 mins to execute. There are no indexes or even a primary key. I thought maybe I can hash a concatenation of the columns now and compare to the migrated version, but with the sensitivity of hash functions, a non material change would likely invalidate this approach.

Any insights would be really appreciated as I'm not sure quite what to do.",1768496501.0,3,6,https://www.reddit.com/r/dataengineering/comments/1qdp2sl/how_do_you_test_db_consistency_after_a_server/
1qdmier,jdbc/obdc driver in data engineering,Can someone please explain where do we use jdbc/odbc drivers in data engineering. How do they work? Are we using it somewhere directly in data engineering projects. Any examples please. I am sorry if this is a lame question.,1768490862.0,11,13,https://www.reddit.com/r/dataengineering/comments/1qdmier/jdbcobdc_driver_in_data_engineering/
1qdk0i0,AI this AI that,"I am honestly tired of hearing the word AI, my company has decided to be AI-First company and has been losing trade for a year now, having invested AI and built a copilot for the customers to work with, we have a forum for our customers and they absolutely hate it. 

You know why they hate it? Because it was built with zero analysis, built by software engineering team. While the data team was left stranded with SSRS reports. 

Now after full release, they want us to make reports about how good itâ€™s doing, while itâ€™s doing shite. 

I am under a group who wants to make AI as a big thing inside the company but all these corporate people talk about is I need something to be automated. How dumb are people? People considering automation as AI! These are the people who are sometimes making decisions for the company. 

Thankfully my team head has forcefully taken all the AI Modelling work under us, so actually subject matter experts can build the models.

Sorry I just had to rant about this shit which is pissing the fuck out of me. ",1768484991.0,81,40,https://www.reddit.com/r/dataengineering/comments/1qdk0i0/ai_this_ai_that/
1qdi3k9,S3 Delta Tables versus Redshift for Datawarehouse,"We are using AWS as cloud service provider for applications built in cloud. Our company is planning to migrate our Oracle on-premise datawarehouse and hadoop big data to cloud. We would like to have a leaner architecture therefore the lesser platforms to maintain the better. For the datawarehouse capability, we are torn whether to use Redshift or leverage delta tables with S3 so that analysis will use a single service (SageMaker) instead of provisioning Sagemaker and Redshift both. Anyone have experience with this scenario and what are the pros and cons of provisioning Redshift dedicated for datawarehouse capability? ",1768479798.0,7,11,https://www.reddit.com/r/dataengineering/comments/1qdi3k9/s3_delta_tables_versus_redshift_for_datawarehouse/
1qdh5me,Data team size at your company,"How big is the data/analytics/ML team at your company? I'll go first.

Company size: *\~*1800 employees  
  
Data and analytics team size: 7.   
3 internals and 4 externals with the following roles:  
1 Team lead (me)  
2 Data engineers  
1 Data scientist.  
3 Analytics engineers (+me when i have some extra time)

My gut feeling is that we are way understaffed compared to other companies.



",1768476760.0,92,85,https://www.reddit.com/r/dataengineering/comments/1qdh5me/data_team_size_at_your_company/
1qddufp,Optimizing data throughput for Postgres snapshots with batch size auto-tuning | pgstream,We added an opt-in auto-tuner that picks batch bytes based on throughput sampling (directional search + stability checks). In netem benchmarks (200â€“500ms latency + jitter) it reduced snapshot times up to 3.5Ã— vs defaults. Details + config in the post.,1768464528.0,2,0,https://www.reddit.com/r/dataengineering/comments/1qddufp/optimizing_data_throughput_for_postgres_snapshots/
1qddfz7,Handling 30M rows pandas/colab - Chunking vs Sampling vs Lossing Context?,"Iâ€™m working with a fairly large dataset (CSV) (~3 crore / 30 million rows). Due to memory and compute limits (Iâ€™m currently using Google Colab), I canâ€™t load the entire dataset into memory at once.

What Iâ€™ve done so far:

- Randomly sampled ~1 lakh (100k) rows
- Performed EDA on the sample to understand   distributions, correlations, and basic patterns

However, Iâ€™m concerned that sampling may lose important data context, especially:

- Outliers or rare events
- Long-tail behavior
- Rare categories that may not appear in the sample

So Iâ€™m considering an alternative approach using pandas chunking:

- Read the data with chunksize=1_000_000
- Define separate functions for:
- preprocessing
- EDA/statistics
- feature engineering

Apply these functions to each chunk

Store the processed chunks in a list

Concatenate everything at the end into a final DataFrame

My questions:

1. Is this chunk-based approach actually safe and scalable for ~30M rows in pandas?

2. Which types of preprocessing / feature engineering are not safe to do chunk-wise due to missing global context?

3. If sampling can lose data context, whatâ€™s the recommended way to analyze and process such large datasets while still capturing outliers and rare patterns?

4. Specifically for Google Colab, what are best practices here?

-Multiple passes over data?
-Storing intermediate results to disk (Parquet/CSV)?
-Using Dask/Polars instead of pandas?

Iâ€™m trying to balance:

-Limited RAM
-Correct statistical behavior
-Practical workflows (not enterprise Spark clusters)

Would love to hear how others handle large datasets like this in Colab or similar constrained environments",1768463055.0,9,17,https://www.reddit.com/r/dataengineering/comments/1qddfz7/handling_30m_rows_pandascolab_chunking_vs/
1qdd2gp,AWS Glue visual etl: Issues while overwriting files on s3,"I am building a Lakehouse solution using aws glue visual etl.When writing the dataset using the target s3 node in visual editor, there is no option to specify writemode() to overwrite  
When i checked in the generated script, it shows .append() as default glue behaviour, and i am shocked to say there is no option to change it.Tried with different file format like parquet/iceberg, same behaviour  
  
This is leading to duplicates in the silver and ultimately impacting all downstream layers.  
Has anyone faced this issue and figured out a solution  
And using standard spark scripts is my last option!! ",1768461698.0,1,1,https://www.reddit.com/r/dataengineering/comments/1qdd2gp/aws_glue_visual_etl_issues_while_overwriting/
1qdb1gg,Senior DE - When did you consider yourself a senior?,"Hey guys, wondering how would you tell when a data engineer is senior, or when did you feel like you had the knowledge to consider yourself as a senior DE? 

Do you think is a matter of time (like certain amount of years of experience), amount of tech stack youâ€™re familiar with, data modeling with confidence, a mix of all of this, etc. Please elaborate on your answers!!

Plus, what would be your recommendations for jumping from junior -> to mid -> to senior, experience wise. 
",1768454943.0,24,22,https://www.reddit.com/r/dataengineering/comments/1qdb1gg/senior_de_when_did_you_consider_yourself_a_senior/
1qdat9l,Bay Area Engineers; what are your favorite spots?,"I'm a field marketer that who works for a tech company that targets engineers (software application, architects, site reliability). Each year it's been getting more difficult to get quality attendees to attend our events. So, I'm asking the reddit engineer world... what are your favorite events? What draws you to attend? Any San Francisco, San Jose, Sunnyvale favorites? ",1768454234.0,1,2,https://www.reddit.com/r/dataengineering/comments/1qdat9l/bay_area_engineers_what_are_your_favorite_spots/
1qd6cg5,Need help regarding warehouse doubt,"Hi there new to data engineer. Learning azure.
So I have doubt like 
We ingest data using adf to adls bronze layer.
From there data bricks pick file and transform data and store to adls silver. 

What next . What is how it goes to gold layer?
Is gold layer act as data warehouse?
Whatever query we perform  on data, that output is from gold layer or silver.

Please Help. ",1768441648.0,0,2,https://www.reddit.com/r/dataengineering/comments/1qd6cg5/need_help_regarding_warehouse_doubt/
1qd17gs,Many DE tasks and priorities to organize,"Where I work, there is no Scrum. Tickets keep coming in, and the coordinator distributes them and sets priorities. There are no sprints, because management frequently overrides priorities due to requests from the board and other management areasâ€”almost on a daily basis. Itâ€™s basically a ticket queue that we execute as it comes.

During the day, I receive many different demands: validating data, mapping new tables, checking alerts from failed processes, discussions about possible data inconsistencies, reviewing PRs, helping interns, answering questions from people on other teams, etc.

Sometimes more than 10 people message me at the same time on Teams. I try to filter, organize priorities, and postpone what is not feasible to do on the same day, but more demands arrive than I can realistically handle, so tasks keep piling up.

We do have a team board, but I donâ€™t like tracking everything there because some tasks are things like â€œtalk to person X about Yâ€ or â€œvalidate what person X did wrong,â€ which I donâ€™t want to expose directly to colleagues and managers. So on the board I keep things more generic, without many comments

Lately, Iâ€™ve been putting everything into a single markdown file (tasks and personal notes). The most urgent items go to the top of the list as a simple TODO, but it keeps growing and sometimes it becomes hard to manage tasks and priorities

Naturally, there are tasks that never get done. My manager is aware of this and agrees that they should only be prioritized when it makes sense, but new ones keep coming in, and I miss having a tool where I could search for similar tasks or something along those lines

Have you ever faced this difficulty in organizing tasks? Do you have any tips for a simple workflow? I tried using some tools like Todoist and Taskwarrior, but I ended up preferring the ease of searching in a single file, even though it grows very large very quickly and eventually becomes messy and difficult to manage. Thanks",1768428852.0,4,4,https://www.reddit.com/r/dataengineering/comments/1qd17gs/many_de_tasks_and_priorities_to_organize/
1qd0f6o,Dagster newbie here: Does anyone have experience writing to an Azure-based Ducklake within the Dagster Project? And then running the whole thing in Docker?,"I am a Dagster newbie and have started my first project, in which I use DuckDB to read json files from a folder and write them to Ducklake. My Ducklake uses Azure Data Lake Storage Gen2 for storage and Postgres as a metadata catalog.

Writing to ADLS has been possible since DuckDB version 1.4.3 and works wonderfully outside of my project.



Locally (via dg dev), I can run the Dagster asset without any problems so that data arrives in Ducklake.

Now I have the whole thing running in containers via Docker Compose (1 for logging, 1 for the web server, 1 for the daemon, and 1 for the codebase), and it is not working. The run can be started, but it breaks at the point of writing with the error messages: 

Error: IO Error: AzureBlobStorageFileSystem could not open file

and 

DuckDB Error: Fail to get a new connection for: https://xxxxxxxxx.blob.core.windows.net.

I have already run a separate container as a test, which runs with the same image as the Dagster codebase server and only executes the Python script of the asset. Everything works there. It seems to me that it only doesn't work in the Dagster project Docker context. 

Can anyone help me, because I'm getting pretty desperate at this point.

",1768427039.0,1,4,https://www.reddit.com/r/dataengineering/comments/1qd0f6o/dagster_newbie_here_does_anyone_have_experience/
1qd08xf,Explore public datasets with Apache Iceberg & BigLake,"Hi r/dataengineering! Iâ€™m part of the Google Open Source team, and sharing a new post from our Biglake team

Since your data should not be locked into a single engine and it should be accessible, interoperable, and built on open standards. They put together this post withÂ public datasetsÂ available via theÂ Apache Iceberg REST Catalog. These are hosted on BigLake and are available for read-only access to anyone with a Google Cloud account.

You can useÂ Apache Spark, Trino, or Flink, and connect to a live, production-grade Iceberg Catalog and start querying immediately. They utilized the classicÂ **NYC Taxi**Â dataset to showcase features like:

* **Partition Pruning:**Â Skip scanning unnecessary data entirely
* **Time Travel:**Â Query the table as it existed at a specific point in the past
* **Vectorized Reads:**Â Batch process Parquet files for high efficiency

What other public datasets would be most helpful for you to see in an open Iceberg format for your benchmarking or testing? I'll be sure to pass it along.",1768426632.0,2,0,https://www.reddit.com/r/dataengineering/comments/1qd08xf/explore_public_datasets_with_apache_iceberg/
1qd04ke,Anyone getting calls for mid level?,"Just curious if anyone is getting calls from recruiters for mid level, or #2/3 in a smaller company. I'm getting calls but for positions that would be a reach for me in today's market. I'm ready to settle but not seeing recruiters reach out with those opportunities. 

Do I need to start throwing my hat in the ring with what I typically consider black hole job applications? Is there somewhere better to find jobs at smaller companies where you do more? That's where I better fit. ",1768426354.0,0,14,https://www.reddit.com/r/dataengineering/comments/1qd04ke/anyone_getting_calls_for_mid_level/
1qcz83k,What is the intent of a SQL test with a question bank provided in advance?,"For any hiring managers in here Iâ€™m curious on this one. I have a technical round for an analytics engineer position and they provided me with a question bank of 7 SQL questions ahead of time, saying they will likely ask me all of them. I think the main thing Iâ€™m curious on is if they provide candidates with the questions ahead of time most people will just figure out the solutions and memorize them so youâ€™d get roughly the same result for everyone. It seems to me then that the intention is to test soft skills in how you go about working and communicating? 
Itâ€™s also the only technical, after this itâ€™s just behavioral rounds with team members ",1768424317.0,2,6,https://www.reddit.com/r/dataengineering/comments/1qcz83k/what_is_the_intent_of_a_sql_test_with_a_question/
1qcw5qe,Data modeling is far from dead. Itâ€™s more relevant than ever,"Thereâ€™s been an interesting shift in the seas with AI. Some people saying we donâ€™t need to do facts and dimensions anymore. This is a wild take because product analytics donâ€™t suddenly disappear because LLM has arrived. 

It seems like to me that multi-modal LLM is bringing together the three types of data:

\- structured 

\- semi-structured

\- unstructured 

Dimensional modeling is still very relevant but will need to be augmented to include semi-structured outputs from the parsing of text and image data. 

The necessity for complex types like VARIANT and STRUCT seems to be rising. Which is increasing the need for data modeling not decreasing it. 

It feels like some company leaders now believe you can just point an LLM at a Kafka queue and have a perfect data warehouse which is still SO far from the actual reality of where data engineering sits today 

Am I missing something or is the hype train just really loud right now? ",1768417492.0,82,43,https://www.reddit.com/r/dataengineering/comments/1qcw5qe/data_modeling_is_far_from_dead_its_more_relevant/
1qcuk7k,Is salting only the keys with most skew ( rows) the standard practice in PySpark?,"Salting every key will produce unneccesary overhead, but most tutorials I see salt all the keys ",1768414063.0,5,11,https://www.reddit.com/r/dataengineering/comments/1qcuk7k/is_salting_only_the_keys_with_most_skew_rows_the/
1qcu4q7,Github repo on Databricks,"I am working on model validation and one goal is to do a code review and reproduce results using the modelâ€™s script. Letâ€™s say the developer shared the py script which is in a github repo:

Example link: github.com/company/maindir/folder1/folder2/folder3/model.py

model.py is a python script with classes and functions. The script has dependencies i.e. it calls function or classes from other py scripts in different folders. All of the dependencies are in github.com/company/maindir

I am using a notebook on Databricks and i want use a function from the model.py. How do I do that without manually copy-paste all the script, main script and dependencies, on my notebook?

Details:
Github and databricks are all company accounts",1768413141.0,2,0,https://www.reddit.com/r/dataengineering/comments/1qcu4q7/github_repo_on_databricks/
1qctcgu,Designing inverted indexes in a KV-store on object storage,,1768411459.0,2,0,https://www.reddit.com/r/dataengineering/comments/1qctcgu/designing_inverted_indexes_in_a_kvstore_on_object/
1qct0b0,Snowflake Certs,"Hi All, 

I am moving back to the snowflake world after working in GCP for a few years. I did my GCP data engineer and GCP cloud architect certs, which were fun but very time consuming. 

For anyone who has done multiple certs how tough are the Snowflake Ones? Which ones are worth doing any maybe more for marketing? 

Iâ€™m excited to come back to Snowflake, but I will miss Bigquery and its pay per query model and automatic scaling and slots. ",1768410711.0,8,6,https://www.reddit.com/r/dataengineering/comments/1qct0b0/snowflake_certs/
1qcs359,How expensive is CDC in terms of performance?,"Hi there,
I'm tasked with pulling data from a source system called diamant/4 (german software for financial accounting) into our warehouse. The sources db runs on mssql with CDC deactivated. For extraction i'm using airbyte with a cursor column. The transformations are done in dbt. 

Now from time to time bookings in the source system get deleted. That usually happens when an employee fucks up and has to batch-correct a couple of bad bookings. 

I'm order to invalidate the deleted entries in my warehouse I want to turn on CDC on the source. I do not have any experience with CDC. Can anyone tell me if it does have a big impact in terms of performance on the source?",1768408709.0,6,5,https://www.reddit.com/r/dataengineering/comments/1qcs359/how_expensive_is_cdc_in_terms_of_performance/
1qcs226,Data retention sounds simple till backups and logs enter the chat,"Weâ€™ve been getting more privacy and compliance questions lately and the part that keeps tripping us up is retention. Not the obv stuff like delete a user record, but everything around backups/logs/analytics events and archived data.

  
The answers are there but theyâ€™re spread across systems and sometimes the retention story changes from person to person.



  
Anything that can help us prevent this is appreciated",1768408642.0,41,4,https://www.reddit.com/r/dataengineering/comments/1qcs226/data_retention_sounds_simple_till_backups_and/
1qcs0x0,Automating Snowflake Network Policy Updates,"We are looking to automate Snowflake network policy updates. Currently, static IPs and Azure IP ranges are manually copied from source lists and pasted into an ALTER NETWORK POLICY command on a weekly basis.



We are considering the following approach:

* Use a Snowflake Task to schedule weekly execution
* Use a Snowpark Python stored procedure
* Fetch Azure Service Tag IPs (AzureAD) from Microsoftâ€™s public JSON endpoint
* Update the network policy atomically via ALTER NETWORK POLICY



We are considering to use External Access Integration from Snowflake to fetch both Azure IPs and static IPs.

**Has anyone implemented a similar pattern in production? How to handle static IPs, which are currently published on an internal SharePoint / Bitbucket site requiring authentication? What approach is considered best practice?**

Thanks in advance.",1768408570.0,3,4,https://www.reddit.com/r/dataengineering/comments/1qcs0x0/automating_snowflake_network_policy_updates/
1qcpucj,What ai tools are out there for jupyter notebooks rn?,"Hey guys, is there any cutting edge tools out there rn that are helping you and other jupyter programmers to do better eda? The data science version of vibe code. As ai is changing software development so was wondering if there's something for data science/jupyter too. 

I have done some basic reasearch. And found there's copilot agent mode and cursor as the two primary useful things rn. Some time back I tried vscode with jupyter and it was really bad. Couldn't even edit the notebook properly. Probably because it was seeing it as a json rather than a notebook. I can see now that it can execute and create cells etc. Which is good. 

Main things that are required for an agent to be efficient at this is 

a) be able to execute notebooks cell by cell ofc, which ig it already can now. 
b) Be able to read the memory of variables. At will. Or atleast see all the output of cells piped into its context. 

Anything out there that can do this and is not a small niche tool. Appreciate any help what the pros working with notebooks are doing to become more efficient with ai. Thanks",1768403695.0,1,8,https://www.reddit.com/r/dataengineering/comments/1qcpucj/what_ai_tools_are_out_there_for_jupyter_notebooks/
1qcoxbb,Tools to Produce ER Diagrams based on SQL Server Schemas,"Can anyone recommend me a good ER diagram tool? 

Unfortunately, our org works out of a SQL Server database that is poorly documented and which is lacking many foreign keys. In fact, many of the tables are heap tables. It sounds very dumb that it was set up this way, but our application is extremely ancient and heap tables were preferred at the time because in the early days of SQL Server bulk inserts ran quicker on heap tables. 

Ideally, I would like a tool that uses some degree of AI to read table schemas and generate ER diagrams. Looked at DBeaver as an option, but Iâ€™m wondering what else is out there. 

Any recommendations? 

Thanks much! ",1768401531.0,11,20,https://www.reddit.com/r/dataengineering/comments/1qcoxbb/tools_to_produce_er_diagrams_based_on_sql_server/
1qcmttf,need guidance on how to build an analytics tool,"I am planning on building a web analytic tool (basically trying to bring a GoogleAnalytics easier to use) and have no technical background.

Here's what I understood from my readings so far :

the minimal viable tech architecture as I understand it is

1. A SDK is running on the website and sending events to an API ingestion (I have no idea how to build both thoses things but that's not my concern at the moment)
2. That API then sends data to GooglePub/Sub that will then send it to
   1. GoogleCloudStorage (for raw data storage, source of truth)
   2. Clickhouse (for quick querying)
3. use dbt to transform data from clickhouse into business ready information
4. Build a UI layer to display information from clickhouse

NB : the tools I list here are what I selected when looking for tools that would be cheap / scalable and give me enough control over the data to later customize my analytic tool as I want.

I am very new to this environment so I am curious to have some of expert insight about my understanding, make sure I don't miss understand or miss out on an important concept here

Thank you for your help ğŸ™",1768396121.0,3,14,https://www.reddit.com/r/dataengineering/comments/1qcmttf/need_guidance_on_how_to_build_an_analytics_tool/
1qcl76i,Self-service BI recommendations,"Hello!

I plan to set up a self-service BI software for my company to allow all employees to make investigations, build dashboards, debug services, etc. I would like to get your recommendations to choose the right tool.

In term of context my company has around 70-80 people so far and is in the financial services sector. We use AWS as cloud provider and a provisioned Redshift instance for our data warehouse. We already use Retool as a ""back-office"" solution to support operations and monitor some metrics, but this tool requires engineers work to add new features, this not self-service.

The requirements I have for it would be:
- Self-service : all employees can build dashboards, make queries with SQL or low-code options
- SSO with existing company account
- Permissions linked to pre-existing RBAC solution
- Compatibility with Redshift 

My current experience in term of BI is limited to Metabase which was very positive (cheap infrastructure, simple to use and manage) so for now I'm thinking to use it again unless you have a better option to suggest. I'm planning to discuss the BI topic with different teams to assess their respective needs and experience too.

Thanks !",1768391120.0,0,9,https://www.reddit.com/r/dataengineering/comments/1qcl76i/selfservice_bi_recommendations/
1qcl1rh,2026 benchmark of 14 analytics agents,"This year I want to set up on analytics agent for my whole company. But there are a lot of solutions out there, and couldn't see a clear winner. So I benchmarked and tested 14 solutions: BI tools AI (Looker, Omni, Hex...), warehouses AI (Cortex, Genie), text-to-SQL tools, general agents + MCPs.

Sharing it in a substack article if you're also researching the space -",1768390612.0,17,6,https://www.reddit.com/r/dataengineering/comments/1qcl1rh/2026_benchmark_of_14_analytics_agents/
1qckbji,When are skills worth more than money?,"When is the right time to move on if your company is consistently exposing you to new (highly sought after) skills, but the pay is not raising in the same level as your ability / skill difficulty relative to the peers in your pay grade?

Strictly speaking about being a DA but learning and working in cloud infrastructure rather than SQL / Tableau",1768388084.0,4,13,https://www.reddit.com/r/dataengineering/comments/1qckbji/when_are_skills_worth_more_than_money/
1qcj2m7,picking the right internship as a big data student,"Hi everyone i'm in my final year as a big data and iot student and i'm supposed to have an internship at the end of this year. Normally this internship will be my only experience or my first look into work so it should be preferally in sth i wanna continue working in. I've been applying to data engineering internships and passed onlu one offer but no answer so far and i got one for using ai in cctv and i already accepted. So i'm lost do i get into ai with cctv and don't look back and after ending the internship maybe i apply to de roles or do i try more to find data internships. 

Any advise would be helpful.",1768383401.0,1,0,https://www.reddit.com/r/dataengineering/comments/1qcj2m7/picking_the_right_internship_as_a_big_data_student/
1qcirse,Table or View for dates master in azure synapse,"I want to create a date master to be used in many stored procedures each for different KPI calculations. So that the dates master is repeatedly used, it should be a view or a table. But which one will be better to be used view or table? If I use table can there be any cons?

  
Dates master is created using row number.

https://preview.redd.it/cq6rxrq38adg1.png?width=517&format=png&auto=webp&s=e07c26d234e640eb87335ee9e36358ed595151b6

",1768382246.0,1,2,https://www.reddit.com/r/dataengineering/comments/1qcirse/table_or_view_for_dates_master_in_azure_synapse/
1qcdtes,Best ETL for 2026,"Hi,

Need help to opt for best etl tool in 2026.  
Currently we are using informatica cloud . i want to move out of this tool .  
Suggest me some etl technology which is used now in common and will have good future .",1768365032.0,30,101,https://www.reddit.com/r/dataengineering/comments/1qcdtes/best_etl_for_2026/
1qca3r2,Am I under skilled for my years of experience?,"My experience: DE in a FTSE financial services company for almost 2 years. 

I am worried that my companies limited tech stack / my narrow role is limiting my career progression - not sure if what my day to day work looks like is normal? My role is primarily around building internal facing data products in Snowflake for the business. I have owned and delivered a significant and highly used 'customer 360' MDM data product as the main/sole data engineer in my team,  but my role is really just that - I don't do much else outside of Snowflake. We also don't use Dbt so I don't have any real world experience with that either.

  
Similar to another post made on here recently, I don't know how to do a lot of stuff that is mentioned on here simply because I've never had the chance to. I don't really know what containerisation is, I don't know how to spin up VM's, all the different Azure/AWS tools.

  
In terms of technical skills, I would rank myself as the following:

* SQL - Intermediate (maybe creeping into advanced here and there but I need AI help). I can write production level code
* Data modelling - beginner (can design/build a 3nf and star schema, I dont understand data vault)
* Python - I'm not specialised at all as we don't really use Python too much but I can write Python code well enough that it is understood by anyone, although it might not be optimal, and I can understand/copy most Python code I've seen. I have a few Python projects I've done over the years.
* APIs - no experience
* Kafka - understand the concepts but I find it so complicated. I've made a new topics and connectors with a lot of help.
* Dbt - 2 projects I've done on my own, no experience at work.
* Airflow - played around with it with some personal projects but nothing major - my team doesn't use it at work so I have no opportunity to
* CI/CD - fairly good understanding
* Documentation - I can make good documentation.",1768354762.0,19,8,https://www.reddit.com/r/dataengineering/comments/1qca3r2/am_i_under_skilled_for_my_years_of_experience/
1qc8xe0,How important is Scala/Java & Go for DEs  ?,"basically a electrical engineer with little experience to coding during bachelors. Switched jobs around 2 years back to DE focused role and basically deal with Python,  REST API, Airflow,SQL,GCP ,GBQ.

Tech stack does not involve Spark. I have seen DEs in Linkdin whom I follow have listed Scala/Java and Golang in their skillset. ( sorry for the Linkdin Cringe they post with always a common hook) 

I have also read Scala/Java go hand in hand with Spark but how important would that be to get a job or switch to a new job etc. 

I don't have production grade experience using Pyspark but lately able to solve questions platforms like StrataScratch and considering building pet projects and reading internals to gain understanding. 

Question: 

1. Should I pursue learning Java or Scala in future ? Would that be helpful in DE setting ?   
  
2. What is purpose of Golang for DEs  

Any help would be appreciated ",1768351642.0,4,12,https://www.reddit.com/r/dataengineering/comments/1qc8xe0/how_important_is_scalajava_go_for_des/
1qc6zzq,Master for Data Engineer,"Hello,

I work as a data warehouse developer in a small company in Washington. I have my bachelors outside the U.S. and have about 4 years of experience working as a Data Engineer overseas. Iâ€™ve been working in the U.S. for roughly 1.5 years now. I was thinking of doing a part time masters along with my current job so I can get a deeper understanding of DE topics and also have a degree in the US for better job opportunities. Iâ€™ve been looking into programs for working professionals and found the MSIM programs at the University of Washington that focus on Business Intelligence and Data Science, as well as the Masterâ€™s in Computer Information Systems at Bellevue University. Iâ€™m considering applying to both.

Would love to hear any recommendations or suggestions for masterâ€™s programs that might be a good fit for my background.

Thanks",1768346775.0,2,5,https://www.reddit.com/r/dataengineering/comments/1qc6zzq/master_for_data_engineer/
1qc67yw,Picking the right stack for the most job opportunities,"Fellow folks in the U.S., outside of the visualization/reporting tool (already in place - Power BI), what scalable data stack would you pick if the one of the intentions (outside of it working & being cost effective, lol) is to give yourself the most future opportunities in the job market? (Note, I have been researching job postings and other discussions online).Â 

I understand itâ€™s going to be a combination of tools, not one tool.

My useÂ cases work don't have ""Big Data"" needs at the moment.

Seems like Fabric is half-baked, not really hot in job postings, and not worth the cost. It would be the least amount of up-skilling for me though.

Seeing a lot of Snowflake & Databricks.

Iâ€™m newish to this piece of it, so please be gentle.Â 

Thanks",1768344871.0,43,45,https://www.reddit.com/r/dataengineering/comments/1qc67yw/picking_the_right_stack_for_the_most_job/
1qc4x46,Azure or AWS,"Iâ€™m transitioning into Data Engineering and have noticed a clear divide in the market. While the basics (SQL, Python, Spark) are universal, the tools differ:

Azure: ADF, Databricks, Synapse, ADLS etc.

AWS: s3,Glue, Redshift, EMR, Snowflake, Airflow, etc.

I spent the last 6 months preparing for the Azure stack. However, now that I'm applying, the ""good"" product-based companies Iâ€™m targeting (like Amex, Barclays) seem to heavily favor the AWS stack.

Is it worth trying to learn both stacks now? Or should I stick to Azure and accept that I might have to start at a service-based company rather than a top-tier product firm? My ultimate goal is just to get my foot in the door as a DE.

Ps: I am having 5 YOE",1768341913.0,21,21,https://www.reddit.com/r/dataengineering/comments/1qc4x46/azure_or_aws/
1qc0nwe,Senior Data Engineer in Toronto Pay,"I spoke with a Talent Acquisition Specialist at Skip earlier today during a call, and she mentioned that the base salary range for the Senior Data Engineer role in Toronto is **$90Kâ€“$110K**. I just wanted to confirm whether this range is",1768332384.0,13,12,https://www.reddit.com/r/dataengineering/comments/1qc0nwe/senior_data_engineer_in_toronto_pay/
1qc03rf,Finishing Masters vs Certificates,"I have recently signed up to start a masters program for data analysis with a some focus on engineering, but I have been having second thoughts. I have been thinking that getting a certificate, and building out a custom portfolio may work fine as well, if not better than a masters (also not to mention I would be saving thousands of dollars in out of pocket tuition). Any thoughts on certificates to get me started down the data engineering path, and if I should or shouldn't stick with the masters program?",1768331164.0,6,1,https://www.reddit.com/r/dataengineering/comments/1qc03rf/finishing_masters_vs_certificates/
1qby5px,Should I switch from data engineering?,"I got laid off on may, but no offer so far.I have 3 years of experience, I mostly used ssis and sql.I did get a certificate for azure after getting laid off.I am kinda lost. I am studying for comptia to get a help desk job.",1768327059.0,8,3,https://www.reddit.com/r/dataengineering/comments/1qby5px/should_i_switch_from_data_engineering/
1qbx8ay,Data engineer with 4 years what do I need to work on,"Hi all,

Iâ€™m a data engineer with 4 years experience , currently earning Â£55k in London at a mid sized company.

My career has been a bit rocky so far and I feel like for various reasons I donâ€™t have the level of skills that I should have for a mid level engineer , I honestly read threads on this sub Reddit and sometimes havenâ€™t even got a clue what people are talking about which feels embarrassing given my experience level.

Since Iâ€™m the only data engineer at my company or atleast in my team itâ€™s hard to know how good I am or what I need to work on.


Hereâ€™s what I can and canâ€™t do so far

I can:
-Do basic Python without help from AI, including setting up and API call

-Do I would say mid level SQL without help from AI

-Write python code according to good conventions like logging, parameters etc

-Can understand pretty much all SQL scripts upon reading them and most Python scripts

-Set up and schedule and airflow DAG (only just learnt this though)

-Use the major tools in the GCP suite mostly bigquery and cloud storage

-Set up scheduled queries

-Use views in bigquery and set them up to sit on a looker dashboard

-have some basic visualisation experience with power bi and looker too

-Produce clear documentation



I donâ€™t know how to:

-Set up virtual machines

-Use a lot of the more niche GCP tools (again I donâ€™t even really know what I donâ€™t know here)

-do any machine learning or data science 

-Do mid level Python problems list list 
comprehensions etc without help from AI

-Do advanced SQL problems without help from AI.

-Use AWS or azure

-Use databricks 

-Use Kafka

-Use dbt 

-Use pyspark

And probably more stuff I donâ€™t even know I donâ€™t know

I feel like my experience is honestly more around the 2 years sort of level,  I have been a little lazy in terms of upskilling but also had a couple of major life events that disrupted my career I wonâ€™t go into here

Where can I get the best bang for my buck so to speak upskill I f over the next year or so the trying to pivot for a higher salary somewhere else, right now I have no problem getting interviews and pass the cultural fit phase mostly as Iâ€™m well spoken and likeable but always fail the technical assesment  (0/6 is my record lol)",1768325094.0,71,40,https://www.reddit.com/r/dataengineering/comments/1qbx8ay/data_engineer_with_4_years_what_do_i_need_to_work/
1qbw9su,Flows with set finish time,"Iâ€™m using dbt with an orchestrator (Dagster, but AirFlow is also possible), and I have a simple requirement:

I need certain dbt models to be ready by a specific time each day (e.g. 08:00) for dashboards.

I know schedulers can start runs at a given time, but Iâ€™m wondering what the recommended pattern is to:

	â€¢	reliably finish before that time

	â€¢	manage dependencies

	â€¢	detect and alert when things are late

Is the usual solution just scheduling earlier with a buffer, or is there a more robust approach?

Thanks!",1768322511.0,2,4,https://www.reddit.com/r/dataengineering/comments/1qbw9su/flows_with_set_finish_time/
1qbvuxn,Your HashMap ran out of memory. Now what?,"Compaction in data lakes can require tracking millions of record keys to match updates against base files. Put them all in a HashMap and you OOM.

Apache Hudi's solution is ExternalSpillableMap - a hybrid structure that uses an in-memory HashMap until a threshold, then spills to disk. The interface is transparent: get() checks memory first then disk, and iteration chains both seamlessly.

Two implementation details I found interesting:

1. Adaptive size estimation: Uses exponential moving average (90/10 weighting) recalculated every 100 records instead of measuring every record. Handles varying record sizes without constant overhead.

2. Two disk backends: BitCask (append-only file with in-memory offset map) or RocksDB (LSM-tree). BitCask is simpler, RocksDB scales better when even the key set exceeds RAM.",1768321617.0,2,0,https://www.reddit.com/r/dataengineering/comments/1qbvuxn/your_hashmap_ran_out_of_memory_now_what/
1qbu1np,Apache Iceberg Table Maintenance Tools You Should Know,,1768317567.0,1,0,https://www.reddit.com/r/dataengineering/comments/1qbu1np/apache_iceberg_table_maintenance_tools_you_should/
1qbtl1g,Data Engineering Security certificates,"Hi, I want to move to other domain (manufacturing -> banking) and Security certificates for data engineers are a great advantage there. Any ideas about easy to get (1 month studying max) certificates? My stack is Azure/databricks/snowflake",1768316508.0,5,0,https://www.reddit.com/r/dataengineering/comments/1qbtl1g/data_engineering_security_certificates/
1qbs00s,"Is maintenance necessary on bronze layer, append-only delta lake tables?","Hi all,

I am ingesting data from an API. On each notebook run - one run each hour - the notebook makes 1000 API requests. 

In the notebook, all the API responses get combined into a single Dataframe, and the dataframe gets written to a bronze delta lake table (append mode).

Next, a gold notebook reads the newly inserted data from the bronze table (using a watermark timestamp column) and writes it to a gold table (also append).

On the gold table, I will run optimize or auto compaction, in order to optimize for end user queries. I'll also run vacuum to remove old, unreferenced parquet files.

However, on the bronze layer table, is it necessary to run optimize and vacuum there? Or is it just a waste of resources?

Initially I'm thinking that it's not necessary to run optimize and vacuum on this bronze layer table, because end users won't query this table. The only thing that's querying this table frequently is the gold notebook, and it only needs to read the newly inserted data (based on the ingestion timestamp column). Or should I run some infrequent optimize and vacuum operations on this bronze layer table?

For reference, the bronze table has 40 columns, and each hourly run might return anything from ten thousand to one million rows.

Thanks in advance for sharing your advices and experiences.",1768312706.0,7,6,https://www.reddit.com/r/dataengineering/comments/1qbs00s/is_maintenance_necessary_on_bronze_layer/
1qbrxvk,Confused whether to shift from Data Science to Cloud/IT as a 5 year integrated Bsc-MSc Data Science student,Iâ€™m a final year MSc data science student and now I got an internship at a data centre with a role of IT Ops. I accepted it cause job market in Data science is really tough. So I want to switch to Cloud and IT. Is that okay? How hard it is?,1768312560.0,2,3,https://www.reddit.com/r/dataengineering/comments/1qbrxvk/confused_whether_to_shift_from_data_science_to/
1qbrhgn,Im Burnt Out,"My company had a huge amount of layoffs last year. My team went from 4 DEs to 2. Right now the other DE is on leave and its just me. 

The amount of work hasnt changed and theres a ton of tribal business logic I never even learned. Every request is high priority. We also merged with another company and the new cto put their data person in charge. This guy only works with SSIS and we are a python shop. He also hates python.

Im completely burnt out and have been job hunting for months. The market is ass and I do 2-3 rounds of interviews just to get ghosted by so no name company. Anyone else in a similar boat? Im ready to just quit and chillax ",1768311393.0,122,91,https://www.reddit.com/r/dataengineering/comments/1qbrhgn/im_burnt_out/
1qbq2eg,Conversational Analytics (Text-to-SQL),"context: I work at a B2B firm  
We're building native dashboards, and we want to provide text-to-sql functionality to our users, where they can simply chat with the agent, and it'll automatically give them the optimised queries, execute them on our OLAP datawarehouse (Starrocks for reference) along with graphs or charts which they can use in their custom dashboards.

**I am reaching out to the folks here to help me with good design or architecture advice, or some reading material I can take inspiration from.**  
**Also, we're using Solr, and might want to build the knowledge graph there. Can someone also comment on can we use solr for GraphRAG knowledge graph.**  
   
I have gone through a bunch of blogs, but want to understand from experiences of others:  
1. Uber text-to-sql  
2. Swiggy Hermes  
3. A bunch of blogs from wren  
4. couple of research papers on GraphRAG vs RAG",1768307410.0,7,10,https://www.reddit.com/r/dataengineering/comments/1qbq2eg/conversational_analytics_texttosql/
1qbpoh1,Relational DBMS systems are GOATed,"I'm currently doing a master's degree in CS and I have taken a few database related courses. In a course I delved deep into the theory of relational algebra, transactions, serializability, ACID compliancy, paging, memory handling, locks etc., it was fascinating to see how decades of research had perfected the relational databases. Not to diss any modern cloud based batch processing big data platforms, but they seem to throw away a lot of clever stuff from RDBMSs as trade-off for bandwidth, which is fine, they do what they are supposed but it feels like boring transactional databases like Postgres, MySQL or Oracle don't get talked about often especially in the 'big data' sphere and 'data driven' world.

PS: I don't have much experience in the industry and feel free to counter my opinions",1768306202.0,81,37,https://www.reddit.com/r/dataengineering/comments/1qbpoh1/relational_dbms_systems_are_goated/
1qbpdqg,Auditing columns are a god's sent for batch processing,"Was trying to figure out a very complex issue from the morning, with zero idea of where tge bad data propagated out of . Just towards the EOD I started looking at the updated\_at of all the faulty data and found one common batch which created all the problems

  
Ik I should have thought of this earlier, but I am an early career DE and I just felt I learn something invaluable today

",1768305262.0,7,0,https://www.reddit.com/r/dataengineering/comments/1qbpdqg/auditing_columns_are_a_gods_sent_for_batch/
1qbp417,How to think like architect,"My question is how can i think like an data architect? - i mean to say that designing data pipelines and optimising existing once, structuring and modelling the data from scratch for scalability and cost saving...

Like i am trying to read couple of books and following online content of Data Engineering, but i know the scenarios in real projects are completely different present anywhere on the internet.

So, I got my basic to intermediate understanding of all the DE related things and concepts and want to brainstorm and practice realworld scenarios so that i can think more accurately and sophisticatedly as a DE, as i am not on any project in my current org.

So, If you guys can share me some of the resources you know to learn and get exposure from and practice REAL stuff or can share some interesting usecases and scenarios you encountered in your projects. I would be greatful and it would also help the community as well.

  
Thanks",1768304330.0,6,7,https://www.reddit.com/r/dataengineering/comments/1qbp417/how_to_think_like_architect/
1qbo9zn,A Diary of a Data Engineer,"An idea I had for a while was to write an article in the style of Â«**A Diary of a CEO**Â», but for data engineering.

This article traces the past 23 years of the invisible work of plumbing, written as my diary as a data engineer, including its ups and downs. The goal is to help newly arriving plumbers and data engineers who might struggle with the ever-changing landscape.

I tried to give advice to my younger self at the start of my career. Insights from hard learnings I got during my profession as an ETL developer, business intelligence engineer, and data engineer:

1. The tools will change. The fundamentals wonâ€™t.
2. Talk to the business people.
3. Youâ€™re building the foundation, not the showcase.
4. Data quality is learned through pain.
5. Presentation matters more than you think.
6. Set boundaries early.
7. Donâ€™t chase every trend.

The tools change every 5 years. The problems donâ€™t. I hope you enjoy this. What's your lesson learned if you are in the field for a while?",1768301385.0,55,2,https://www.reddit.com/r/dataengineering/comments/1qbo9zn/a_diary_of_a_data_engineer/
1qbnziv,MySQL Metadata Locks,"A long-running transaction holding a metadata lock forever has the capability to bring down your entire application.
A real-world scenario: you submit a DDL while a transaction is holding a metadata lock, and hundreds of concurrent queries are fired against the same table. The database comes under a very high load. The load remains high until the transaction rollbacks or commits. Under very high load, the server does nothing meaningful, just keeps context switching, a.k.a thrashing. 
This blog shows how to detect and mitigate this scenario.",1768300336.0,3,0,https://www.reddit.com/r/dataengineering/comments/1qbnziv/mysql_metadata_locks/
1qbnr9h,Am I making a mistake building on motherduck?,"I'm the cofounder of an early stage startup. Our work is 100% about data, but I don't have huge datasets either, you can think of it as running pricing algorithms for small hotels. So we delve into booking data, pricing data and so on. So about 400k rows per year per client. we have about 10 clients so far. 

  
I've been a huge fan of duckdb for a long time, been to duckdb events. I love motherduck, it's very sleek, it works, I haven't seen a bug so far (and been using it for a year!). It's alright in terms of pricing. 

  
Currently our pattern is basically DLT to GCS, GCS to motherduck, DBT from motherduck to motherduck. Right now, the only reason I use motherduck is that I love it. I don't know how to explain it, but everything \*\*\*\*\* works. 

  
Am I making a mistake by having two cloud providers like this? Will this bite me because in the end motherduck will probably never have as many tools as GCP and if we want to scale fast, I will probably start saying i.e. oh well i can't do ML on motherduck so I'll put that in bigquery now? Curious to hear your opinoin on this. ",1768299495.0,25,37,https://www.reddit.com/r/dataengineering/comments/1qbnr9h/am_i_making_a_mistake_building_on_motherduck/
1qbkcbp,Getting Started in Data Engineering,"Hey everyone ,
I have been a Data analyst for quite a while but I am planning to shift to Data Engineering Domain.

I need to start prepping for the same. Core concepts , terminologies and other important parts.
So can you guys suggest some books which are well known and highly recommended for the above scenario to get started.
Please do let me know.
Thanks",1768286616.0,23,14,https://www.reddit.com/r/dataengineering/comments/1qbkcbp/getting_started_in_data_engineering/
1qbilj6,Is my storage method effective?,"Hi all,

Iâ€™m very new to data engineering as a whole, but I have a basic idea of how I want to lay out my data to minimise storage costs as much as possible, as Iâ€™ll be storing historical data for a factoryâ€™s efficiency.

Basically, Iâ€™m receiving a large CSV file every 10 minutes containing name, data, quality, data type, etc. To save space, I was planning to split the data into two tables: one for unchanging data (such as name and data type) and another for changing data, as strings take up more storage.

My basic approach was going to be:  
**CSV â†’ SQL landing table â†’ unchanging & changing data tables**

Weâ€™re not yet sure how we want to utilise the data, but I essentially need to pull in and store the data before we can start testing and exploring use cases.

The data comes into the landing table, we take a snapshot of it, send it to the corresponding tables, and then delete only the snapshot data from the landing table. This reduces the risk of data being lost during processing.

The changing data would be stored in a new table every month, and once that data is around five years old it would be deleted (or handled in a similar way).

I know this sounds fairly simple, but there will be thousands of data entries in the CSV files every 10 minutes.

Do you have any tips or advice? Is it a bad idea to split the unchanging string data into a separate table to save space? Once I know how the business actually wants to use the data, Iâ€™ll be back to ask about the best way to really wow them.

Thanks in advance.",1768280846.0,4,7,https://www.reddit.com/r/dataengineering/comments/1qbilj6/is_my_storage_method_effective/
1qbg6yd,The ACID Test: Why We Think Search Needs Transactions,,1768273944.0,0,1,https://www.reddit.com/r/dataengineering/comments/1qbg6yd/the_acid_test_why_we_think_search_needs/
1qbd771,Best way to run dbt with Airflow for a beginner team,"Hi. My team is getting started deploying airflow for the first time and we want to use dbt for our transformations.  One topic of debate we have is whether or not we should use the DockerOperator/KubernetesPodOperator to run dbt or if to run it with something like the BashOperator.  Iâ€™m trying to strike the right balance of flexibility without the setup being too overly complex. Therefore I wanted to ask if anyone had any advice on which route we should try and why.

For context we with deploy Airflow on AKS using the CeleryExecutor. We also plan to use dlthub for ingestion.

Thanks in advance for any advice anyone can give.",1768266031.0,9,8,https://www.reddit.com/r/dataengineering/comments/1qbd771/best_way_to_run_dbt_with_airflow_for_a_beginner/
1qbaolw,Jobs To Work While In School For Computer Science,"Iâ€™m currently pursuing my A.A to transfer into a BS in Computer Science w/ Software Development concentration. My original plan was to complete an A.S in Computer Information Technology w/certs to enter into an entry level position in Data science but was told I couldnt transfer an A.S to a university. Iâ€™m stuck now, not knowing what I can do in the mean time. I wanna be on a Data Scientist, Data Analyst or Data Administrator track,can someone give me some advice? ",1768259804.0,0,3,https://www.reddit.com/r/dataengineering/comments/1qbaolw/jobs_to_work_while_in_school_for_computer_science/
1qb7d5g,3 years Data engineer in public sector struggling to break into Gaming. Any advice?,"Iâ€™ve been working as a Data Engineer for 3 years, mostly in Azure. I build ETL pipelines, orchestrate data with Synapse (and recently Fabric), and work with stakeholders to create end-to-end analytics solutions. My experience includes Python, SQL, data modeling, and building a full datawarehouse/dataplatform from multiple source systems including API's Mostly around customer experience, products, finance and contractors/services.

Right now Iâ€™m in the public sector/non-profit space, but I really want to move into gaming. Iâ€™ve been applying to roles, and Iâ€™ve been custom-tailoring my CV for each one trying to  highlight similar tech, workflows, and the kinds of data projects Iâ€™ve done specifically relating to the job spec  but Iâ€™m not getting any shortlists.

Is it just that crowded? I sometimes struggle to hear back even if it's a company in my sector. am I missing something? need advice

Edit: I do mean data engineering for a games company ",1768252224.0,14,10,https://www.reddit.com/r/dataengineering/comments/1qb7d5g/3_years_data_engineer_in_public_sector_struggling/
1qb78rd,Hiring perspective needed: survey-heavy analytics experience,"Hi everyone.

looking for a bit of advice from people in the UK scene.

Iâ€™ve been working as an analytics engineer at a small company, mostly on survey data collected by NGOs and local bodies in parts of Asia (KoBo/ODK-style submissions).

Stack: SQL, Snowflake, dbt, AWS, Airflow & Python. Tableau for dashboards.

Most of the work was taking messy survey data, cleaning it up, building facts/dims + marts, adding dbt tests, and dealing with stuff like PII handling and data quality issues.

Our marts were also used by governments to build their yearly reports.

Is that kind of background seen as â€œtoo nicheâ€, or do teams mostly care about the fundamentals (modelling, testing, data quality, governance, pipelines)?

Would love to hear how people see it / any tips on positioning.

Thank you.",1768251943.0,8,1,https://www.reddit.com/r/dataengineering/comments/1qb78rd/hiring_perspective_needed_surveyheavy_analytics/
1qb728v,Reviews on Data Engineer Academy?,Work in data already - but Iâ€™m the least technical person in my department.  I understand the 3000 ft up perspective of our full stack -  and am considered a senior leader.  I need to up skill - particularly in SQL and get more comfortable in our tools (dbt & snowflake primarily).  Iâ€™ve been getting ads from this company and Iâ€™m curious about others experiences ,1768251544.0,7,5,https://www.reddit.com/r/dataengineering/comments/1qb728v/reviews_on_data_engineer_academy/
1qb3twi,Databricks compute benchmark report!,"We ran the full TPC-DS benchmark suite across Databricks Jobs Classic, Jobs Serverless, and serverless DBSQL to quantify latency, throughput, scalability and cost-efficiency under controlled realistic workloads. 

Here are the results: [https://www.capitalone.com/software/blog/databricks-benchmarks-classic-jobs-serverless-jobs-dbsql-comparison/?utm\_campaign=dbxnenchmark&utm\_source=reddit&utm\_medium=social-organic](https://www.capitalone.com/software/blog/databricks-benchmarks-classic-jobs-serverless-jobs-dbsql-comparison/?utm_campaign=dbxnenchmark&utm_source=reddit&utm_medium=social-organic)Â ",1768244506.0,23,4,https://www.reddit.com/r/dataengineering/comments/1qb3twi/databricks_compute_benchmark_report/
1qb0ty1,Web based Postgres Client | Looking for some feedback,"I've been building a Postgres database manager that is absolutely stuffed with features including:

* ER diagram & schema navigator
* Relationship explorer
* Database data quality auditing
* Simple dashboard
* Table skills (pivot table detection etc...)
* Smart data previews (URL, geo, colours etc...)

I really think I've built possibly the best user experience in terms of navigating and getting the most out of your tables.

Right now the app is completely standalone, it just stores everything in local storage. Would love to get some feedback on it. I haven't even given it a proper domain or name yet!

Let me know what you think:  
[https://schema-two.vercel.app/](https://schema-two.vercel.app/)",1768238158.0,2,4,https://www.reddit.com/r/dataengineering/comments/1qb0ty1/web_based_postgres_client_looking_for_some/
1qb0ba6,Forecast Help - Bank Analysis,"Iâ€™m working on a small project where Iâ€™m trying to forecast RBCâ€™s or TD's (Canadian Banks) quarterly Provision for Credit Losses (PCL) using only public data like unemployment, GDP growth, and past PCL.

Right now Iâ€™m using a simple regression that looks at:

* current unemployment
* current GDP growth
* last quarterâ€™s PCL

to predict this quarterâ€™s PCL. It runs and gives me a number, but Iâ€™m not confident itâ€™s actually modeling the right thing...

If anyone has seen examples of people forecasting bank credit losses, loan loss provisions, or allowances using public macro data, Iâ€™d love to look at them. Iâ€™m mostly trying to understand what a sensible structure looks like.",1768237056.0,5,2,https://www.reddit.com/r/dataengineering/comments/1qb0ba6/forecast_help_bank_analysis/
1qavf3s,Data Tech Insights 01-09-2026,"Ataira just published a new Data Tech Insights breakdown covering major shifts across healthcare, finance, and government.  
Highlights include:  
â€¢ Identity governance emerging as the top hidden cost driver in healthcare incidents  
â€¢ AI governance treated like thirdâ€‘party risk in financial services  
â€¢ Fraud detection modernization driven by deepfakeâ€‘enabled scams  
â€¢ FedRAMP acceleration and KEVâ€‘driven patching reshaping government cloud operations  
â€¢ Crossâ€‘industry push toward standardized evidence, observability, and reproducibility

Full analysis:  
[https://www.ataira.com/SinglePost/2026/01/09/Data-Tech-Insights-01-09-2026](https://www.ataira.com/SinglePost/2026/01/09/Data-Tech-Insights-01-09-2026)

Would love to hear how others are seeing these trends play out in their orgs.",1768225904.0,1,1,https://www.reddit.com/r/dataengineering/comments/1qavf3s/data_tech_insights_01092026/
1qaufq6,Salary negotiation,"What do you think is the best I could ask for the first switch?

I faced a situation where I asked for a 100% hike, and the HR representative arrogantly responded, ""Why do you need 100%? We can't give you that much."" He had an attitude of ""take it or leave it."" Is it their strategy to round me in low pay?

How should I respond in this situation? What  mindset shd I have while negotiating salary?

FYI, I'm de with 2.6yoe and currently earn 8.5, and my expectation is 16 .",1768223347.0,0,10,https://www.reddit.com/r/dataengineering/comments/1qaufq6/salary_negotiation/
1qau2wr,Seeking advice for top product based company,"Hi reddit,

I want work on top product based company as data engineer.

What's your suggestion to achieve this???",1768222362.0,0,3,https://www.reddit.com/r/dataengineering/comments/1qau2wr/seeking_advice_for_top_product_based_company/
1qasjr3,Need architecture advice: Secure SaaS (dbt + MotherDuck + Hubspot),"Happy Monday folks!

**Context**Â I'm building a B2B SaaS in a side project for brokers in the insurance industry. Data isolation is criticalâ€”I am worried to load data to the wrong CRM tool (using Hubspot)

**Stack**: dbt Core + MotherDuck (DuckDB).

    API â†’ dlt â†’ MotherDuck (Bronze) â†’ dbt â†’ Silver â†’ Gold â†’ Python script â†’ HubSpot
    Orchestration for the beginning with Cloud Run (GCP) and Workflows

**The Challenge**Â My head is spinning and spinning and I don't get closer to a satisfying solution. AI proposed some ideas, which were not making me happy. Currently, I will have a test run with one broker and scalability is not a concern as of now, but (hopefully) further down the road.

I am wondering how to structure a Multi-Tenancy setup, if I scale to 100+ clients. Currently I  use strict isolation, but I'm worried about managing hundreds of schemas.

**Option A: Schema-per-Tenant (Current Approach)**Â Every client gets their own set of schemas:Â `raw_clientA`,Â `staging_clientA`,Â `mart_clientA`.

* âœ…Â **Pros**: ""Gold Standard"" Security. Permissions are set at the Schema level. Impossible to leak data via a missedÂ `WHERE`Â clause. easy logic forÂ `dbt run --select tag:clientA`.
* âŒÂ **Cons**:Â **Schema Sprawl**. 100 clients = 400 schemas. The database catalog looks terrifying.

**Option B: Pooled (Columnar)**Â All clients share one table with aÂ `tenant_id`Â column:Â `staging.contacts`.

* âœ…Â **Pros**: Clean. Only 4 schemas total (`raw`,Â `stage`,Â `int`,Â `mart`). Easy global analytics.
* âŒÂ **Cons**:Â **High Risk**. Permissions are hard (Row-Level Security is complex/expensive to manage perfectly). One missedÂ `WHERE tenant_id = ...`Â in a join could leak competitor data. Also incremental load seems much more difficult and the source data comes from the same API, but using different client credentials

**Option C: Table-per-Client**Â One schema per layer, but distinct tables:Â `staging.clientA_contacts`,Â `staging.clientB_contacts`.

* âœ…Â **Pros**: Fewer schemas than Option A, more isolation than Option B.
* âŒÂ **Cons**:Â **RBAC Nightmare**. You can't justÂ `GRANT USAGE ON SCHEMA`. You have to script permissions for thousands of individual tables. Visual clutter in the IDE is worse than folders.

**The Question**Â Is ""Schema Sprawl"" (Option A) actually a problem in modern warehouses (specifically DuckDB/MotherDuck)? Or is sticking with hundreds of schemas the correct price to pay for sleep-at-night security in a regulated industry?

Hoping for some advice and getting rid of my headache!",1768217560.0,2,13,https://www.reddit.com/r/dataengineering/comments/1qasjr3/need_architecture_advice_secure_saas_dbt/
1qasd31,What Developers Need to Know About Apache Spark 4.1,"In the middle of December 2025 Apache Spark 4.1 was released, it builds upon what we have seen in [Spark 4.0](https://medium.com/@cralle/what-developers-need-to-know-about-apache-spark-4-0-508d0e4a5370?sk=2a635c3e28a7aa90c655d0a2da421725), and comes with a focus on lower-latency streaming, faster PySpark, and more capable SQL.",1768216932.0,8,3,https://www.reddit.com/r/dataengineering/comments/1qasd31/what_developers_need_to_know_about_apache_spark_41/
1qaraba,Being honest: A foolish mistake in data engineering assessment round i did?,"Recently I've been shortlisted for assessment round for one of the company. It was 4 hrs test including advance level sql question and basic pyspark question and few MCQ.

I refrain myself from taking AI's help to be honest and test my knowledge but I think this was mistake in current era...
I solved Pyspark passing all test cases and also the advance SQL by own logic upto 90% correct since descripencies in one scenario row output... But still got REJECTED....

I think being too honest is not an option if want to get hired no matter how knowledgeable or honest you're...",1768213147.0,14,21,https://www.reddit.com/r/dataengineering/comments/1qaraba/being_honest_a_foolish_mistake_in_data/
1qaoqlz,Caught the candidate using AI for screening,"Guy was not able to explain facts and dimensions in theory but said he know in practical when asked him to write code for trimming the values he wrote regular expression immediately, even daily users do not remember syntax easily. When asked him to explain each letter of expression he started choking said he remembered it as it is because he used it earlier . Nowadays its very tough to find genuine working people because these kind of people mess up the project pretty badly ",1768203409.0,301,96,https://www.reddit.com/r/dataengineering/comments/1qaoqlz/caught_the_candidate_using_ai_for_screening/
1qamtsj,Automating ML pipelines with Airflow (DockerOperator vs mounted project),"Note: I already posted the same content in the MLOps sub. But no response from there. So posting here for some response.

Hello everyone,

Im a data scientist with 1.6 years of experience. I have worked on credit risk modeling, sql, powerbi, and airflow.

Im currently trying to understand end-to-end ML pipelines, so I started building projects using a feature store (Feast), MLflow, model monitoring with EvidentlyAI, FastAPI, Docker, MinIO, and Airflow.

Im working on a personal project where I fetch data using yfinance, create features, store them in Feast, train a model, model version ing using mlflow, implement a championâ€“challenger setup, expose the model through a fastAPI endpoint, and monitor it using evidentlyAI.

Everything is working fine up to this stage.

Now my question is: how do I automate this pipeline using airflow?

1. Should I containerize the entire project first and then use the dockeroperator in airflow to automate it?

2. Should I mount the project folder in airflow and automate it that way?

I have seen some youtube videos. But they put everything in a script and automate it. I believe it won't work in real projects with complex folder structures.

Please correct me if im wrong.",1768196748.0,6,7,https://www.reddit.com/r/dataengineering/comments/1qamtsj/automating_ml_pipelines_with_airflow/
1qaivmq,Live data sports ticker,"Currently working on building a live sports data ticker, pulling NBA data + betting odds, pushing real-time updates. 

Currently, pushing to Github, pulling from GitHub with an AWS EC2 instance and pushing to MQTT on AWS IOT 

I am working to change my monolithic code to micro services running GO/better logging/reducing api hits.

Eventually this will push to Raspberry Piâ€“powered LED boards over Wi-Fi/MQTT. This is currently pushing to a virtual display board, for easier trouble shooting. 

(I do have working versions of NFL/MLB but focusing on perfecting one sport right now)
 ",1768185185.0,2,0,https://www.reddit.com/r/dataengineering/comments/1qaivmq/live_data_sports_ticker/
1qahyip,Best Bronze Table Pattern for Hourly Rolling-Window CSVs with No CDC?,"Hi everyone, I'm running into bit of dilemma with this bronze level table that I'm trying to construct and need some advice. 

The data for the table is sent hourly by the vendor 16 times in the day as a CSV that has transaction data in a 120 day rolling window. This means each file is about 33k rows by 233 columns, around 50 MB. There is no last modified timestamp, and they overwrite the file with each send. The data is basically a report they run on their DMS with a flexible date range, so occasionally we request a history file so they send us one big file per store that goes across several years.

The data itself changes state for about 30 days or so before remaining static, so that means that roughly 3/4s of the data may not be changing from file to file (though there might be outliers).

So far I've been saving each file sent in my Azure Data Lake and included the timestamp of the file in the filename. I've been doing this since about April and have accumulated around 3k files.

Now I'm looking to start loading this data into Databricks and I'm not sure what's the best approach to load the bronze layer between several approaches I've researched.

Option A: The bronze/source table should be append-only so that every file that comes in gets appended. However, this would mean I'd be appending 500kish rows a day, and 192m a year which seems really wasteful considering a lot of the rows would be duplicates.

Option B: the bronze table should reflect the vendors table at the current state, so each file should be upserted into the bronze table - existing rows are updated, new rows inserted. The criticisms I've seen of this approach is that it's really inefficient, and this type of incremental loading is best suited for the silver/warehouse layer.

Option C: Doing an append only step, then another step that dedupes the table based on a row hash after a load. So I'd load everything in, then keep only the records that have changed based on business rules.

For what it's worth, I'm hoping to orchestrate all of this through Dagster and then using DBT for downstream transformations.

Does one option make more sense than the others, or is there another approach I'm missing? ",1768182682.0,9,17,https://www.reddit.com/r/dataengineering/comments/1qahyip/best_bronze_table_pattern_for_hourly/
1qafyy3,"How to transform million rows of data where each row can range from 400 words to 100,000+ words, to Q&A pair which can challenge reasoning and intelligence on AWS cheap and fast (Its for AI)?","I have a dataset with **\~1 million rows**.  
Each row contains **very long text**, anywhere from **400 words to 100,000+ words**.

My goal is to **convert this raw text into high-quality Q&A pairs** that:

* Challenge **reasoning and intelligence**
* Can be used for **training or evaluation**

Thinking of using **large models like LLaMA-3 70B to generate Q&A from raw data**

I explored:

* **SageMaker inference** â†’ too slow and very expensive
* **Amazon Bedrock batch inference** â†’ limited to \~8k tokens

**I tried to dicuss with ChatGPT / other AI tools** â†’ no concrete scalable solution

My **budget is \~$7kâ€“8k (or less if possible)**, and I need something scalable and practical.",1768177488.0,2,7,https://www.reddit.com/r/dataengineering/comments/1qafyy3/how_to_transform_million_rows_of_data_where_each/
1qaamfb,Low retention of bronze layer record versions and lineage divergence,"In the bronze layer, our business is ok (and desires) the clean up of older versions of records.  In fact, we can't guarantee that we'll be able to keep this history forever.  

We'll always keep active records and can always re-build bronze with active records.  

However, we do have gold level data and aggregate fact table, and it's possible that some of the records in gold could be from a snapshot in time.  

Let's say there are 3 records in a gold fact that summarize a total.  
Record 1: ID=1, ver=5 ,Amount=$100  
Record 2: ID=2, ver=5, Amount=$100  
Record 3: ID=3, ver=3, Amount=$50

There will be a point in time where this gold fact will persist and not be updated even if Record with ID=1 has a change in the amount in bronze layer.  This is by design and is a business requirement.

Eventually in bronze, record with ID=1 changes to ver=6 and Amount now=$110.

This time, we don't want to update the gold fact for this scenario, so it remains as ver=5.

Eventually, in bronze, due to retention, we lose the bronze record for ver=5, but we still keep v=6.  The gold still has a record of what the value was at the time and a record that it's based on the record being v=5.

The business is fine with it; and in fact they prefer it.  The like the idea of being able to access the specific version in bronze as it was at the time, but if it's lost due to retention then they are ok with that because they will just trust the number in the gold fact table; they'll know why it doesn't match source by comparing the version value.  

As a data expert, I struggle with it.  

We lose row-version lineage back to bronze, but the business is ok with that risk.  

As data engineers, how do you feel about this scenario?  We can compromise on the implementation, and I believe we are still ensuring trust of the data in other ways (for their needs) by keeping the copy of the record (what the value was at the time) in gold for the purposes of financial review and analysis.  

Thoughts? Anything else you'd consider?",1768164543.0,3,4,https://www.reddit.com/r/dataengineering/comments/1qaamfb/low_retention_of_bronze_layer_record_versions_and/
1qa5wt5,How much time really it will take to prepare for data engineering?,I'm working in kind of support role basically in fusion side. I want to get into data engineering field how much time really it will really take?,1768153935.0,0,3,https://www.reddit.com/r/dataengineering/comments/1qa5wt5/how_much_time_really_it_will_take_to_prepare_for/
1qa55gv,Any good video tutorial/demo on YouTube that demonstrates solid DE pipelines?,I wonder if there is solid demo of how to build DE pipelines so that those who are just starting could watch and get the grasp of what is the DE anyway? ,1768152243.0,5,1,https://www.reddit.com/r/dataengineering/comments/1qa55gv/any_good_video_tutorialdemo_on_youtube_that/
1qa2h7b,Would Going From Data Engineer to Data Analyst be Career Sxicide?,"Ive been a data engineer for about 8 years and am on the market for Senior DE positions. 

I recently have been interviewing for a Senior Security Data Analyst Position at a cybersecurity company. The position is python heavy and mostly focuses on parsing large complex datasets from varying sources. I think its mostly done in notebooks and pipelines are one off, non-reoccurring. The pay would be a small bump from 140k to maybe 160-170k plus bonus and options. 

The main reason Im considering this is because I find cybersecurity fascinating. It also seems like a better market overall. Should I take a position like this or am I better off staying as a strict data engineer? Should i try and negotiate title so it doesnt have the word analyst in it?",1768146109.0,7,11,https://www.reddit.com/r/dataengineering/comments/1qa2h7b/would_going_from_data_engineer_to_data_analyst_be/
1qa14ef,Data engineer job preparation,"Hi All,

As per header I am currently preparing for data engineer for 5+ years. If anyone is doing the same we can connect and help each other with feedback and suggestions to improve. Tech stack is sql, python, pyspark, gcp/AWS. If anyone have good knowledge in databricks to please help in paid training that will be helpful. Please DM if anyone interested to connect.",1768142831.0,2,2,https://www.reddit.com/r/dataengineering/comments/1qa14ef/data_engineer_job_preparation/
1qa0j9s,Datbricks beginner project,"I just completed this project which simulates pos for a coffeshop chain and streams the realtime data with eventhub and processes it in the Databricks with medallion architecture .

Could you please provide helpful feedback?",1768141331.0,1,0,https://www.reddit.com/r/dataengineering/comments/1qa0j9s/datbricks_beginner_project/
1qa0923,Inside Data Engineering with Hasan Geren,"Hello folks,

Hope everyone is doing well. I am sharing my latest article from the Inside Data Engineering series, covering the below topics for the new DEs out there.

* Practical insightsÂ â€“ Get a clear view of what data engineers do in their day-to-day work.
* Emerging trendsÂ â€“ Stay informed about new technologies and evolving best practices.
* Real-world challengesÂ â€“ Understand the obstacles data engineers face and how they overcome them.
* Myth-bustingÂ â€“ Uncover common misconceptions about data engineering and its true impact.

Let me know if this is helpful.

  
Please suggest and be a part of the series if you like. 



Thanks

Junaid",1768140597.0,0,0,https://www.reddit.com/r/dataengineering/comments/1qa0923/inside_data_engineering_with_hasan_geren/
1q9ylqb,Polars vs Spark for cheap single-node Delta Lake pipelines - safe to rely on Polars long-term?,"Hi all,

Iâ€™m building ETL pipelines in Microsoft Fabric with Delta Lake tables. The organizations's data volumes are small - I only need single-node compute, not distributed Spark clusters.

**Polars** looks perfect for this scenario. I've heard a lot of good feedback about Polars. But Iâ€™ve also heard some warnings that it might move behind a paywall (Polars Cloud) and the open-source project might end up abandoned/not being maintained in the future.

**Spark** is said to have more committed backing from big sponsors, and doesn't have the same risk of being abandoned. But it's heavier than what I need.

If I use Polars now, am I potentially just building up technical debt? Or is it reasonable to trust it for production long-term? Would sticking with Spark - even though I donâ€™t need multi-node - be a more reasonable choice?

Iâ€™m not very experienced and would love to hear what more experienced people think. Appreciate your thoughts and inputs!",1768135880.0,54,35,https://www.reddit.com/r/dataengineering/comments/1q9ylqb/polars_vs_spark_for_cheap_singlenode_delta_lake/
1q9v7cu,Job Switch,"Hi , I am 23 M from India. I work at a reputed service based company as a data engineer. It says data engineer all I do is migrated db from legacy systems to snowflake. I haven't got any hands on experience on the core data engineering. The salary and leave policies are crap. I have 1.5 yr of experience. How do I switch? With the new gen ai how should I update my skills ? Please help me ",1768124002.0,0,3,https://www.reddit.com/r/dataengineering/comments/1q9v7cu/job_switch/
1q9r69d,How do you handle realistic demo data for SaaS analytics?,"Whenever Iâ€™m working on a new SaaS project, I hit the same problem once analytics comes into play: demo data looks obviously fake.

Growth curves are too perfect, thereâ€™s no real churn behavior, no failed payments, and lifecycle transitions donâ€™t feel realistic at all.

I got some demo datasets from a friend recently, but they had the same issue, everything looked clean and smooth, with none of the messy stuff that shows up in real products.

Churn, failed payments, upgrades/downgrades, early vs mature behaviorâ€¦ those details matter once you start building dashboards.

Would love to hear whatâ€™s actually worked in real projects.

",1768109841.0,4,4,https://www.reddit.com/r/dataengineering/comments/1q9r69d/how_do_you_handle_realistic_demo_data_for_saas/
1q9gsng,Porfolio worthy projects?,"Hi all! I'm a junior data engineer interested in DE / BE. Iâ€™m trying to decide which (if any) of these projects to showcase on my CV/portfolio/LinkedIn, and Iâ€™d love if you could take a very quick look and give me some feedback on which are likely to strengthen vs hurt my CV/portfolio. 

[**data-tech-stats**](https://github.com/simon-milata/data-tech-stats) ([Live Demo](https://dts.simonmilata.com/))  
My latest project which I'm still finishing up. The point of this project was to actually deploy something real while keeping the costs close to 0 and to design an actual API. The DE part is getting the data from GitHub, storing it in S3, aggregating and then visualizing it. My worry is that this project might be a bit too simple and generic. 

[**Study-Time-Tracker-Python**](https://github.com/simon-milata/Study-Time-Tracker-Python)  
This was my first actual project and the first time using git so the code quality and git usage weren't great. It's also not at all something I would do at work and might seem too amateurish. I do think it's pretty cool tho as it looks pretty nice, is unique and even has a few stars and forks which I think is pretty rare.

[**TkinterOS**](https://github.com/simon-milata/TkinterOS)  
This was my second project which I made cause I saw GodotOS and thought it would be cool  to try to recreate it using Tkinter. It includes a bunch of games and an (unfinished) file system. It also has a few stars but the code quality still is still bad. Very unrelated to work too.  


I know this might feel out of place being posted on a DE sub but these are the only presentable projects I have so far and I'm mostly interested in DE. These projects were mostly made to practice python and make stuff.  
  
For my next project I'm planning on learning PySpark and trying Redshift / Databricks. My biggest issue is that I feel like the difficulty of DE is the the scale of the data and regulations which is very hard / very expensive to recreate. I also don't really want to make simple projects which just transform some fake data once.  


Sorry for the blocks of text I have no idea how to write reddit posts. Thank you for taking the time to read this. :)",1768081804.0,15,6,https://www.reddit.com/r/dataengineering/comments/1q9gsng/porfolio_worthy_projects/
1q9gczg,How to analyze and optimize big and complex Spark execution plans?,"Hey All,

I am working with advertising traffic data and the volume is quite huge for the processing period of a month,before creating final table I do some transformationw which mostly consists of some joins and a union operation.

The job is running for 30 minutes, so when I checked the DAG plan to find any obvious gotchas, I was facing a a complex DAG (with AQE enabled). 

I am not sure on how to approach optimizing this SQL snippet, challenge is that some of tables which I am using in joins are actually neater views themselves, so the thing becomes quite large.

Here are the options I have come up so far:
1. Materialising the nested view as it is used across multiple places,  I am not sure if Spark caches the result for reuse or if it recomputes is every time but couldn't hurt to have a table?

2. Try to find stages with the largest time and see if I can pinpoint the issue, I am not sure if the stage will provide enough hints to identify the offending logic any tips on what to look for in stages? The stage plan are not always obvious (to me) on which join is getting executed, only see whole stage code gen task if I double click on the stage

",1768080767.0,4,4,https://www.reddit.com/r/dataengineering/comments/1q9gczg/how_to_analyze_and_optimize_big_and_complex_spark/
1q9f6p1,Looking for advice from folks whoâ€™ve run large-scale CDC pipelines into Snowflake,"Weâ€™re in the middle of replacing a streaming CDC platform thatâ€™s being sunset. Today it handles CDC from a very large multi-tenant Aurora MySQL setup into Snowflake. 

* Several thousand tenant databases (like 10k+ - don't know exact #) spread across multiple Aurora clusters
* Hundreds of schemas/tables per cluster
* CDC â†’ Kafka â†’ stream processing â†’ tenant-level merges â†’ Snowflake
* fragile merge logic thatâ€™s to debug and recover when things go wrong

Weâ€™re weighing: Build: MSK + Snowpipe + our own transformations or buying a platform from a vendor

Would love to understand from people that have been here a few things

* Hidden cost of Kafka + CDC at scale? Anything i need to anticipate that i'm not thinking about?
* Observability strategy when you had a similar setpu
* Anyone successfully future proofed for fan-out (vector DBs, ClickHouse, etc.) or decoupled storage from compute (S3/Iceberg) 
* If you used a managed solution, what did you use? trying to stay away from 5t. Pls no vendor pitches either unless you're a genuine customer thats used the product before 

Any thoughts or advice?",1768078087.0,8,12,https://www.reddit.com/r/dataengineering/comments/1q9f6p1/looking_for_advice_from_folks_whove_run/
1q9df31,how to get a job with 6 YOE and 9 month gap?,"I have 6 yoe in date engineering but have 9 month gap due to health complications (now resolved). 

Should i raise attention/address the 9 month gap while applying?   
Currently just applying without addressing it at all (not sure if this it the best way to go about this..)  
  
How should i go about this to maximize my chances of getting a new DE job? 

Appreciate any advice. Thanks.",1768073980.0,6,8,https://www.reddit.com/r/dataengineering/comments/1q9df31/how_to_get_a_job_with_6_yoe_and_9_month_gap/
1q9c1a5,PySpark Users what is the typical Dataset size you work on ?,"My current experience is with BigQuery, Airflow and SQL only based transformations. Normally big query takes care of all the compute, shuffle etc and I just focus on writing proper SQL queries along with Airflow DAGs. This also works because we have the bronze and gold layer setup in BigQuery Storage itself and BigQuery works good for our analytical workloads. 



I have been learning Spark on the side with local clusters and was wondering what is typical data size Pyspark is used to handle ? How many DE here actually use Pyspark vs simply modes of ETL.   
  
Trying to understand when a setup like Pyspark is helpful ? What is typical dataset size you guys work etc. 

Any production level insights/discussion would be helpful. ",1768070842.0,37,23,https://www.reddit.com/r/dataengineering/comments/1q9c1a5/pyspark_users_what_is_the_typical_dataset_size/
1q99ffn,would this help you guys if i built this?,"Im a teen CS student and I've worked among data analysts and under them. Pushing back on deadlines can be tough sometimes and keeping track of all the changes adds up to hours of work and can be hard to organize. I know Jira boards exist but what if I built a project management software (thinking like web app) that implements version tracking for recurrent client dashboards, easy client onboarding, and change logging, which directly addresses issues, such as tracing changes, avoiding repeated exports through better versioning, and organizing client-specific workflows. It could reduce manual re-exports by providing a centralized hub for revisions, approvals, and history, potentially integrating with tools like Power BI for automation.

I know this is not the root of the problem, but do you think that a tool like this could at least save you some time and annoyance by having version control and cross function visibility for dashboards, allowing you to organize tasks, push back on deadlines, and gain approval all on one platform. I could also add features to allow for easy onboard of new recurring clients etc. Let me know .",1768064827.0,0,6,https://www.reddit.com/r/dataengineering/comments/1q99ffn/would_this_help_you_guys_if_i_built_this/
1q97wup,Need help picking a DE course on Coursera. Deeplearning.ai or IBM?,"I've been looking for a course that'll give me a good start with example labs and projects in data engineering. In my country most job postings require Google or AWS cloud, and Deeplearning.ai's course series has a partnership with AWS. On the other hand, IBM's DE course series seem to be more popular.

Have any of yall tried it?

I also signed up for Zoomcamp, so I'll take a look at how that goes. ",1768061286.0,6,4,https://www.reddit.com/r/dataengineering/comments/1q97wup/need_help_picking_a_de_course_on_coursera/
1q95bfj,What's the purpose of live data?,"Unless you are displaying the heart rate or blood pressure of a patient in an ICU, what really is the purpose of a live dashboard.",1768054986.0,51,62,https://www.reddit.com/r/dataengineering/comments/1q95bfj/whats_the_purpose_of_live_data/
1q933el,Osmos io alternative needed,"A client project has come to our agency where they were using osmos.io but after Microsoft acquisition, they have received a notice that Osmos is shutting on Feb 28th. We need to figure out the transition as they are not already on Fabric.

These things are needed:

- File native ingestion that handles CSV drops from S3
- Inflight transofrmation
- Lowcode mapping

Any competitors with similar offerings? Or do we just migrate them to Fabric? I dont think that is a good idea in any case.",1768048875.0,13,7,https://www.reddit.com/r/dataengineering/comments/1q933el/osmos_io_alternative_needed/
1q92eti,Need support / Help from the communnity,"Guys, I want your support. I am working as a Data Engineer intern in a startup company. (Fresher though). Note : There is no one in my team currently ( I am only one in the Data Team ) I have a setup a data pipeline in our company. Tools used:

1. Airflow

Snowflake (US region)PowerBI.Bigquery Flow: Raw Mobile App events (From Firebase) -> Bigquery -> Snowflake -> PowerBI (Entire pipeline is orchestrated by Airflow). All the transformations for creating One big table, Fct tables, aggregate tables will be done by Snowflake and stored in Snowflake itself. The powerBi is connected to the Agg data. The visuals are created on top of the data loaded (via Import mode) using the DAX in powerBI. The thing is our mobile app and some backed data (which will be used for joining along with the Mobile event data) has users data which is region specific. App will have users from different regions. I don't know so much knowledge about the compliance and all. But our founder said that that each country data should be stored in particular region. To avoid the issues due to the compliance, I have make these things in a way. There is one person (He is working in another company Apple(like a friend of my founder), will suggest some things like a mentor but he has not have much time to interact with me ), suggested for the s3 + iceberg. But I have so many questions like :

1. Which tools to use ?

If I have to process the data, some compute engines like there (Snowflake, Presto, Trino) is there. Do we have to setup each instance per region for processing each region data ? Guys, If you have anything to help me, i am open to hear. If I failed to explain my scenario to you, sorry for that.",1768046621.0,7,3,https://www.reddit.com/r/dataengineering/comments/1q92eti/need_support_help_from_the_communnity/
1q9016g,"Rubber ducking a Bigquery, Airbtype, Looker strategy.","I'm piecing together a low cost data warehouse for a small-medium business. I'm basically the CTO and a generalist/developer/architect, and what I really lack in the data engineering  space is someone to bounce my ideas off.  
  
So please feel free to poke holes in any of this.

**Priorities**

* **High** \- Cost effective, low-code, avoid lock-in (eg OSS)
* **Low** \-  Performance, real-time

**Sources**

* Shopify Plus (customers, products, orders)
* GA
* Xero
* A subscription tool called SKIO with an AP**I**

**Why BigQuery**

I find it pretty cheap if you are judicious about what goes in it. So for example we have about 150k orders per year and I don't need line items because Shopify does that deeper analysis really well. BigQuery would eat that up out volume.

**Why Looker**

I think Looker is very cost effective and can plug in lots of stuff. For eg I can have Spreadsheets for some data and join it in Looker to BigQuery.

**Why AirByte**

A big part of this is deciding on an ETL. When I started we were using Celigo but found them to be pretty inflexible on billing. It's just a good example of lock-in I want to avoid.  
  
So I've been testing the Airbyte Shopify->BigQuery connector and it seems to do what i need. While it has the commercial solution, I feel confident I can switch to the community (open source) version later if I wanted to self host an ETL (which i think is a good long-term strategy in this day and age).

**Lock-in thoughts**

We are a Microsoft/Sharepoint place and while I don't dislike Azure I'm not a huge fan of Microsoft. We happened to already have Looker and GA because our Shopify vendor preferred it, and the business has never questioned this. So essentially I've put a Google Cloud strategy around these tools, and adding BigQuery is a bit of a no brainer. Obviously there is cloud lock-in to Google here, but in the massive event of Google dropping Looker/BigQuery I reckon we would survive as a business since most of our operational stuff is over in Shopify.

**Doubts**

The Shopify analytics platform is insanely good. If I could add some custom data to it I wouldn't be here writing this post. But I don't think that's in their business strategy to be a generic BI tool.

Unexpected costs are always a concern. Monitoring unexpected cloud costs, or regretting a SaaS product when you see ""Contact Sales for Pricing"" on some feature. I dunno if I can avoid that.

  


",1768038040.0,6,37,https://www.reddit.com/r/dataengineering/comments/1q9016g/rubber_ducking_a_bigquery_airbtype_looker_strategy/
1q8zg1d,Data Engineering Youtubers - How do they know so much?,"This question is self explanatory, some of the youtubers in the data engineering domain, e.g. Data with Baara, Codebasics, etc, keep pushing courses/tutorials on a lot of data engineering tech stacks (Snowflake, Databricks, Pyspark, etc) , while also working a full time job. I wonder How does one get to be an expert at so many technologies, while working a full time job? How many hours do these people have in a day?",1768035821.0,248,62,https://www.reddit.com/r/dataengineering/comments/1q8zg1d/data_engineering_youtubers_how_do_they_know_so/
1q8ye1d,Complete End to End Data Engineering Project | Pyspark | Databricks | Azure Data Factory | SQL,,1768031932.0,21,1,https://www.reddit.com/r/dataengineering/comments/1q8ye1d/complete_end_to_end_data_engineering_project/
1q8s3le,Recurrent dashboard deliveries with tedious format change requests are so fucking annoying . Anyone else deal with this ?,"Iâ€™m an analyst and my team is already pretty overloaded. On top of regular tickets, we keep getting recurring requests to make tiny formatting changes to monthly client dashboards. Stuff like colors, fonts, spacing, or fixing one number.

Our workflow is building in Power BI, exporting to PowerPoint, uploading the PPT to SharePoint, then saving a final PDF and uploading that to another folder for review. The problem is Power BI exports to PPT as images, so every small change means re-exporting the entire deck. One minor request can turn into multiple re-exports.

When this happens across a bunch of clients every month, it adds up to hours of wasted time. Is anyone else dealing with this? How are you handling recurring dashboards with constant formatting feedback, or automating this in a better way?",1768012272.0,6,4,https://www.reddit.com/r/dataengineering/comments/1q8s3le/recurrent_dashboard_deliveries_with_tedious/
1q8q2un,Coco Alemana â€“ Professional data editor that works with SQL and Amazon S3 natively,"Hi!

  
We've been building a tool that makes working with data extremely easy.

Specifically focused on the ad-hoc / last minute analytics segment, and data science (although using it for data engineering is totally possible too).

Some of the capability includes loading data from any source, cleaning, graphing it, writing raw SQL, exporting, etc.

A while back I posted on this forum about our ability to preview parquet directly from the OS... We've taken that and expanded it into this!



For those interested in the tech stack:

1. C++ for internal processing engine + DuckDB, Swift + AppKit for UI
2. Wrote our own custom caching engine, and SQL transpiler in C++. Transpiler takes DuckDB SQL and converts it into native SQL for full predicate pushdown (i.e. Athena, BigQuery, ClickHouse, Snowflake, Postgres, etc).
3. Wrote our own graphing library from scratch, all GPU native using custom shaders.
4. Wrote a custom Amazon S3 OS integration to replicate S3 buckets on Finder. Intercepts sys-level calls to reference remote data.
5. Super limited use of AI code. i.e. no vibe coding. Claude is high as a kite. Can't deal with that.

We've put a ton of effort into this, so hope you find it cool!

You can check it out here:Â [www.cocoalemana.com](https://www.cocoalemana.com)

Thanks :)

[](/submit/?source_id=t3_1q8074h)",1768006848.0,6,0,https://www.reddit.com/r/dataengineering/comments/1q8q2un/coco_alemana_professional_data_editor_that_works/
1q8pa8s,Data Engineering Academy,"Hello all, Iâ€™ve heard of this company called data engineering academy. I want to earn a data engineering role, but Iâ€™m not really sure on how/where to start. This company advertises its business model in a very enticing way: 20 guaranteed interviews, unlimited mock interviews, rework applications, course plan, and they apply to the jobs for you (showing you the list). However itâ€™s a relatively expensive investment. If it is worth it or you are/having to do the course Iâ€™d love to hear your experience. It does appear previous experiences about a year ago strongly discouraged taking it, however if changes were made to address previous issues then I see the value. I know that there are many other ways of getting into the field so alternatives are also extremely appreciated.",1768004784.0,5,22,https://www.reddit.com/r/dataengineering/comments/1q8pa8s/data_engineering_academy/
1q8ljtv,Fabric Data Lineage Dependency Visualizer,"Hi all,

Over the Christmas break, I migrated my lineage solution to a native Microsoft Fabric Workload. This move from a standalone tool to the Fabric Extensibility Toolkit provides a seamless experience for tracing T-SQL dependencies directly within your tenant.

The Technical Facts:

â€¢ Object-Level Depth: Traces dependencies across Tables, Views, and Stored Procedures (going deeper than standard Item-level lineage).

â€¢ Native Integration: Built on the Fabric Extensibility SDKâ€”integrated directly into your workspace.

â€¢ High-Perf UI: Interactive React/GraphQL graph engine for instant upstream/downstream impact analysis.

â€¢ In-Tenant Automation: Metadata extraction and sync are handled via Fabric Pipelines and Fabric SQL DB.

â€¢ Privacy: Data never leaves your tenant.

Open Source (MIT License):

The project is fully open-source. Feel free to use, fork, or contribute. Iâ€™ve evolved the predecessor into this native workload to provide a more robust tool for the community.

Greetings,

Christian",1767995624.0,3,0,https://www.reddit.com/r/dataengineering/comments/1q8ljtv/fabric_data_lineage_dependency_visualizer/
1q8grqg,Mock help?,"Hi all,
I have 10+ years of experience in data with 8 direct data engineering, including leading teams and build enterprise solutions.

My res is awesome and I get through three sets of recruiting screens a week. I somehow have failed like... 12? Iview with HM or tech screening so far and I havent gotten a lick of feedback.  Somehow I'm failing with my approach but with no error messages I have no clue what's going wrong. 

Is anyone willing to do a mock with me?",1767984668.0,3,9,https://www.reddit.com/r/dataengineering/comments/1q8grqg/mock_help/
1q8f0kg,Databricks declarative pipelines - opinions,"Hello.

Weâ€™re not on databricks as yet, but probably will be within a few months. The current fabric poc seems to be less proof of concept and more pile of c*** 

Fortunately Iâ€™ve used enough of databricks in the past to know my way around it. But potentially weâ€™ll be looking at using declarative pipelines which Iâ€™m just researching atm. 
Look like the usual case of great for simple, standard stuff which turns into a nightmare when things get complicatedâ€¦.


Does anyone have any practical experience of these, or can point me at useful (I.e not just marketing) resources?

Ta!
",1767980822.0,13,10,https://www.reddit.com/r/dataengineering/comments/1q8f0kg/databricks_declarative_pipelines_opinions/
1q8e6c8,How to choose the optimal sharding key for sharding sql (postgres) databases?,"As the title says if I want to shard a sql databse how can I choosse what tthe sharding key should be without knowing the schema beforehand?

This is for my final year project where I am trying to develop a application which can allow to shard sql databases. the scope is very limited with the project only targeting postgres database and only point quires with some level of filtering allowed. I am trying to avoid ranges or keyless aggregation queries as they will need the scatter-gather approach and does not really add anything towards the purpose of project.

Now I decided to use hash based routing and the logic for that itself is implemetd but I cannot decide how do I choose the sharding key which will be used to decide where the query is to be routed ? I am thinking of maintaining of a registry which maps each key to its respetive table. However as I tried to see how this approach works for some schemas I noticed that many table use same fields which are also unique, which means we can have same sharding key for mutiple tables. We can use this try to groups such tables together in same shard allowing for more optimised query result.

However i am unable to find or think of any algorithm that can help me to find such fields across tables. Is there any feasible solution to this? thanks for help!",1767978974.0,0,10,https://www.reddit.com/r/dataengineering/comments/1q8e6c8/how_to_choose_the_optimal_sharding_key_for/
1q8ble3,why does lance need a catalog? genuinely asking,"ok so my ML team switched to lance format for embeddings a few months ago. fast for vector stuff, cool.

but now we have like 50 lance datasets scattered across s3 and nobody knows what's what. the ML guys just name things like user\_emb\_v3\_fixed.lance and move on.



meanwhile all our iceberg tables are in a proper catalog. we know what exists, who owns it, what the schema looks like. standard stuff.

  
started wondering - does lance even have catalog support? looked around and found that gravitino 1.1.0 (dropped last week) added a lance rest service. basically exposes lance datasets through http with the same auth as your other catalogs.



[https://github.com/apache/gravitino/releases/tag/v1.1.0](https://github.com/apache/gravitino/releases/tag/v1.1.0)



the key thing is gravitino also supports iceberg so you can have both your structured tables and vector datasets in one catalog. unified governance across formats. pretty much what we need

thinking of setting it up next week. seems like the only apache project that federates traditional + multimodal data formats

questions:

1. anyone actually cataloging their lance datasets? or is everyone just yolo-ing it
2. does your company treat embeddings as real data assets or just temporary ml artifacts

genuinely curious how others handle this because right now our approach is ""ask kevin, he might remember""",1767973306.0,24,3,https://www.reddit.com/r/dataengineering/comments/1q8ble3/why_does_lance_need_a_catalog_genuinely_asking/
1q8biej,Should I learn any particular math for this job?,"I've taken Discrete Math when I was working in software development. I've since earned a MS in Data Analytics and am working as a database manager/analyst now. I want to transition to data engineering long-term and am buffing up my SQL, Python, and following the learning resources on the community wiki as well as using DataCamp. But I read online that Linear Algebra is really important for engineering. Before I invest a bunch of time into that, is it really good to know? I'm glad to learn it if other people in the field recommend doing so. Thank you. ",1767973117.0,5,10,https://www.reddit.com/r/dataengineering/comments/1q8biej/should_i_learn_any_particular_math_for_this_job/
1q8a6aq,Anything I should look out for when using iceberg branch capability?,"I want to use iceberg branch feature as a way to create a stage table, and run some sets of test and table metrics before promoting it to main. Just wanted to hear folks pratical experience with this feature and if I need to watch out for anything.

Thanks ",1767970074.0,2,0,https://www.reddit.com/r/dataengineering/comments/1q8a6aq/anything_i_should_look_out_for_when_using_iceberg/
1q89jka,AbInitio : Is it the end?,"Hi all,

I am at bit of crossroads and would like suggestions from experts here.

I have spent my entire career working with AbInitio ( over 10 years) , and I feel the number of openings for this tool at my experience are very less. Also, all the companies that uses to work with AbInitio just a few years ago are trying really hard to move away from it. 

That brings me to crossroads in my career, with nothing else to show up forâ€¦..

I would like the experts here suggest what should be a good course of action for someone like me? Should I go learn Spark? Or Databricks, Or Snowflake? How long would it usually take to build a similar level of expertise in these tools that I have in AbInitio???",1767968573.0,14,24,https://www.reddit.com/r/dataengineering/comments/1q89jka/abinitio_is_it_the_end/
1q89cew,"Is there a better term or phrase for ""metadata of ETL jobs""?","I'm thinking of revamping how the ETL jobs' orchestration metadata is setup, mainly because they're on a separate database. The metadata includes typical fields like `last_date_run, success, start_time, end_time, source_system, step_number, task` across a few tables. The tables are queried around the start of an ETL job to get information like the specific jobs to kick off, when the last time the job was run, etc. Someone labeled this a 'connector framework' years ago but I want to suggest a better name if I rework this since it's so vague and non-descriptive.

It's too early in the morning and the coffee hasn't hit me yet so I'm struggling to think of a better term - how would you call this? I'd rather just use a industry-wide term or phrase if I actually end up renaming this.",1767968092.0,10,9,https://www.reddit.com/r/dataengineering/comments/1q89cew/is_there_a_better_term_or_phrase_for_metadata_of/
1q857di,Dbt fundamentals with BigQuery help,"I've just started the dbt fundamentals course, using BigQuery as a data warehouse, and I've run into a problem. When I try to run the `dbtf run` command I get the error that my dataset ID ""dbt-tutorial"" is invalid. The ""Create a profiles.yml file"" part of the course says the database name is ""dbt-tutorial"", so (the top part of) my profiles.yml looks like this:

    default:
    Â  target: dev
    Â  outputs:
    Â  Â  dev:
    Â  Â  Â  type: bigquery
    Â  Â  Â  threads: 16
    Â  Â  Â  database: dbt-practice-483713
    Â  Â  Â  schema: dbt-tutorial
    Â  Â  Â  method: service-account

I realize the schema should likely be part of my own project, which currently doesn't have any schema, but she never explains this in the course. When I change dbt-tutorial to dbt\_tutorial, the error becomes that I either don't have permission to query table dbt-tutorial:jaffle\_shop.customers, or that it doesn't exist.

In ""Set up a trial BigQuery account"" she runs some select statements but never actually adds any data to the project through BigQuery, which she does do in the Snowflake video. I also changed raw.jaffle\_shop.customers to `\`dbt-tutorial\`.jaffle_shop.customers`, as the raw schema doesn't exist. 

  
Am I meant to clone the dbt-tutorial.jaffle\_shop data into my own project? Have I not followed the course correctly?",1767955848.0,3,1,https://www.reddit.com/r/dataengineering/comments/1q857di/dbt_fundamentals_with_bigquery_help/
1q82jk3,can someone help with insights in databricks apps?,"so i need to gather all the doc there is about insights(beta) available on databricks apps. 

it basically shows who all have accessed the apps, uptime, and app availability. 

itâ€™s still beta version so iâ€™m happy to get all the help i can 

thank you ",1767945982.0,6,0,https://www.reddit.com/r/dataengineering/comments/1q82jk3/can_someone_help_with_insights_in_databricks_apps/
1q81qwi,What's your approach to versioning data products/tables?,"We are currently working on a few large models, which let's say is running at version 1.0. This is a computationally expensive model, so we run it when lots of new fixes and features are added. How should we version them, when bumping to 1.1?

* Do you add semantic versioning to the table name to ensure they are isolated?
* Do you just replace the table?
* Any other?",1767943025.0,2,5,https://www.reddit.com/r/dataengineering/comments/1q81qwi/whats_your_approach_to_versioning_data/
1q7sc09,Migrating from Data Analytics to Data Engineering: Am I on the right track or skipping steps?,"Currently, I'm interning in data management, focusing mainly on data analysis. Although I enjoy the field, I've been studying and reflecting a lot about migrating to Data Engineering, mainly because I feel it connects much more with computer science, which is my undergraduate course, and with programming in general.

The problem is that I'm full of doubts about whether I'm going down the right path. At times, this has generated a lot of anxiety for meâ€”to the point of spending sleepless nights wondering if I'm making the wrong choices or getting ahead of myself.

The company where I'm interning offers access to Google Cloud Skills Boost, and I'm taking advantage of it to study GCP (BigQuery, pipelines, cloud concepts, etc.). Still, I keep wondering:
Am I doing the right thing by going straight to the cloud and tools, or should I consolidate more fundamentals first?
Is it normal for this transition to start out ""confusing"" like this?

I would also really appreciate recommendations for study materials (books, courses, learning paths, practical projects) or even tips from people who already work as Data Engineers. Honestly, I'm a little lost â€” that's the reality. I identified quite a bit with Data Engineering precisely because it seems to deal much more with programming, architecture, and pipelines, compared to the more analytical side.


For context, today I have contact/knowledge with:

â€¢ Python

â€¢ SQL

â€¢ R

â€¢ Databricks (creating views to feed BI)

â€¢ A little bit of Spark

â€¢ pandas

I would really like to hear the experience of those who have already gone through this migration from Data Analytics to Data Engineering, or those who started directly in the area.

What would you do differently looking back?


Thank you in advance",1767916337.0,20,12,https://www.reddit.com/r/dataengineering/comments/1q7sc09/migrating_from_data_analytics_to_data_engineering/
1q7s6fv,question on data tables created,"for context i am a data analyst at a company and we often provide our requirements for data to the data engineering team. i have some questions on their thought process but i do not want them to feel m attacking them so asking here. 

1. we have snowflake multiple instances - i have observed they create tables in an instance of their choice and do not have a system for that. 

2. I know tha usually we have dim tables and fact tables but what i have observed is that would create one big table with say year fy 2026 repeating across. Is it because snowflake is cheap and can handle a lot of stuff that works? ",1767915960.0,2,2,https://www.reddit.com/r/dataengineering/comments/1q7s6fv/question_on_data_tables_created/
1q7qr1l,Is copilot the real deal or are sellers getting laid off for faltering Fabric sales?,"Reports say that Microsoft is about to layoff another 20k folks in xbox and azure - but xbox folks have denied the report; azure is suspiciously quiet...

I am wondering if copilot transforming the way Microsoft works and they can shed 20k azure sellers or is another case of faltering sales are being compensated by mass staff reductions?

People keep telling me no-one is buying Fabric, is that what is happening here? Is anyone spending real money on Fabric? We have just convinced our management to go all in on GCP for the data platform. We are even going to ditch Power BI for Looker.",1767912535.0,36,38,https://www.reddit.com/r/dataengineering/comments/1q7qr1l/is_copilot_the_real_deal_or_are_sellers_getting/
1q7n37e,Is it bad to take a career break now considering the ramping up of AI in the space ?,"Hi all,  


I find myself in a bit of a predicament and hoping for some insight / opinions from you all.  
I have around four years experience as an analyst and almost a year in data engineering, but the role is extremely high-pressure and I've been burnt out for about the last 6 months, I'm just pushing hard to get through it. Note, I've spoken to my colleagues and they agree our workload definitly exceeds what the norm is in other companies, rather than this being a skill issue. Iâ€™ve been offered a six-month contract in a familiar area that is data related but significantly less stress and a role I have done before. I would earn my annual salary in 6 months and definitly get some mental health recovery, but Iâ€™m worried that stepping away from data engineering so early, especially given how fast the field and AI tooling are evolving, could make it difficult to re-enter as I would be unfamialir with the new tools / processes. I personally am quite worried about job security in the near to mid future and don't want to further damage my prospects. At my current role we are literally using and upgrading our AI tech stack on a weekly basis and it's hard enough to keep up while I'm in it, let alone if I leave the domain. All to say, with the constant improvements and upgrades in AI and essentially my current role shifting from programming and more to ideation and agent managment, would taking a break to make noticably more money and give me a mental health break be a bad idea due to the struggles of then re-entering the market. 

I'd appreciate any opinions, I'm all ears!

Thanks all ! 

",1767904298.0,9,31,https://www.reddit.com/r/dataengineering/comments/1q7n37e/is_it_bad_to_take_a_career_break_now_considering/
1q7my33,Do you have front end access?,I suspect the answers to be split. The people who move data from point A to point B won't but those in smaller businesses or also involved with design will have front end access. I'm working at a hospital now and although I understand protecting PII it's like working with an arm tied behind my back. ,1767903981.0,0,17,https://www.reddit.com/r/dataengineering/comments/1q7my33/do_you_have_front_end_access/
1q7lpg6,Anyone using JDBC/ODBC to connect databases still?,"I guess that's basically all I wanted to ask.

I feel like a lot more tech and company infra are using them for connections than I realize. I'm specifically working in Analytics so coming from that point of view. But I have no idea how they are thought of in the SWE/DE space.",1767901253.0,90,88,https://www.reddit.com/r/dataengineering/comments/1q7lpg6/anyone_using_jdbcodbc_to_connect_databases_still/
1q7lme0,Python - Ultimate,"Working as a junior data engineer and realised that python is the ultimate thing you need to know for ingestions. Be it from APIs, sources etc etc. Python has a library for everything. Python is the ultimate ofc along with SQL",1767901071.0,0,3,https://www.reddit.com/r/dataengineering/comments/1q7lme0/python_ultimate/
1q7lcal,How to get back into data engineering after a year off,"I was working as a data engineer for 6 years but was laid off. I have been searching to get a position but it's been difficult. At this point its been a year since the layoff. I know this is considered a red flag by a lot of companies so I was thinking of getting some certifications. Specifically Databricks professional developer, AWS data engineer certification & AWS Machine Learning certification. Reason being that at my past role I worked with Databricks/AWS & did some work in the machine learning space working with our data scientist. My question is with the expense off the certifications & time required to prepare is this",1767900451.0,7,3,https://www.reddit.com/r/dataengineering/comments/1q7lcal/how_to_get_back_into_data_engineering_after_a/
1q7i1v1,"I turned my Manning book on relational database design into a free, open-access course with videos, quizzes, and assignments","I'm the lead author of [Grokking Relational Database Design](https://mng.bz/PRER) (Manning Publications, 2025), and over the past few months I've expanded the book into a full open-access course.

**What it covers:** The course focuses on the fundamentals of database design:

* ER modeling and relationship design (including the tricky many-to-many patterns)
* Normalization techniques (1NF through BCNF with real-world examples)
* Data types, keys, and integrity constraints
* Indexing strategies and query optimization
* The complete database design lifecycle

**What's included:**

* 28 video lectures organized into 8 weekly modules
* Quizzes to test your understanding
* Database design and implementation assignments
* Everything is free and open-access on GitHub

The course covers enough SQL to get you productive (Week 1-2), then focuses primarily on database design principles and practice. The SQL coverage is intentionally just enough so it doesn't get in the way of learning the core design concepts.

**Who it's for:**

* Backend developers who want to move beyond CRUD operations
* Bootcamp grads who only got surface-level database coverage
* Self-taught engineers filling gaps in their knowledge
* Anyone who finds traditional DB courses too abstract

I originally created these videos for my own students when flipping my database course, and decided to make them freely available since there's a real need for accessible, practical resources on this topic.

**Links:**

* Full course on GitHub: [https://github.com/StructuredCS/grokking-relational-database-design](https://github.com/StructuredCS/grokking-relational-database-design)
* Complete playlist: [https://www.youtube.com/playlist?list=PL3fg3zQpW0k4UO9eBDLdroADnB18ZAOgj](https://www.youtube.com/playlist?list=PL3fg3zQpW0k4UO9eBDLdroADnB18ZAOgj)

Happy to answer questions about the course content or approach.",1767893485.0,58,6,https://www.reddit.com/r/dataengineering/comments/1q7i1v1/i_turned_my_manning_book_on_relational_database/
1q7ef9r,Datacompose: Verified and tested composable data cleaning functions without dependencies,"# The Problem: 

I hate data cleaning with a burning passion. I truly believe if you like regex then you have Stockholm syndrome. So built a library with commonly used data cleaning functions that are pre verified that can be used without dependencies in your code base. 

 
Before:

```

# Regex hell for cleaning addresses
df.withColumn(""zip"", 
    F.regexp_extract(F.col(""address""), r'\b\d{5}(?:-\d{4})?\b', 0))
df.withColumn(""city"",
    F.regexp_extract(F.col(""address""), r',\s*([A-Z][a-z\s]+),', 1))
# Breaks on: ""123 Main St Suite 5B, New York NY 10001""
# Breaks on: ""PO Box 789, Atlanta, GA 30301""  
# Good luck maintaining this in 6 months

```

Data cleaning primitives are small atomic functions that you are able to put into your codebase that you are able compose together to fit your specific use ages. 

```

# Install and generate
pip install datacompose
datacompose add addresses --target pyspark

# Use the copied primitives
from pyspark.sql import functions as F
from transformers.pyspark.addresses import addresses

df.select(
    addresses.extract_street_number(F.col(""address"")),
    addresses.extract_city(F.col(""address"")),
    addresses.standardize_zip_code(F.col(""zip""))
)

```

[PyPI](https://pypi.org/project/datacompose/) | [Docs](https://datacompose.io) | [GitHub](https://github.com/datacompose/datacompose)


",1767885421.0,2,2,https://www.reddit.com/r/dataengineering/comments/1q7ef9r/datacompose_verified_and_tested_composable_data/
1q7aw05,"Who does everyone keep milking SCDs , but noone talks about RCDs","TIL this term even exists. Have watched so many dimensionsonal modelling playlists, none of them covered this",1767876418.0,0,12,https://www.reddit.com/r/dataengineering/comments/1q7aw05/who_does_everyone_keep_milking_scds_but_noone/
1q7atkw,Deeplearning.ai Data Engineering Course,"I've been looking for a course to give me a swift introduction and some practise into data engineering.

There's IBM's course and Deeplearning.ai's course on Coursera. I'm indecisive between the two. IBM one is long and covers a lot of stuff. Deeplearning.ai also has a quality and teaching style I'm fond of, and has a partnership with AWS.

Which one do you recommend and why?",1767876217.0,2,6,https://www.reddit.com/r/dataengineering/comments/1q7atkw/deeplearningai_data_engineering_course/
1q7asla,Data ingestion to data lake,"Hi

Looking for some guidance. Do you see any issues using UPDATE operations during ingestion to bronze delta tables for existing rows? ",1767876131.0,3,7,https://www.reddit.com/r/dataengineering/comments/1q7asla/data_ingestion_to_data_lake/
1q793r0,Spark job slows to a crawl after multiple joins any tips for handling this,"Iâ€™m running a Spark job where a main DataFrame with about 820k rows and 44 columns gets left joined with around 27 other small DataFrames each adding 1 to 3 columns. All joins happen one after another on a unique customer ID.

Most tasks run fine but after all joins any action like count or display becomes painfully slow or sometimes fails. Iâ€™ve already increased executor memory and memory overhead, tweaked shuffle partition counts, repartitioned and persisted between joins, and even scaled the cluster to 2-8 workers with 28â€¯GB RAM and 8 cores each. Nothing seems to fix it.

At first I thought it would be simple since the added tables are small. Turns out that the many joins combined with column renaming forced Spark to do broadcast nested loop joins instead of faster broadcast hash joins. Changing join types helped a lot.

Has anyone run into something like this in production? How do you usually handle multiple joins without killing performance? Any tips on caching, join strategies, or monitoring tools would be really helpful.  
  
TIA

",1767870697.0,29,18,https://www.reddit.com/r/dataengineering/comments/1q793r0/spark_job_slows_to_a_crawl_after_multiple_joins/
1q76ve8,What do you think about design-first approach to data,"How do you feel about creating data models and lineage first before coding?

Historically this was not effective because it requires discipline, and eventually all those artifacts would drift to the point of unusable. So modern tools adapt by inferring the implementation and generates these artifacts instead for review and monitoring.

However now, most people are generating code with AI. Design and meaning become a bottleneck again. I feel design-first data development will make a comeback.

What do you think?",1767862453.0,11,28,https://www.reddit.com/r/dataengineering/comments/1q76ve8/what_do_you_think_about_designfirst_approach_to/
1q74is3,Databricks Real world scenario problems,"I am trying to clear databricks data engineer role job but I donâ€™t have that much professional hands on experience, would want to some of the real world scenario questions you get asked and what their answers could be.

One question I am constantly asked what are common problems you faced while running databricks and pyspark in your Elt architecture.",1767853877.0,9,7,https://www.reddit.com/r/dataengineering/comments/1q74is3/databricks_real_world_scenario_problems/
1q70p8d,Remote Data Engineer - Work/Life Question,"For the Data Engineers in the group that work fully remote:

\- what is your flexibility with working hours?

\- how many meetings do you typically have a day? In my experience, DE roles mostly have a daily standup and maybe 1-2 other meetings a week. 



I am currently working full time in office and looking for a switch to fully remote roles to improve my work/life flexibility. 

  
I generally much prefer working in the evenings and spending my day doing what I want. ",1767842251.0,39,44,https://www.reddit.com/r/dataengineering/comments/1q70p8d/remote_data_engineer_worklife_question/
1q6wi44,Data Engineering certificate,"Tenho trabalhado com anÃ¡lise de dados hÃ¡ cerca de 3 anos. Como faÃ§o parte de uma empresa pequena, meu papel vai alÃ©m da anÃ¡lise pura e frequentemente lido tambÃ©m com tarefas de engenharia de dados â€” construindo e mantendo pipelines de dados usando Airflow, dbt e Airbyte.

Atualmente, estou buscando uma transiÃ§Ã£o mais formal para um cargo de Engenheiro de AnÃ¡lise ou Engenheiro de Dados e gostaria de receber conselhos sobre quais certificaÃ§Ãµes realmente ajudam nessa transiÃ§Ã£o.

CertificaÃ§Ãµes como Engenharia de AnÃ¡lise, Engenheiro de Dados do Google/AWS ou certificaÃ§Ãµes relacionadas ao Airflow valem a pena na prÃ¡tica? Alguma recomendaÃ§Ã£o baseada em experiÃªncia real de contrataÃ§Ã£o?  
  
\--------  
  
Iâ€™ve been working in data analytics for about 3 years. Since Iâ€™m part of a small company, my role goes beyond pure analysis, and I often handle data engineering tasks as well â€” building and maintaining data pipelines using Airflow, dbt, and Airbyte.

Iâ€™m currently looking to move more formally into an Analytics Engineer or Data Engineer role and would love some advice on which certifications actually help in this transition.

Are certifications like Azute Analytics Engineering, Google/AWS Data Engineer, or Airflow-related certs worth it in practice? Any recommendations based on real hiring experience?",1767831297.0,7,4,https://www.reddit.com/r/dataengineering/comments/1q6wi44/data_engineering_certificate/
1q6w5sr,"Does your org use a Data Catalog? If not, then why?","In almost every company that I've worked at (mid to large enterprises), we faced many issues with ""the source of truth"" due to any number of reasons, such as inconsistent logic applied to reporting, siloed data access and information, and others. If a business user came back with a claim that our reports were inaccurate due to comparisons with other sources, we would potentially spend hours trying to track the lineage of the data and compare any transformations/logic applied to pinpoint exactly where the discrepancies happen.

I've been building a tool on the side that could help mitigate this by auto-ingesting metadata from different database and BI sources, and tracking lineage and allowing a better way to view everything at a high-level.

But as I was building it, I realized that it was similar to a lightweight version of a Data Catalog. That got me wondering why more organizations don't use a Data Catalog to keep their data assets organized and tie in the business definitions to those assets in an attempt to create a source of truth. I have actually never worked within a data team that had a formatlized data catalog; we would just do everything including data dictionaries and business glossaries in excel sheets if there was a strong business request, but obviously those would quickly become stale.

**What's been your experience with Data Catalog? If your organization doesn't use one, then why not (apart from the typically high cost)?**

My guess is the maintenance factor as it could be a nightmare maintaining updated business context to changing metadata especially in orgs without a specialized data governance steward or similar. I also don't see alot of business users using it if the software isn't intuitive, and general tool fatigue.
",1767830448.0,61,85,https://www.reddit.com/r/dataengineering/comments/1q6w5sr/does_your_org_use_a_data_catalog_if_not_then_why/
1q6vkqb,Hot take: search is not the big data problem for AI. Knowledge curation is.,,1767829002.0,0,2,https://www.reddit.com/r/dataengineering/comments/1q6vkqb/hot_take_search_is_not_the_big_data_problem_for/
1q6urxw,11 Apache Iceberg Cost Reduction Strategies You Should Know,,1767827101.0,0,1,https://www.reddit.com/r/dataengineering/comments/1q6urxw/11_apache_iceberg_cost_reduction_strategies_you/
1q6uoz2,Advice - Incoming Meta Data Engineering Intern,"Hi everyone! I was recently fortunate enough to land a Data Engineering internship at Meta this summer and wanted to ask for advice on how best to prepare.

Iâ€™m currently a junior in undergrad with a background primarily in software engineering and ML-oriented work. Through research and projects, Iâ€™ve worked on automating ML preprocessing pipelines, data cleaning, and generating structured datasets (e.g., CSV outputs), so I have some exposure to data workflows. That said, I know production-scale data engineering is a very different challenge, and Iâ€™d like to be intentional about my preparation.

From what Iâ€™ve read, Metaâ€™s approach to data engineering is fairly unique compared to many other companies (heavy SQL usage, large-scale analytics), and a lot of internal tooling. Right now, Iâ€™m working through the dataexpert .io free bootcamp, which has been helpful, but Iâ€™m hoping to supplement it with additional resources or projects that more closely resemble the work Iâ€™ll be doing on the job.

Ideally, Iâ€™d like to build a realistic end-to-end project, something along the lines of:

* Exploratory data analysis (EDA)
* Extracting data from multiple sources
* Building a DAG-based pipeline
* Surfacing insights through a dashboard

# Questions:

1. For those whoâ€™ve done Data Engineering at Meta (or similar companies), what skills mattered most day-to-day?
2. Are there any tools, paradigms, or core concepts youâ€™d recommend focusing on ahead of time (especially knowing Meta uses a largely internal stack)?
3. On the analytical side, whatâ€™s the best way to build intuition, should I try setting up my own data warehouse, or focus more on analysis and dashboards using public datasets?
4. Based on what I described, do you have any project ideas or recommendations that would be especially good prep?

For reference I am not sure which team I am yet and I have roughly 5 months to prep (starts in May)",1767826908.0,10,7,https://www.reddit.com/r/dataengineering/comments/1q6uoz2/advice_incoming_meta_data_engineering_intern/
1q6spxv,Clustering on BigQuery,"
I have a large table in BQ c. 1TB of data per day. 

Itâ€™s currently partitioned by day. 

I am now considering adding clusters. 

According to Googleâ€™s documentation:

https://docs.cloud.google.com/bigquery/docs/clustered-tables

The order of the clustered columns matter.

However when I ran a test, that doesnâ€™t seem to be the case. 

I clustered my table on two fields (field1,field2)

Select count(*) from table where field2 = â€œyesâ€ 

Resulted in 50gb of less data scanned vs the same query on the original table. 

Does anyone know why this would be the case? 

According to the documentation this shouldnâ€™t work. 

Thank you! 
",1767822425.0,1,3,https://www.reddit.com/r/dataengineering/comments/1q6spxv/clustering_on_bigquery/
1q6sbey,BigQuery? Expensive? Maybe not so much!,"Hey guys! Pleasure to meet you. I'm the CEO of [CloudClerk.ai](http://CloudClerk.ai), a startup focused on enabling teams to properly control their BigQuery expenses. Been having some nice conversations with other members of this subreddit and other related ones, so I figured I could do a quick post to share what we do in case we could help someone else too!

In CloudClerk we want to return to teams the ""ownership"" of their cost information. I like to make some stress on the ownership because we've seen other players in the sector help teams optimize their setup but once they leave, the teams are as clueless as before and need to contact them again in the future.

We like to approach the issue a bit differently, by giving clients all the tools they need to make informed decisions about changes in their projects. To do so we leverage 4 different elements: 

* **Audits** that are only billed based on success cases that we define together with clients. 
* **Mentoring services** to share our knowledge with employees of businesses.
* Our **platform** that allows to find, monitor and track the exact sources of cost (query X, table Y, reservations, etc) in less than 10 minutes.

>We expect to have ready by the end of the month necessary features like building custom dashboards from our exploring tool and having automatic alerting by analyzing trends of consumption based on different needs. We started as a service, so we are basically producticing all the elements that we used internally in a way where even a 6 year old could benefit from them.

* Our own **custom AI agents**, specialized in optimizing costs in BigQuery. Since we know IP & PII are deal breakers for some, we also built a protective layer that can be toggled on to ensure that actual data never gets to them, without hindering optimization recommendations.   

Clients should be able to, initially, find their sources of expenses and have automatic recommendations, and once fully embbeded, to not even need to find sources of expenses, but have direct explanations on what should be optimized and how to do so. Similarly, forget about getting alerts and debugging. If you get an alert, expect to have a clear explanation shortly after. 

These are just some of the things we will be implementing in the following weeks, but expect more updates in the near future! So far we've had very good results in cutting businesses costs, but more importantly, clients know how we did it and they can benefit from it. 

Would love to hear your opinion, thoughts, critics. Hit us up if you are curious, if you know this could help you, or even if you just want to have a quick chat with new ideas! 

Hope you have a great day and happy new year!









  
",1767821416.0,0,6,https://www.reddit.com/r/dataengineering/comments/1q6sbey/bigquery_expensive_maybe_not_so_much/
1q6rmkj,Interesting databricks / dbt cost and performance optimization blog post,Looks like Calm shaved off a significant portion of their databricks bill and decreased clock time by avoiding dbt parsing. Who would have thought parsing would be that intensive. [https://blog.calm.com/engineering/how-we-cut-our-etl-costs](https://blog.calm.com/engineering/how-we-cut-our-etl-costs),1767819877.0,24,7,https://www.reddit.com/r/dataengineering/comments/1q6rmkj/interesting_databricks_dbt_cost_and_performance/
1q6r2l0,Do you still apply to SWE roles?,"Iâ€™m a new grad with two data engineering internships. Iâ€™ve been reading that data engineering is only an emphasis of software engineering, where software engineering is more generalized. Does that mean itâ€™s safe to apply to general SWE roles with hopes of being placed into data? ",1767818611.0,5,2,https://www.reddit.com/r/dataengineering/comments/1q6r2l0/do_you_still_apply_to_swe_roles/
1q6omnc,Is it still worth starting Data Engineering now in 2026?,"Hi everyone,

I am 24 yo and trying to make a realistic decision about my next career step.

I have an engineering background in Electronics and I have been working full time in electronics for about two years. At the same time, I am currently enrolled in a Computer Scienceâ€“related masterâ€™s program, which is more of a transition program for people who want to move into programming, because I donâ€™t come from a strong CS background.

I have realized that electronics is not what I want to do long term and I donâ€™t enjoy it anymore, and I am looking and struggling for a meaningful change in my career.

I am considering to invest this year into learning Data Engineering, with the goal of being job ready for a junior Data Engineer until 2027

What Iâ€™m trying to understand realistically is:
1. How competitive the junior Data Engineering market really is right now?
2. Someone who is starting now has real chances of landing a first job in this field?
3. How much AI is realistically going to reduce entry level opportunities?

I will be honest, I have been feeling quite demotivated and unsure about my next steps, and I donâ€™t really know what the right move is anymore. Thanks a lot for taking the time to read this and for any perspectives you are willing to share.",1767813183.0,27,61,https://www.reddit.com/r/dataengineering/comments/1q6omnc/is_it_still_worth_starting_data_engineering_now/
1q6of6k,LC but recruiters say no LC,"Interviewers are atleast asking LC medium/hard for staff roles but recruiters dont mention it at all!! Why do recruiters not want us to get hired?! And how do we focus on so many concepts, tools to know along with lc! Ugh! And this is not even FAANG! :(",1767812745.0,0,6,https://www.reddit.com/r/dataengineering/comments/1q6of6k/lc_but_recruiters_say_no_lc/
1q6l5be,Looking for Realistic End-to-End Data Engineering Project Ideas (2 YOE),"Iâ€™m a Data Engineer with ~2 years of experience, working mainly with ETL pipelines, SQL, and cloud tools. I want to build an end-to-end project that realistically reflects industry work and helps strengthen my portfolio.

What kind of projects would best demonstrate real-world DE skills at this level? Looking for ideas around data ingestion, transformation, orchestration, and analytics.",1767805860.0,14,14,https://www.reddit.com/r/dataengineering/comments/1q6l5be/looking_for_realistic_endtoend_data_engineering/
1q6j5wn,query caching for data engineering pipelines (ai/ml),"Hi everyone - looking for some community wisdom on Ai/ML pipelines

(Disclaimer: this is for my startup so I have a monetary interest)

My cofounder just finished v1 of our zero-config transparent Postgres proxy that acts as a cache. Self-refreshes using the postgres CDC stream.

The primary use case we've been building for is as a more elegant and efficient alternative to Redis TTL, that would also reduce implementation and management overhead.

My question is whether you all think there may be clear applications/value of this kind of tool to ML/Ai pipelines. And if so, where would be a good place to start fleshing that out? I'm not fluent enough in Ai/ML to know.

(I'm a product manager by trade - my cofounder is a 20 year postgres vet but mostly in the web app space)

Have a look and thanks for any insights! Our product is [pgache.com](http://pgache.com)",1767801539.0,0,3,https://www.reddit.com/r/dataengineering/comments/1q6j5wn/query_caching_for_data_engineering_pipelines_aiml/
1q6ixt6,DE Blogging Without Being a Linkedin Lunatic,"Hello,

I am a sales engineer who's been told it would help my career if I do some blogging or start trying to ""market myself."" Fun.   
I think it would be cool, however I don't want to sound like a pretentious Linkedin Lunatic who's doing more boasting than something that would be entertaining/insightful to read.

Is there a DE community or place to blog that would be receptive to non-salesy type posts??",1767801037.0,34,26,https://www.reddit.com/r/dataengineering/comments/1q6ixt6/de_blogging_without_being_a_linkedin_lunatic/
1q6ienl,Open-Source data and error tracking for Airflow,"Iâ€™m a SWE turned data engineer, long time lurker in this sub, and Iâ€™ve been using Airflow for a while now. One thing thatâ€™s consistently been a pain for me is monitoring: Airflow doesnâ€™t give me a great way to track errors across DAGs/runs/environments, and setting up alerts with the included stats+prometheus always turns into a mini-project.

I tried the usual stuff:

* Airflow UI + logs + retries (fine for one-off debugging, rough for patterns)
* The common open-source Airflow + Grafana dashboards (I spent time wiring it upâ€¦ then basically never used it)
* Sentry (I love Sentry as a SWE, and Airflow has integrations), but it still felt awkward because Sentry doesnâ€™t really build around the concepts of  DAGs in the way I wanted, and Iâ€™d often end up with noisy/irrelevant errors + not enough context to quickly group recurring failures

So I built a self-hosted solution for myself (planning to open source it soon) with these features:

* 100% self-hosted: no data leaves your servers
* Easy install (Docker Compose + a lightweight Python SDK alongside Airflow)
* Operator-aware metrics: automatically integrate with popular operators to collect useful signals (e.g., row counts for SQL/dbt-ish tasks, file sizes/counts for S3-style tasks, etc.)
* Custom metrics in-code: a simple syntax to emit your own metrics right inside Airflow tasks when you need it
* Error tracking across DAGs + environments: aggregate/fingerprint failures so recurring issues are obvious (and you can fix them before they hit prod)
* Built-in alerting focused on data issues (currently: row count drops, schema changes, unexpected null counts)
* Notifications via SMTP or Resend (e.g., new errors / alert triggers)
* A modern and lightweight dashboard (Next.js)

Is this something other people are interested in, or did I just solve a ""me problem""?  
Are there any features or integrated alerts that would make this worth your while?

Cheers",1767799849.0,5,3,https://www.reddit.com/r/dataengineering/comments/1q6ienl/opensource_data_and_error_tracking_for_airflow/
1q6i8vy,"In case you're deciding what data engineering cert to go for, I've put together a infographic you can skim for all of Snowflake's certifications",,1767799497.0,32,6,https://www.reddit.com/r/dataengineering/comments/1q6i8vy/in_case_youre_deciding_what_data_engineering_cert/
1q6hnaf,Help with Data Governance,"I recently finished a course on Data Governance and management, been applying for roles but no success, I don't have experience in the field and have stayed updated with data, I have the Dama dmbok cert, powerbi and az 900 cert, I also have a stem background. 

What can I do to improve success at landing a role? I have watched loads of YouTube videos to fill knowledge gap but I need hands on experience as well but I'm confident in the things I have learnt. 

Just looking for some advise on interviews and how to ace them, gotcha questions that can take me unaware as there's not a lot of those online compared to fields like analytics, engineering, data science, etc.

Any help would be appreciated. 

I also hope this is the right sub for this. Thanks.

",1767798147.0,2,3,https://www.reddit.com/r/dataengineering/comments/1q6hnaf/help_with_data_governance/
1q6hhef,Liquid clustering in databricks,"I want to know if we can process 100tb of data using liquid clustering in databricks. If yes, do we know what is the limit on the size and if no, what is the reason behind that?",1767797777.0,1,4,https://www.reddit.com/r/dataengineering/comments/1q6hhef/liquid_clustering_in_databricks/
1q6hgg4,EventFlux â€“ Lightweight stream processing engine in Rust,"I built an open source stream processing engine in Rust. The idea is simple: when you don't need the overhead of managing clusters and configs for straightforward streaming scenarios, why deal with it?

It runs as a single binary, uses 50-100MB of memory, starts in milliseconds, and handles 1M+ events/sec. No Kubernetes, no JVM, no Kafka cluster required. Just write SQL and run.

To be clear, this isn't meant to replace Flink at massive scale. If you need hundreds of connectors or multi-million event throughput across a distributed cluster, Flink is the right tool. EventFlux is for simpler deployments where SQL-first development and minimal infrastructure matter more.

GitHub: [https://github.com/eventflux-io/engine](https://github.com/eventflux-io/engine) 

Feedback appreciated!",1767797715.0,6,0,https://www.reddit.com/r/dataengineering/comments/1q6hgg4/eventflux_lightweight_stream_processing_engine_in/
1q6fxsb,DE career advice needed,"I have a non cs degree from Indian university. Did my masters in Data science in the US as soon ad i graduated. I got an internship that converted to full time job once i graduated from a consulting company mainly focused on data engineering( 50-70% Informatica , 30% other tools like Snowflake, databricks, looker ,powerbi , airflow, etc)

I was mostly doing POC s during my internship and was put on a very basic data cleaning client work - that mainly was similar to a small clg project involving xcel sheet of data that i had to clean using pandas / numpy and do some address validation)

Later i was put on an oracle to snowflake migration project where i was following orders from an architect. It was a 6  month project where i worked on breaking down the oracle logics that were 1000 lines of sql. Identifying where the joins were and basically

Broke down the whole hierarchy. It was financial data and involved 30 + tables. After that the architect drew out the entire data model structure for snowflake and we ran the ddl and created ddl for dims and facts. (Basically raw layer).

Then he gave us the logic to build out the following layers and we had to work on the logics together sometimes. He was not a pro on sql so he would just say- join this and this but we need this column to be used . Something like this.

We did all the typical stuff- developed in dev, moved to qa, did the testing all by ourself. We were 2 developers in the project. Who had to take ownership for everything snowflake related. 

Then came the client uat testing. So many arguments and so many questions. We had to take care of everything. It was cool to have ownership. Finally after making changes and testing vigourously we finally moved the data to prod env and then left the project.

Now Iâ€™ve been working with the ceo. Other clients are now catching up on the AI wave and want us to use ai in our daily workflow. But almost all of them in the company are resistant. I guess itâ€™s a mix of no time, and fear of replacement. So the ceo wants me and one more person with similar background as me to push ai to these ppl. So my work has completely moved to vibe coding. I am trying to automate a few use cases in the company. We are trying to connect snowflake and looker and similar tools to cursor / Claude and make the offshore team understand how to use them. Itâ€™s a work in progress. I am trying to understand informatica and projects related to that and see if we can use AI in the workflow too. 

From having a manager micromanage every 2 hours during the client project to now basically being self managing, a lot has changed in a few months. With a lot of resistance and no time availability from ppl, and i also have very less idea about these projects, i am stressed. 

I want to look for other jobs, but not sure what level / what role to apply for. Pls help me out if you guys have any suggestions to my current work and also to my job search. Thanks!! ",1767794022.0,1,1,https://www.reddit.com/r/dataengineering/comments/1q6fxsb/de_career_advice_needed/
1q6frgw,Mysql insert for 250 million records,"Guys i need suggestions to take on this problem.

I have to insert around 250million records into mysql table.

What i hv planned is - dividing data into 5m records each file. And then inserting 5m records using spark jdbc.

But encountered performance issue here as initial files took very less time(around 5mins) but then later files started taking longer like an hour or two.

Can anyone suggest a better way here.

Edit1- Thanks everyone for all the suggestions.

So due to DB side limitations load data option was unavailable.

Removing indexes helped alot. Initial 50m were inserted in no time. Remaining chunks took some more time but it was constant. Loading completed within a day run.",1767793584.0,23,27,https://www.reddit.com/r/dataengineering/comments/1q6frgw/mysql_insert_for_250_million_records/
1q6cug2,Data Lineage & Data Catalog could be a unique tool?,"Hi,

Iâ€™m trying to understand how Data Lineage and Data Catalog are perceived in the market, and whether their roles overlap.

I work in a company where we offer a solution that covers both. To simplify: on one hand, some users need a tool to trace data and its evolution over timeâ€”this is data lineage, and it ties into accountability. On the other hand, you need visibility into the information (metadata) about that data, which is what a data catalog provides. This is usually in one solution package. 

From your experience, do you think having a combined solution is actually useful, or is it not worth it? If so, what do you use for data governance? ",1767784924.0,9,11,https://www.reddit.com/r/dataengineering/comments/1q6cug2/data_lineage_data_catalog_could_be_a_unique_tool/
1q6862h,Warehousing for dataset with frequent Boolean search and modeling,"As the title states, I've got a large data set, but let's first  clarify what I mean by ""large"" -- about 1MM rows, unsure on total file size, estimating about a gig or two. Two tables.

I'm not a data engineer but I am sourcing this dataset from a python script that's extracting support ticket history via API and pushing to a CSV (idk if this was the best idea but we're here now...)

My team will need to query this regularly for historical ticket info to fill in gaps we didn't import to our new support system. I also will want to be able to query it to utilize it in reports.

We have metabase for our product... But I don't have much experience with it, not sure if that's an option??

Where can I host this data that isn't a fat .zip file that will break my team's computers?",1767767801.0,6,5,https://www.reddit.com/r/dataengineering/comments/1q6862h/warehousing_for_dataset_with_frequent_boolean/
1q60wdp,Tools Rant,"if someone has experience with BigQuery and other ETL tools and the job description goes like needs Snowflake, Dagster etc.   
  
These tools don't match my what I have and yes I have never worked on them but how difficult/different would be grab things and move at a pace ? 

1. Do I have to edit my entire CV to match the job description ?

2. Do you guys apply for such jobs or you simple skip it ? If you do get through it how do you manage the expectations etc ?  ",1767747232.0,2,8,https://www.reddit.com/r/dataengineering/comments/1q60wdp/tools_rant/
