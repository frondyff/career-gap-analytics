post_id,comment_rank,comment_body,comment_score
1qwx5xo,1,"Don‚Äôt be afraid to say ‚ÄúI don‚Äôt know, let me get back to you with specifics‚Äù. 

This will carry through to all levels over your career. Soft-skills are just as (if not more) important than hard-skills.",11
1qwx5xo,2,"Honestly, you should ask your team. Also try to‚Ä¶ you know‚Ä¶ use less AI‚Ä¶",8
1qwx5xo,3,"How are you using the AI? Do you have a clear idea of what the code you need written is, and using AI to help with the syntax and then debugging and testing after? What worked for me was thinking about what I needed a piece of code to do, and then go from there, making sure to test the output, regardless of whether or not I wrote the code using AI or not. 

Think about: can I isolate this piece of logic to a single method? Are there shared attributes I can leverage through classes? Once you have an idea of what you need it to do, you can go through the steps and break it apart into different methods. Then it‚Äôs just a matter of testing to make sure it works. Hope this helps.",3
1qwx5xo,4,"did you validate and test in staging

did you make sure there were no side effects which might occur as a result of the changes you pushed if theres an error",2
1qwx5xo,5,"They're not worried you use AI, they're worried you don't know what the code does",2
1qwx5xo,6,"The short answer is almost everything, in production you should generally be careful.


This doesn't sound like your fault. Maybe the bar is higher and things work differently at those caliber places.


But in most places jr's have mid level and seniors for whom an aspect of their job is teaching and keeping jr's safely productive.


If these people exist and you aren't asking questions, then it's your fault.",1
1qwx5xo,7,"Is it a greenfield project, or is there existing code that you could use as inspiration? As in, you take a running job, you copy that, understand what each step does and tweak it for your purpose.",1
1qwx5xo,8,"- Don't deploy any code on Friday. 
- Ask AI why it did everything it did. Sometimes it'll say ""oh that's a mistake, I should do it some other way."" Sometimes it'll explain why it did something. Then you can say ""but I'm aggregating data on line 45 of my query. This logic won't work in that case."" To which the AI could say ""oh yes you're completely right. Let me think of another solution.""
- Seek out a mentor. Someone that works on things that are interesting for you personally. 
- Don't jump to making solutions. A lot of people do it. Even if you have a ticket with everything detailed out, take a few minutes to chat or message the person who created the ticket. Ask them the context, why they need you to work on something. What will it serve if you do it. Maybe you'll learn / discover something. Maybe you'll understand the business and stakeholders better.",1
1qwwgeo,1,"I have few things that I can discuss with you considering the overall situation, feel free to connect",1
1qwwgeo,2,"MIS programs are wildly different, some are very hands on technical and some are project/product mgr.

Masters are also situational. You either do them to party longer in college, to escape a bad major, change a career path you‚Äôre unhappy with, augment your skill set or because your job/future career path expects it.

If it‚Äôs the latter two you should ideally be seeking a masters/mba from a higher regarded college/program and not a degree mill that you pay money for largely zero actual knowledge. 

MIS will have overlap with what you‚Äôve already studied so truly new skills are going to be few. Which begs the question why spend that money and time when you could be making money and growing your professional experience instead.",2
1qwwdsl,1,Mainly I am looking for an alternative to PIT ( Point in Time ) feature of Elasticsearch Index . which freezes the data in segments until task is done . An Alternative to that for Starrocks Views,1
1qwvy5y,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qwvy5y,2,"Joe Reis is dropping a book on data modelling in 1 month, if you're patient enough...

Also he has a whole substack about that book if you can read from a PC screen without having your eyes bleeding

https://practicaldatamodeling.substack.com/",6
1qwsx9s,1,"The first one is pretty good if you never work on a real DE project. The course provide enough basic knowledge about pipeline, orchestration and CI/CD  to help with your projects. Don‚Äôt know anything about the second one tho",3
1qwrxe8,1,"If people who used notebooks in prod could read, they would be very angry right now.",171
1qwrxe8,2,"Quality is subjective.

If ""most prod jobs"" are for dashboards that nobody uses, who cares if the data is consistent, interpretable and accurate!",37
1qwrxe8,3,Well databricks ‚Äúnotebook‚Äù aren‚Äôt literal notebook.,51
1qwrxe8,4,"Whether its a notebook or not isn't really the issue.  The issue is whether there is a proper version control, change control, and rollback process.  Notebooks *usually* don't have that in practice.  But you can do VC, CI/CD, and testing with notebooks.  If you do then there's nothing wrong with using them in prod.


Some more thoughts: The issue that I usually see in practice is that once you start introducing these things then notebooks' convenience is reduced, so there's a lot of resistance against controls that prevent people from just yeeting something into production.  And once you let people yeet their non-important work, its only a matter of time before people start yeeting important work.",10
1qwrxe8,5,"I'll give you an example of how we use synapse notebooks (had no say in technology used, but generally it's enough for our needs) which are orchestrated via pipelines and triggers:

* all processing logic is maintained in a custom python library covered with unit tests and a few integration tests, separate from transform logic
* this custom library has very minor dependencies on Azure (only a few generic functions) and could be migrated to databricks or something similar if necessary
* all notebooks import the library and use same flow, so everything is familiar from notebook to notebook
* simple transformations are mapping based
* more complex transformations are implemented in the notebooks but they can only output a dataframe (later handled by processing module), they can't write to the env directly or depend on some global variables (in some cases wrapper functions can be used to circumvent that)
* changes are committed and deployed using CI/CD
* development and debugging is generally done directly in the notebook but has no effect until it ends up on the main branch and becomes a part of the project
* in most cases when something fails, it's related to env or env specific data and most convenient way is to debug it via notebook which is already part of the isolated workspace and connected storage accounts

Do you think there's anything wrong with this workflow and how would spark jobs improve it?",10
1qwrxe8,6,You said notebooks - I heard HTML/CSS,5
1qwrxe8,7,"I will refuse to share my demo notebooks for exactly this reason. ""Obviously it'll need to be productionalized first.""",2
1qwrxe8,8,I like using notebooks for the entry points. Notebooks help explain the pipeline / job better than just plain code.,2
1qwrxe8,9,"I export as a .py and the script does the same thing, I just rename it as prod. That isn't what others do? Notebooks are for dev I thought",2
1qwrxe8,10,Most?? No prod job should be a notebook,2
1qwn9ak,1,You guys are documenting business logic?,47
1qwn9ak,2,"I'm not a fan of capturing what code is doing in documents. The where clause clearly defines what it is doing.¬†
I prefer capturing why.¬†


So very similar to yours but I'd only capture why you're filtering for those countries and category in this model.¬†",14
1qwn9ak,3,"Depends. I would document the reason why in a comment above. (e.g. decided by manager x to only use x and y). But the code also is a way to capture business logic, so don't go overboard repeating.

 If it's possible I would add a data test (isn't really possible in your example). If you kept the column you could add a dbtutils accepted values for example.",8
1qwn9ak,4,"We do both. We make inline documentation and also we generate documentation for dbt models using LLM. In our case we use Cline or Claude Code. 

We have a specific workflow for each type of model that the LLM follows to generate the docs. 
Moreover, we instruct the LLM to produce the same structure of documentation for each type of model.

For example, curated (mart/gold etc.) models are directed more for the end user and contain sections such as purpose, business use cases, metrics ect. So not too technical. 

Intermediate models ,i.e., models used to split up complex transformation logic are documented with the developers as an audience, so they are much more technical and basically explain what we are doing CTE by CTE.

For most models #Business Logic is a section where we explain the specific business logic/rules implemented. 

The LLM infers these business logic by lopking both at the RAW query and inline comments. Also if it detects a business logic from a query, we instruct it to add short inline comments explaining it. So it goes both ways.

Using LLM to generate documentation increases the doc coverage in our project and increases the uniformity of documentation structure across all models. 

In turn this documentation is then indeed used to power AI Agents for Data Discovery - think chatting with agents to discover which/where data is stored to answer specific request, so they don't answer the question, they just point the user to where they can probably find their answer. It may be also a documented exposure, not model per se. 
And eventually it could also power AI analytics, where they actually access a semantic/metrics layer or directly query the database based on all of the context. 


So yes, we document business logic and use LLMs to do so for uniformity and completeness.",8
1qwn9ak,5,"I err on the side of over-commenting. Any bit of code that would raise the ""why the hell did we do this?"" question by another dev or you in 1-12 months should be commented. 

For the record, we don't have SQL absolutely loaded with comments, just for cases where it's not obvious in the model name why we'd be filtering to ""tech"" for example.",2
1qwn9ak,6,"I think the table description is the best place to document this, the filter is affecting the whole table.

Maybe you can make the filter section a bit easier to scan:

    models:
      - name: gold_top_sales
        description: |
          Monthly sales for top countries and top product category, as defined by business stakeholders (reviewed every 3 years).
    
          **Filter (included rows):**
          - `country` in the defined list of top countries (e.g. US, GB).
          - `category` matches the selected top product category (e.g. tech).

  
Another suggestion would be to avoid hardcoding the countries and the category, but put them into two seeds `top_countries` and `top_category` (these could become just models in future) and reference them in the query, so it's clear that you are picking the top countries and category and from where.",2
1qwn9ak,7,"lol, just join the bandwagon and promise documentation and never deliver. Let's face it. The only person who's going to read it is your replacement, and they're not going to do it because it's going to be out of date.",2
1qwn9ak,8,Are you looking for a semantic layer?,1
1qwn9ak,9,"So to preface - my team has a documentation setup inside DBT that is dedicated towards feeding the information into an LLM that our BA's utilize to ask questions of instead of us.

So for us it would look something like this

    models:
    - name: gold_top_sales
    ¬† description: Monthly sales on our top countries and the top product catergory defined by business stakeholdes every 3 years.
    ¬† columns:
    ¬† - name: country
    ¬† ¬† description: >
    ¬† ¬† ¬† ¬† ¬† Two letter country code for location the sale was placed.
    ¬† ¬† meta:
    ¬† ¬† ¬† display_name: Country Code
    ¬† ¬† ¬† canonical_for: [""country"", ""location"", ""country code""]
    ¬† ¬† ¬† synonyms: [""nation"", ""region""]
    ¬† ¬† ¬† source:
    ¬† ¬† ¬† ¬† model: bronze_monthly_sales
    ¬† ¬† ¬† ¬† column: country
          filtered_on: true
            filter_values: [""US"", ""GB""]

The caveat here is that DBT Cloud in recent weeks has decided it doesn't like our Meta tag. Right now it's just a warning, but I'm concerned that there's going to be a breakage at some point soon.",1
1qwn9ak,10,"I think well written SQL will usually do it on its own, unless your source schema is an absolute nightmare. That said, now with AI and a bit of SME input, you can probably find a way to document the high level business logic/rules in short order...",1
1qwleru,1,"I'm every place I've worked data PMs have been an utter useless waste of space and at least in my experience a good EM or Lead can do the majority of the planning. This is specifically for data teams - PMs have their place at an org level. 

The existence of PMs in a structure which already has directors, EMs, Team leads and Architects is a red flag to me and signals bloated org politics. 

My happiest and most productive part of my career was a brief period where the majority of the PMs were out on maternity leave by some miracle.",8
1qwleru,2,"this is pretty common lately, a lot of pm glue work has slid onto engineers. when no one owns intake or prioritization, every random thought feels heavier than it should. it usually only improves once a clear owner for planning exists, even if it is informal.",1
1qwk3ci,1,Queries are already text,12
1qwk3ci,2,So text to SQLGlot?,3
1qwk3ci,3,NO,3
1qwk3ci,4,"This is essentially natural language querying. Ive worked on a similar problem, and it primarily boils down to the 'scale' of data you want to query, along with other factors, from the number tables in the DB to the complexity of the data, everything becomes a pain in the ass. 

Solutions like Amazon Q or MS Copilot work very well with small, less complex and relatively simple data, theyre able to provide accurate results and build spectacular dashboards, however as soon as you try to ""plug in"" real world data, it all goes to shit, at least in my experience. ",2
1qwk3ci,5,I guess I don‚Äôt see the difference with just using any LLM to spit out a query for you.,2
1qwk3ci,6,I think this is a great idea. Just make sure you document that you need to be super precise in how you use natural language - maybe think about standardising a particular format and set of keywords? Just off the top of my head a user could prompt something like 'select this data from this table where this condition is true'.,1
1qwjl09,1,In my project the trend is to bring the gold layer tables from a different datawarehouse into the bronze layer of the current in development new datawarehouse.,3
1qwjl09,2,"I would be questioning the rationale for the language change, for the same reason you are.  Lineage is a lot easier if you don't do that translation.

It might be its own pain, but I'd probably suggest that Silver should be Polish named tables, with English named views on top of them to do the translation.  That would at least keep your translation front and center in one place and avoids renaming in your pipeline.

It",2
1qwjl09,3,People who don‚Äôt understand the consequences of some architectural decisions they make and stick to it no matter what and put in that trusted position to make these decisions tells you something about a companies culture.,1
1qwfjfr,1,"We have been using Lakeflow / Declarative Pipelines (DLT) for a while and some use cases it works great. But end to end we haven‚Äôt found a use case where it works without issue. We have use an append style to Bronze and a cleaned up version to Silver mostly matching back to source. 

You are going to find out there are a lot of caveats and areas where it won‚Äôt work as expected. We have also run into lots of areas where it fails for one reason or another and the default answer is to fully refresh the tables. Which if you are trying to keep history that is all blown away. 

That said we are still using it and the pieces that work work well but it is still not a 100% solution.",1
1qwfjfr,2,Don't expect to see the polished experience provided by Fivetran in Lakeflow any time soon if at all.,3
1qwfjfr,3,"Hi u/Vegetable_Ad8136, I'm on the product team at Fivetran. I'm sorry to hear you've had a rough experience. Would you be willing to share more about it with me? You can get ahold of me via DM here, create a support ticket (just ask for an intro to Dan Lynn and link this post), or you can ping me on linked (username danklynn) as well. Thanks!",1
1qwdlph,1,üëç,25
1qwdlph,2,üëç LGTM,7
1qwdlph,3,üëå,6
1qwdlph,4,üëç,4
1qwdlph,5,üëç,3
1qwdlph,6,üëçüèª,3
1qwdlph,7,üëç,3
1qwdlph,8,"Always choose the option with 

Little to no maintenance once setup 
Time spent vs time saved

First one is key.",6
1qwdlph,9,üëç indeed,2
1qwdlph,10,üëç,2
1qwd5of,1,"We've been using DuckDB in production for a year now, running and generating the queries we need with Python code.  

So far it's gone great. No major problems.

We switched from developing new pipelines in PySpark to doing so with DuckDB mainly on the basis that:

1. We observed that the actual data loads we were processing were never big enough to necessitate a Spark cluster.
2. Getting rid of Spark meant we could get rid of the whole complexity of running a JVM using the massive collection of libraries Spark requires (with all of their attendant security vulnerabilities) and replace it with a single, dependency-free DuckDB compiled binary.
3. When we tested it against Spark on our real data it ran about 10 times faster and used half the resources (and _yes_, I'm sure the Spark code could have been optimised better, but that's what our testing for our specific use-case showed).

Point 3 was the major one that allowed us to convince ourselves this was a good idea and sell it to management.",105
1qwd5of,2,"It all depends on the size, complexity, and purpose of your stack. In my case, we use DuckDB to detach some queries from Snowflake that even with the smallest compute engine size, would be an overkill, so it's very useful with our processing pipelines. Aside from that, DuckDB is fantastic for Data Analysts, as they can make use of their computers instead of draining resources from the DWH. We also use it in its WASM version as part of the Evidence.dev stack, which nurtures a lot of our dashboards.",24
1qwd5of,3,"Yes, heavily using DuckDB. We work with less data than most companies here I suspect, enough that tables used for analytics can be loaded into instances of our webserver in-memory for extremely quick data analytics on the front end.

So each instance is a Docker image running Django and periodically redownloading the latest DuckDB file (which is an output of our data pipeline elsewhere) and then allowing for views to be constructed via direct access to DuckDB.

I've been thinking about building a proper database driver between Django and DuckDB but for now, a combination of generating direct SQL and using polars have given us everything we need.",11
1qwd5of,4,"I do. It's very useful although currently only a very small part. Traditionally my company uses sql in database but seeing the performance benefits of duckdb, my company is planning on using a data lake like delta lake and duck DB to do the processing

Currently my biggest issue I'm trying to figure out is how I want to update the data in delta tables because I'm mainly using polars to insert the data. I don't really have much experience in this but if anyone has any tips on how I can update delta tables using polars instead of pyspark I am all ears",3
1qwd5of,5,If you‚Äôre using a severless function for some lighter weight ETL it can be used.,4
1qwd5of,6,"We use DuckDB in production. Our dwh is Snowflake and I built a tool that runs worksheets (series of SQL statements) in Snowflake with little templating (Go text/template library). Some workloads started using Snowflake as an engine - in worssheet query from s3 and copy back to s3 immediately.

Then we added support to DuckDB instead, now all processing happens inside the tool, so paying AWS instead of Snowflake.

However, working with big parquets is still better in Snowflake - maybe it‚Äôs me, but ‚Äúselect from s3://prefix-with-parquets limit 100‚Äù hangs in DuckDB while taking 100ms in Snowflake.",2
1qwd5of,7,"we use it for ad-hoc analytics and local development but not as a primary production db
the sweet spot ive found is:
	‚àô	running queries against parquet/csv exports without spinning up a full warehouse
	‚àô	prototyping analytics pipelines before pushing to snowflake
	‚àô	internal tools where you need fast aggregations but dont need concurrent writes
the limitation is it‚Äôs single-process - no concurrent write access, so anything with multiple users writing data simultaneously is a no-go. reads scale fine though
seen some teams embed it in data apps where users query pre-built datasets, works great for that. but if you need a traditional multi-user transactional system it‚Äôs not the right tool
what‚Äôs your use case? might be able to give a more specific take‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",2
1qwd5of,8,"using it for (ELT)->L

canonical on duckdb then load",2
1qwd5of,9,"Yup! Using it as a sink for data when I have to pull user information from Active Directory, a website, and another user directory. Have to reconcile all three to make sure they match or certain exceptions are met. It‚Äôs real nice to front load the LDAP query and not have to deal with latency unless I need to reach back out to Active Directory.",2
1qwd5of,10,Hex.tech is using it in analytics layer as in memory db. Works v ery nice,1
1qwd3bg,1,How does it compare to AWS AppFlow which is quite affordable and easy to set up to sync data from Salesforce into S3/Athena?,2
1qw4d41,1,"You can find our open-source project showcase here: https://dataengineering.wiki/Community/Projects

If you would like your project to be featured, submit it here: https://airtable.com/appDgaRSGl09yvjFj/pagmImKixEISPcGQz/form

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qw4d41,2,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qw2p4x,1,"Every company is different but I've certainly found value in it. I'm 8 years in at a company where I was the first DS. I'm now the Sr. on a team of 4. The company has tripled in size since I was hired. Lately we've been able to scale up our data infrastructure and build things I would not have dreamed of when I started. Many of these things are multi-year projects.

It's not the way to truly maximize your earnings, it's true that the biggest raises come from job hopping. But there is a lot of value in stability, and having the opportunity to ""go deep"" on your particular industry, tech stack, problem type.

The field does move fast but that's more about your toolset and workflow. Expertise is cumulative. I never feel like I'm ""falling behind."" When you work on the same kind of thing for years it's not actually hard to be aware of the state of the art. A big part of the job of a DS is to find the right tool for the job.",38
1qw2p4x,2,"I would say only if the company makes money and has leadership that will listen to constructive criticism. You need the money for investment, then you also need the leadership so your projects don't get shut down because no one understands the value it may bring.",10
1qw2p4x,3,"Yes, at some point you simply can‚Äôt take on bigger projects unless you‚Äôve been around long enough to see big projects play out. 

Ideally you find a place you like that allows this plus new stuff along the way.",6
1qw2p4x,4,"Depends on what stage of your dev career.  
  
Regardless of stage; a helpful framework is ‚Äúlearn, earn or quit‚Äù. https://youtu.be/eLelgy5zRv4     
To summarize - any job should be fueling your learning/skills, paying you big bucks, or both. If, by your own measure, your job isn‚Äôt helping you learn or earn then it‚Äôs time to go. Doesnt matter if you‚Äôve at a job 2 months or 2 years. You need to determine this for yourself.   
    
That said, if you‚Äôre early in your career, I see no problem with job hopping to get into better roles (learn more or earn more). There is not strict time limit - if you have a new job offer in hand, then it doesn‚Äôt matter if you were at your previous place ‚Äúlong enough‚Äù.      
However, if your current stage is promo from senior ‚Äî> staff or switch from dev ‚Äî> management, you usually need to do that by staying at the same company for a while. You usually won‚Äôt get that promo from switching companies.",3
1qw2p4x,5,"There's no one size fits all answer.

  
For every hiring manager that thinks consulting is good because it exposes you to lots of things, there's one worries they'll have to deal wtih breaking your habit of delivering stuff 75% complete and dlieverables that check the box but don't scale at all.

  
If you want to get into exec roles later in your career - showing you prepped data for funding rounds, working with exit migrations, etc. helps.",4
1qw2p4x,6,"This is a broad ask and super dependent on a lot of factors.

The basic answer is 2-3 is fine but keep a line in the water. Sometimes after 2-3 years, data isn‚Äôt a priority anymore for the business, the business is cutting costs, or the fun part is done and/or someone else would pay you more to do what you‚Äôve just gone through.

You can easily stay if none of those factors are true or if it‚Äôs stable and you‚Äôre personal life benefits more (raising kids, manageable hours, good benefits)",4
1qw2p4x,7,"There is value in it. You'll see how robust decisions from a few years ago were. It's an own skill, but greenfield projects are more common + pay raise is normally worth hopping. As always, a trade-off.",2
1qw2p4x,8,"I‚Äôve found there‚Äôs real value in staying long enough to see second and third order effects. The first year is usually pipelines and quick wins, year two is reliability and trust, year three is when you see whether the org actually changes how it works around data. You learn a lot about trade-offs you never hit in short projects, things like ownership, on-call pain, and what ‚Äúdone‚Äù really means at scale. The downside is skill atrophy if the stack stagnates, so you have to be intentional about pushing upgrades or side projects. Consulting keeps you sharp on tools, but you miss the long tail problems that only show up after systems have been running for a while.",1
1qw2p4x,9,I‚Äôm a rookie but I feel like I need time to develop technically I get paid well and have good work life balance and work remote no idea if I should push 3-4 or leave at 2,1
1qw2p4x,10,"Value resides in where you believe it resides. 

Every person will value things differently, and weight different aspects of their role differently when evaluating how much they like their current position. For some, getting in on the ground floor and seeing an entire IT department grow from excel sheets to cloud based processing is valuable to them, so they choose to stay at a company for 10+ years to see that through. For others, hopping around get exposure to new tools and bump up their salary each job change is what's most important to them. It could even be as simple as you enjoy the people you work with so much that it doesn't feel like work at all, so you choose to stick around for your whole career. I envy the person that can do that. 

You go after what you value, and make sure you commit to it.

I've always had respect for folks that have many years in the industry or the company I've worked. I think they provide immense value to the big picture of the organization, share insight to industry trends, and offer decades of experience that you don't find in a textbook. 

With that being said, it's important to know when it is your time to move on. If working at the cutting edge of tech is what you value most, and your current position doesn't offer that, then you know what you need to do.",1
1qw09c0,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qw09c0,2,"That sounds like a great way to create some really toxic team dynamics!

Remember, people will optimize for what you measure.  So you better make *really* sure that what you measure is what actually adds value, or what you want people to be focused on.

Doing something around story points is awful, but is one of a small number of equally bad choices.  You can minimize the harm by equalizing story points available across the team, and using discretion to add points (or subtasks) dynamically for things that wind up being more complex than estimated.  It still puts a LOT more pressure (somewhere) on estimating and updating points.  And you will still get people trying to cherry pick tasks with the most ROI (points to level of effort) or pad the points numbers.  Have fun with that!

I would still rather have a performance review from a human who knows all the nuances of what I did and why.  That way I can focus on doing things we (collectively and dynamically) agree matter, and not worry about gaming metrics.  Then it is upper management's job to make sure those leaders are good at what they do, and my option to go elsewhere if they prove not to be.",47
1qw09c0,3,Smells like laying groundwork for stack ranking ‚Ä¶ and bottom-trimming. üòÇ,8
1qw09c0,4,"I am the sole engineer with a team of about 10 active analysts across departments who know SQL and use our new data warehouse.  (trying to keep the context brief)

So take my response as someone basically shooting from the hip:

Bring it to your team and decide together.

Just be straight with them that you've already objected and have been overruled, and you want input from your team so you can make it as fair as possible.

I assume leadership wants the metrics to be presented in a style they're already used to and may already be defined.

This is probably a situation where you're guaranteed to end up with someone who is upset.  But at least you can sleep knowing you tried your best to be fair and respectful to your team.  And they should appreciate that you gave them a chance to suggest things that affect their livelihood.

Also, will you be able to try things?  ""Ok, let's try these KPIs for 3 months and see if I agree with them as the manager or whatever"".  Your call if they'd be impersonal enough to present to your team as well.  

If you feel like the KPIs suck, at least you'd have a starting point.",11
1qw09c0,5,"If I were a DE on your team I would start looking for a new place to work at the first sign of this. If you're using Agile methodologies right now, and that's something that your leadership claims to want, you need to be able to explain to your leadership why this ask is incompatible with Agile. Measure the product, not the people. Your leadership should care about things like uptime, error rates, and performance/cost improvements. I guarantee that fulfilling your leadership's ask here will result in a worse outcome for delivery and more employee turnover, but hey, it will also get them the pretty numbers they want, so I guess they can count that as a win. ",3
1qw09c0,6,"I second the idea about brainstorming with the team. As a manager, i would want to know my managers KPI and my teams KPIs.

Then connect what you can to the individuals. 

My team had people who are deep in one aspect or another. So I would highlight the things they  deliver on specifically. The define how that impacts the team KPI. CICD expert. They provide risk mitigate and reduce deployment process time. 
KPI all jobs deployed from devops",1
1qw09c0,7,"I know it's controversial, but where are you on DORA metrics?",1
1qw09c0,8,Go to your leadership and tell them that you're only doing team kpis,1
1qw09c0,9,"I measure my team based on fuck ups.

Fuck ups happen and in many ways I like it when the safe fuck ups happen - it means someone tried something, were daring, brave, different.

When unsafe fuck ups happen, still got their back but until someone else claims that crown, you will be wearing it.",1
1qw09c0,10,"The challenge you're running into is real - uniform individual metrics in DE teams almost always create perverse incentives.

Here's what I've seen actually work vs backfire:

What backfires:

‚Ä¢‚Å†¬† ‚Å†Story point velocity becomes a negotiation game immediately

‚Ä¢‚Å†¬† ‚Å†Lines of code or PR counts incentivize wrong behavior

‚Ä¢‚Å†¬† ‚Å†Ticket volume comparisons ignore invisible work (mentoring, architecture thinking, debugging others' pipelines)

What tends to work:

‚Ä¢‚Å†¬† ‚Å†Business context connection: This is the most critical element. How well does the engineer understand the business problem and translate it into data solutions? The best DEs aren't just pipeline builders - they're translators between business needs and technical implementation.

‚Ä¢‚Å†¬† ‚Å†Pipeline health ownership: % of your pipelines with <2% failure rate over rolling 30 days

‚Ä¢‚Å†¬† ‚Å†Data quality resolution: time-to-fix for issues in domains you own

‚Ä¢‚Å†¬† ‚Å†Impact metrics: each engineer proposes 2-3 measurable outcomes at quarter start, manager approves. Gets leadership their ""individual accountability"" while acknowledging that DE work is inherently diverse.

The root issue is often that leadership wanting uniform metrics doesn't trust qualitative judgment. Sometimes the real answer is educating upward on why engineering measurement is different from sales quotas.",1
1qvz9ql,1,"This is a very common growing pain. Since you are already landing raw data in Redshift, you are perfectly positioned for an ELT pattern. 
IMO answer here really is dbt. 
It handles managing dependencies, testing, and creating those Silver/Gold layers using SQL. Since you already use Airflow, you can use Airflow to trigger dbt jobs after the raw data lands.

still more info needed about volume & target latency for business. 

Looker works best with wide denormalized data and can‚Äôt or shouldn‚Äôt do heavy lifting.",9
1qvz9ql,2,"What kind of data are u capturing? Telemetry, user behavior, transactional data? Generally you should strive to ensure quality at the finest granularity. A common pitfall is cleaning data during the aggregation step or in a downstream data store (e.g. redshift).",2
1qvz9ql,3,"I've used snowflake and Teradara before and based on what I've experience, snowflake is super fast for analytics and growing data and easy to debug. 

You can add a cdc layer or like (core tables -> semantic tables (processed and cleaned data) -> views ) and then feed the views as data source for the looker dashboards.",2
1qvz9ql,4,"To ""improve sophistication"", you should add a layer of data observability. It covers data quality and ETL/ELT job monitoring. Some tools also have metadata repository (catalog) and governance (lineage, etc) built in.   
You start looking sophisticated but also nip the data problems in the bud.",2
1qvz9ql,5,How much data do you have to process daily?,1
1qvz9ql,6,"Yeah, ELT with dbt sounds like a solid move tbh.  It'll help you build those silver and gold layers for more refined data views.",1
1qvqvyc,1,"With 11 years of experience and currently working in a product-based company, I believe that if you are not finding your current role interesting, it is better to switch to another product-based organization. Service-based companies usually involve working on a wide range of data engineering tools with limited depth, where deep specialization is not always required.",7
1qvqvyc,2,"Working in any Indian consulting company will make you kill yourself, no matter whatever they tell you, don‚Äôt do it - talking from experience üòÇ",4
1qvqvyc,3,"You can always go there, see how it is and if you won‚Äôt like it go back to some product company, maybe even Boeing if you play it right",1
1qvoj0i,1,"It's literally power bi

  
the idea there are $2bn of data engineering workloads running on fabric cannot be true",12
1qvoj0i,2,">forced migration

ah the Microsoft way",16
1qvoj0i,3,It would definitely validate what I read here on this subreddit,7
1qvjogq,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qvjogq,2,">**With this new AI era is it too late to get into data engineering at this point?**

I feel like this is getting asked pretty much every week at this point so I'd go through the subreddit and take a look at the previous threads.",7
1qvjogq,3,Comfy cosy sounds preferable‚Ä¶Wait perhaps until the AI dust settles,4
1qvjogq,4,"A few things there...

DE can get boring if you do it right. Actually every system. 

If the data is easy, AI can do it. 

Old companies have too many legacy stuff to just use AI. 

You have all you need there. Start learning with your colleagues and by yourself and then decide for the switch in 6-9 months.",3
1qvjogq,5,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qvjogq,6,"I would add more details to your post, current YOE, skills etc",1
1qvjogq,7,"I think AI is gradually reaching the point where it can replace a junior engineer, and it very closes already. I would take advantages of that and buy myself a subscription and learn DE with it. Let it generate code and teach yourself how to read and crique it. If you are not able to land a DE job in the future, you still ends up with skill to able to use AI in your workflow.",1
1qvizak,1,"Yeah... Their tools are also sometimes python scripts, written with AI (nothing againstAI for code but not like this) and are unmaintainable pieces of garbage...
All that for the small price of hundreds of thousands of dollars if not millions. Ask me how I know lol",191
1qvizak,2,"Well It was a 3 week project, and the IT said it will take 2 weeks to just get access to data. Also the exec only paid for a POC + a roadmap slides that can score him the next year's bonus.",46
1qvizak,3,Professional maturity is recognizing when to laugh when the consultant leaves the room. Does your boss need political cover for a decision that must be made? That is the only time to use the consultant. Nobody got fired when the consultants advice didn't work out. They're an insurance product.,43
1qvizak,4,"As a side question, does any of these companies do any good projects? I feel like they're a fraud at this point lol",15
1qvizak,5,"I work for a medium size consulting firm that often gets contracts to come and fix the ‚Äúwork‚Äù that these big companies sold. Often that means completely needing to redo everything from scratch. C-suite loves these big companies, but the directors have to convince the VPs to use us constantly to fix the big firms screw ups.

It‚Äôs a very expensive way to run a business.",11
1qvizak,6,Massive fuck you to all the consultants out there getting paid to literally churn out tech debt,11
1qvizak,7,Are you me?,4
1qvizak,8,"Ugh, all my homies hate BCG",1
1qvi7fu,1,Dagster is definitely up there as it has first class integration with DBT.,38
1qvi7fu,2,"Airflow on EC2 via docker compose? I know it's hardly production-grade deployment, but for a poc it would work. You'll have no trouble using SSHOperator to run your dbt commands where your dbt deployment runs, or BashOperator for local Airflow runs.",6
1qvi7fu,3,"We are using airflow and run dbt image with Kubernetes pod operator. So in dbt repo we build the image and in airflow we called the dbt run command for each models. The lineage is defined by airflow dag.

More proper way that I know is using dbt cosmos airflow extension that basically will compile all dbt models and automatically create the lineage",6
1qvi7fu,4,"Dagster is fantastic for this, I 100% recommend trying this first. We host Dagster in k8 with dbt configs that have automation conditions. Everything is incremental and updated as soon as dependencies are updated. Dev experience and UI are pretty intuitive too.

Airflow is another great option. It has been around longer than Dagster and functionally, it will do the same thing. It really boils down to personal preference between those 2.",5
1qvi7fu,5,"What is your setup?   
In which cloud are you?  
Do you have a K8S cluster?  
How much do you know about Airflow?  
Do you need to run other things beside dbt?",3
1qvi7fu,6,"I‚Äôm running Airflow + Cosmos + dbt in production hosted on multiple EC2 instances as separate environments. No docker, everything from PyPi running in uv venvs. 

Tbh it‚Äôs hard mode because we‚Äôre a government department with a proxy that blocks docker hub, not allowed to use any SaaS because security. Plus a bunch of other annoying policies that forced me to choose this path. The other option I am comparing is Dagster OSS.

But the running costs for the whole thing is like 100/month.",2
1qvi7fu,7,In my previous company we were using AWS managed airflow(MWAA) with dbt-athena adapter the cost was less and set up was also easy,1
1qvi7fu,8,"I went with Argo Workflows because we already had good Kubernetes infrastructure. Works great for us. Have to set up some hooks to capture job info in our database for operational use, but normal logging got us pretty far.",1
1qvi7fu,9,Bitbucket Pipelines or GitHub Actions can be a very easy path.,1
1qvi7fu,10,"Hi, I am adding DBT support to https://dagctl.io. It is built on k8s and adds in all of the nice to have developer experience things that are a slog to build and maintain internally. We launched initially with support for SQLMesh. We are aiming to be an alternative to the outrageous pricing of DBT cloud. I would love to pick your brain about your DBT workflow and how you envision running it in prod.",1
1qvi7ae,1,"Data Governance is a job itself. People don't take it seriously. And nowadays, nobody wants to own nothing, titles, services, DBs... How can one gocern what they don't understand and they don't take responsibility over?",7
1qvi7ae,2,"There's fragmentation in the tool chain because most of us migrated away from all-in-one solutions like Informatica or DataStage years ago because they promised all of those things under one umbrella but delivered half the desired quality.

Each tool in the tool chain nowadays is usually considered best-of-breed in its given category. Designing solutions that bring out the best of each tool and wiring it all together into a comprehensive platform is what you hire good Data Engineers for. Personally, I would much rather be tying together disparate tools that excel in at least one category rather than being forced to use an all-in-one commercial solution that excels at none of them.",6
1qvi7ae,3,"No, it's fucked.",1
1qvi7ae,4,"All in one tools have and still do exist yet none have risen to the top as the silver bullet. We‚Äôre talking about trying to fix a people/process issue with technology. If bad data governance processes exist and/or people are not educated to effectively uphold those processes, then technology won‚Äôt solve the problem alone, it requires all three.

We use dlt/dbt/Power BI/Dagster, but each process is well defined and understood, and we don‚Äôt have many issues.",1
1qvi7ae,5,"Governance costs money and slows things down.
I won‚Äôt say that nobody gives a toss about it, but not a lot do.",1
1qvi7ae,6,"Disclaimer: I‚Äôm the founder of bruin.

This is basically the pain that pushed us to build it. Without some kind of framework, the modern stack is just hard to keep sane.

You‚Äôve got orchestration, ingestion, dbt, data quality, catalog‚Ä¶ all different tools, all doing their own thing. You end up spending more time wiring stuff together than actually trusting the data, and governance is always the thing you promise to ‚Äúadd later‚Äù.

What worked for us was putting everything in one place. Governance stops being a separate project, and the bonus is that AI can finally use the context properly which speeds teams up a lot.

Without a proper framework around it, it's not an easy job to do it. In modern data stack, there are lots of tools that you have to bring together. Orchestrator, Ingestion, Transformation, Data Quality, Data Catalog, etc.",0
1qveu1c,1,"If the companies cannot get their money worth, expect further salary erosions.",10
1qveu1c,2,"120k is really low for senior, which companies are you seeing that for?",4
1qveu1c,3,"not sure where you live, but could it be location based? some companies have changed comp structures with RTO mandates and if you aren't in the top markets you might get less, whereas many companies didn't differentiate.  ",1
1qvd8zc,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qvd8zc,2,"If you dont have a lot of data personnel, and not in capacity to hire. 

Stay away from Open source tools as those are best when good in house maintainers are available. Else they end up blowing the systems.

Be it code based or gui based, level of expertise needed is almost the same. GUI based tools have huge upfront buy in cost. If less complicated data work, then prefer using SQL with DBT combo.

You need 1-2 data engineers, with company your scale, there need not to be hierarchy. The existing project manager managing the analytics team can operate for requirements.

Choice of lakehouse or warehouse depends on what kind of data is there, more pdf and unstructured then you need a combination , most cases the warehouse is sufficient, OLTP syntax are pretty much same in warehouse systems.

Use DBT docs for documentation at start, later move to governance tools. Snowflake also has catalogs now and data bricks has unity. 

Use of roles in snowflake is best bet for this audits etc

Learn python + sql",1
1qv7786,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qv7786,2,"Did that move about 3 years ago, can share my 2 cents.

Why I switched: I realized the parts of DE I enjoyed most were modeling data for analysts, arguing about metrics definitions, and making dashboards actually useful. I cared less about perfect infra and more about ‚Äúare people making better decisions with this.‚Äù Analytics engineering was basically that full time.

Day to day difference: as a DE I spent a lot of time on infra, pipelines, performance, random firefighting. As an AE I‚Äôm still writing SQL and some Python, but I‚Äôm way closer to the business. More time on dbt models, tests, documentation, semantic layers, and collaborating with PMs / analysts. Less Kafka / Spark / k8s, more ‚Äúwhat does ‚Äòactive user‚Äô actually mean and can we stop changing it every quarter.‚Äù

Step up or step down: it‚Äôs more of a fork than a ladder. If you like infra and scale problems, AE will feel softer. If you like modeling, metrics, and being involved in product / strategy chats, it feels like a step up because your impact is easier to see. Comp for me stayed roughly the same, slightly higher later because I became the ‚Äúmetrics owner‚Äù and that‚Äôs valuable at product‚Äëled companies.

Branding / career path: I brand myself as ‚Äúdata platform + analytics‚Äù now. Titles I see after AE: senior AE, analytics lead, data product manager, head of data, sometimes even PM for data products. You become a good candidate for roles where you‚Äôre the bridge between pure engineering and pure analytics.

Gotchas:
You will write a lot of SQL and stakeholder‚Äëfriendly docs.
You‚Äôll spend more time in meetings explaining data than tuning clusters.
If you hate ambiguity and politics around definitions, it can be frustrating.

With 8 years as DE you‚Äôre probably overqualified on the tech side. The real question is whether you want to move closer to the business and be judged more on clarity, communication and ‚Äúdoes this metric make sense‚Äù than on how elegant your pipeline is. If yes, AE is a pretty solid move.",65
1qv7786,3,"I have done it all, SDE to BIA to BIE to DE to AE now. Consistent thing across all the roles is having good grasp on business context and basics.",15
1qv7786,4,I think for every 4 DEs there is one AE. I have seen many teams working like that. So less chances to land a job.,16
1qv7786,5,"I have maybe a different perspective.. I was a full stack dev (DE/AE) for 10+years and recently i was moved to AE specific role as part of a reorg. I still do many DE tasks, but not nearly as many as before. Because I can do both, when I need data from a new source, alot of the time ill write the process and send it to the DEs to deploy. 

I work with a DE only people and ive noticed a here a huge difference in design patterns. Their solutions tend to be one off purpose built imports. They almost never parameterize their solutions and any meta data used is typically a json file in the app and locked behind version control.

When you are tasked with modeling an enterprise asset, you're forced to figure out ways of genericize your solution to support future initiatives. Alot of times this means data driven solutions for DE tasks... and I dont see many DE only peers that use data to drive their pipelines. 

This is a great opportunity for you to *up-skill your DE game.",6
1qv7786,6,What is analytics engineering?,8
1qv7786,7,I really enjoyed AE but just started DE so we will see how it goes,1
1qv7786,8,"It is innate on my last scenario/position.

All analyst resigned. Only me and my Manager was on survival mode in BI Dept.

It lasts about 3 years before the dept dissolved, and our team absorbed by Data Science Dept.

The experience being AE is superb. It's like I know how to get/transform data and read/analyze the data.

The experience of knowing what's the problem of the company, revenues, KPI Metrics or any business problem and having a solution and recos for it is one of the best exp I had.",1
1qv7786,9,What are the DE roles doing in organisations with a DE/AE split? Just looking after ingestions?,1
1qv7786,10,AE is mostly a click-n-drag engineer.,-16
1qv74oi,1,"honestly this is the right call if the client is risk-averse. schema evolution in bronze sounds nice in theory but in practice it can introduce subtle issues downstream, especially if silver/gold transformations assume a fixed schema. treating new columns as change requests is more work upfront but way easier to reason about when something breaks. the one thing i'd watch out for is that one-off backfill pipeline, make sure you're not accidentally duplicating records if the CDC stream already captured some of those changes.",2
1qv74oi,2,"Seems silly to be so rigid on the bronze. Silver I completely understand, to an extent. But sometimes people dont listen to reason. 

  
That being said, I am at a company who had a bit more rigid landing, but we dont use CDC because reasons. Our tables are quite stable because they are afraid to touch them so that benefits us. However, we do bring in parquet files that get generated from a stream which gets way more new columns. The way I currently handle that is I check schema and if a new column shows up I alert for it and ignore it in the process so it can at least continue and do the job as the new field is rarely needed immediately for reporting. I basically have to handle that in a very similar manner, I update records for the most recent set of files with the new column data that did not get put in with the run today or whenever the column first came up.",1
1qv74oi,3,"Any way to build it dynamically? Read the schema from the source, change on the fly and load? If you need to know what changed and when this doesn't help but can be very helpful when trying to automatically keep everything in the data lake.",1
1qv5w7i,1,This is why customer support and customer success are crucial. Time to roll your own ELT.,43
1qv5w7i,2,"I empathize with you OP and while I think Fivetran sucks, all these tools in the ""connector"" space are varying degrees of bad so while I would recommend getting off of Fivetran because the amount you're paying is actually crazy, you will run into SOME issue with all of the tools.",43
1qv5w7i,3,Fivetran is predatory. The fact DBT sold to them made beyond sad.,27
1qv5w7i,4,u/DigitalDelusion \- I'm a PM at Fivetran. I'll DM you to get more details.,30
1qv5w7i,5,"that‚Äôs brutal, especially with reverse-elt where everything is chained together. cutting service with no past due notice or grace period is an ops failure, not a billing one, and 24 to 48 hours is a long time when prod depends on it.",6
1qv5w7i,6,"definitely sounds like some CSM somewhere messed up big time.  There should have been ways to reach someone at your company.  Shutting you off is bush league.

Personally I like Fivetran.  It allows us to focus on other DE tasks (and let them deal with changing API's...etc).   

However, if this happen to us, I'd insist that they provide us at least 3 months free usage and if not.  Look at alternatives or roll your own.",4
1qv5w7i,7,No customer service? How about no customers then.,3
1qv5w7i,8,"the 24-48 hour reinstatement after they screwed up the billing is wild. that's basically saying 'we broke it but you get to eat the downtime.' at minimum i'd be documenting everything for contract renegotiation leverage, especially since they confirmed the billing change in writing.",3
1qv5w7i,9,"This is crazy, as customer support used the be a huge thing for Fivetran. If that's gone all will they have is high prices.",3
1qv5w7i,10,"This is exactly where trust breaks. When your warehouse feeds everything, ‚Äú24‚Äì48 hour reinstatement‚Äù is unacceptable.

I‚Äôm one of the founders at SupaFlow. We‚Äôre an alternative to Fivetran focused on pipeline reliability, sane pricing, and real humans when things go sideways.

Not here to pile on ‚Äî just saying teams shouldn‚Äôt have to accept this as normal.",1
1qv3q0b,1,"If you're using dbt, don't you get column level lineage for free?",6
1qv3q0b,2,"The ML pipeline blind spot is real. We hit the same wall - notebooks that touch data just vanish from lineage.

One thing that helped was instrumenting our feature store with write hooks. Anytime a notebook writes to a feature table, we log the source columns + notebook path to a separate metadata table. It's manual plumbing but catches maybe 70% of what would otherwise be invisible.

For cross-warehouse, have you looked at OpenLineage? The spec lets you emit events from Databricks/Spark side too. Not trivial to set up but at least gets you a unified format across the estate.

Curious about your 80/20 caching strategy for common lineage paths. Are you precomputing those at extraction time or lazily caching on query?",4
1qv3q0b,3,"We have about half that table count. We use dbt, and it just tracks all that for us.

That being said I do pull down data types and precision for all columns to autogenerate and sync yaml in dbt. Takes like ~2-3 minutes to do so with the python snowflake core sdk's async iterators.

There's lineage tracking functionality as well native in snowflake enterprise: https://docs.snowflake.com/en/sql-reference/functions/get_lineage-snowflake-core",3
1qv3q0b,4,"sqlglot solves most of those parsing problems, seems like a mistake to implement it yourself.",5
1qv3q0b,5,curious why the schema/column change happening that frequently?,2
1qv3q0b,6,Can't iceberg v3 help with row level lineage feature?,2
1qv3q0b,7,"Wait for snowflakes acquisition of select star to be integrated, column level lineage was a part of their core features. Not sure on their integration timeline unfortunately",1
1qv3q0b,8,"Wrote my thesis on this - felt the same pain - regex > sqlglot. Legacy platform so I needed too many exceptions. What I ended up using was llm + tool call to info schema. Output to json and json to Memgraph. It was on a small data set, but I was then able to talk to my lineage. Eg. ‚ÄúTransformation failed, where should I look for bug‚Äù 

Nice to see that others are thinking on the same lines.",1
1qv0iyl,1,"Sorry that it didn't work out for you and thanks for sharing your experience. Some of these companies do not have a team selection process for DE and given how competitive the market is, you may have been second best to someone that they have already interviewed.

For the sys design, were they expecting a YAML or was that the agreed upon method to explain the design? How in-depth did you need to get into involving things that are not normally in the JD for data engineering like networking, security, shared resources, etc?",59
1qv0iyl,2,"They are looking for Unicorns.
Not normal folks",21
1qv0iyl,3,Design databricks in YAML was meant to be databricks asset bundle for running pipelines?,11
1qv0iyl,4,Design data bricks???? Bro I gotta switch out,8
1qv0iyl,5,"Even I had Doordash interview and I felt like I do good. At the end, I received a call saying I didn‚Äôt explain two points in data modelling round correctly.

In my few, designing exactly the same data model that interviewer have in his mind in one hour is not possible. 


After some permutations and combinations, i realised that they aren‚Äôt looking for someone who can perform. They are looking for perfect 10/10 candidate which seems fair given they are paying top buck.",13
1qv0iyl,6,How was the technical screen went? I am about to have a call with recruiter ?,3
1qv0iyl,7,"that sounds frustrating, especially the mismatch between what was tested and what the role likely does day to day. honestly feels like either the loop wasn‚Äôt well calibrated or they were already unsure what they wanted, which is rough but not a reflection of your performance.",3
1qv0iyl,8,India or us ?,9
1qv0iyl,9,"Run, dont walk. Did you see the whistle blower letter this past week? Who do you think designed the system that determines whether a contractor is ""desperate""? Do you want to maintain that system, that helps the company further exploit its own staff?

Edit - the story got debunked as fake, my bad",7
1qv0iyl,10,Thank you for sharing.,2
1quz6kq,1,"Interesting idea and relates to my current project but I also have a data dictionary. Did you integrate something like that along with it? I'm trying to think about how I'd use it, I almost feel like data analysts would be a better audience. Although our job descriptions change drastically from corporate to corporate",1
1quz6kq,2,any idea how gemini cli performs currently? super cool¬†,1
1quz6kq,3,I just use cursor with the bigquery mcp server.,1
1quxzvb,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1quxzvb,2,"The principle still applies. Spark will serialize the data and transfer to python process. If you run out of memory, you get no error, your task will hang indefinitely until you kill it. Had this problem on spark 3.5, but it should apply even to the newest version.

You can alter your session to have a bigger memory overhead. Look what each memory options on spark does. There is an option to increase the memory the JVM uses, and one that increases the memory outside the JVM (which the python process will consume)",33
1quxzvb,3,You can write python arrow udfs which are much more efficient,24
1quxzvb,4,"In my experience this rarely is a problem you will encounter.
But if you do writing it in spark scala is a band aid to a problem that you are kicking down the road.",9
1quxzvb,5,"Always. Avoid UDF if you can. However, if you must, it's fine to a degree. Not many people actually need optimization.",5
1quxzvb,6,"Yes, though less than they used to be.  
Native functions (the built in spark sql) functions are the best because of their inner optimizations, vectorization, and being able to be optimized by the planner

When it comes to python vs java\scala udfs, python is still the slowest since well, python is just slower, but most of the overhead that used to be a thing like serialization is less of an issue now with the usage of Arrow udfs 

Basically try to keep to native functions, but python udfs arent that much of a taboo as older guides might suggest. And unless it's some special performance issue, i wouldn't introduce scala\java udfs to a pyspark project",2
1quxzvb,7,Shouldn‚Äôt have this problem,1
1quxzvb,8,Have you considered using Ray for expensive UDFs?,0
1quwnj3,1,"It‚Äôs just duckdb, roll out your own version",31
1quwnj3,2,Their new ‚ÄúLite‚Äù plan pricing starts at $0/month according to your latest link. The ‚Äústarts at‚Äù implies they probably have cheaper paid options for slightly more features in line with their old lite plan,18
1quwnj3,3,That seem quite sensible. Was quoted 360 euro/month/SEAT for DBT Cloud enterprise. Never again will I suggest DBT Labs to any of our clients.,8
1quwnj3,4,"price jumps like that usually force you to separate ‚Äúwhat do i actually need‚Äù from ‚Äúwhat was just convenient.‚Äù for your size, a lot of the value motherduck gives is managed convenience, not new capability.

i have seen setups stay very stable by keeping duckdb local or in a small service, parquet in object storage, and being very disciplined about ingestion and freshness checks. grafana does not really care where the sql engine lives as long as it is predictable.

the big thing i would think about is ops overhead. once you leave a managed service, you are signing up to own upgrades, disk issues, and weird edge cases. that can still be worth it at 100 giB, but only if you are honest about how much time you want to spend babysitting it.

personally i would prototype the boring self managed path first and see if it actually hurts. many times the pain never shows up, and you avoid locking yourself into a price that no longer matches your scale.",5
1quwnj3,5,"Okay it seems that there is a lite plan, but as soon as you exhaust the free quota it goes to pay-as-you-go pricing. It can be cheaper or more expensive but just saying that they just remove the prosumer tier",2
1quwnj3,6,"For 100 GiB, $250/month is steep. Honestly, just run DuckDB on a small EC2 instance; it'll cost you peanuts. Or use AWS Athena directly on your S3 parquet files; you can integrate that with Grafana. Much cheaper, especially if query volume isn't massive.",1
1quwnj3,7,Aws s3 or Azure blob and python using airflow and clickhouse is all you need. Don‚Äôt rely on subscriptions. Better to have your own setup it takes time. But once you‚Äôre on a roll it will be easy. If you need help reach out to me.,1
1quwnj3,8,ClickHouse is still the fastest and cheapest option,0
1quwnj3,9,Hear me out: snowflake. For that data amount you'll never spend past the 25 a month minimum.,1
1quwnj3,10,Haven't you noticed what is the price of gold recently? We live in a FIAT world. People have to put food on the table.,-2
1quvm92,1,"It's a mixture right now, but yes a lot of the market is like that. It's just companies hear ""AI"" and think they just need someone to push a button but it still requires lots of knowledge and understanding how they need to cleanse the data and hyperparameter tuning and understanding what model works best for what and all that. There was a small period in time, and maybe it's still true, where it was just throw XGBOOST at it and go sip your coffee, and thats what a lot of folk were doing. But that does a disservice to a lot of the actual data scientists out there doing the maths and actually understanding inputs and outputs.

I guess I ranted a bit there, the short story long is, yes, that is common, and I typically avoid it if I can. But you sometimes have to do what the market dictates even if it's not sustainable.",21
1quvm92,2,"Yeah, it's going to happen eventually. Now management doesn't have to get involved in those pesky details that used to justify their pay. Hell mine can't even escalate problems correctly. It's like they put a quarterly PowerPoint together and sit back until quarters end and see how things shook out. They've successfully separated their individual success or failure from the teams success/failure. It's the most absurd thing I've seen and they're only getting away with it due to the shortage. I wouldn't put it past some conspiracy plan enacted to knock devs down from their high perch they got to during COVID, working multiple jobs in just 40 hour weeks. 

It's just the managers/leaders taking another step away from responsibilities",15
1quvm92,3,‚ÄúT-shaped‚Äù seems to be an increasingly popular buzzword. Deep knowledge in a one/few areas but broad enough knowledge to contribute and assist in other areas.,11
1quvm92,4,"I feel like we're all being asked to do more with less these days. Good in terms of learning opportunities, but obviously burdensome at times.",8
1quvm92,5,"instead AI/ML, I take care of Platform Engineering/CICD.",2
1quvm92,6,"Did you see my post from last week? that's what i'm seeing across 15% of the industry, and i think it's going to be the end state by end of year. Also AE is getting replaced - see the recent openai, snowflake about semantic views and feedback memory layer - so it looks to me like it's 3 options for people: go fullstack, stay in (shrinking) legacy until it dies, or get unemployed.",1
1quvm92,7,Can you share ai application tutorials you followed?,1
1quvm92,8,"I got hired as Business Intelligence Engineer, it includes data engineering (ETL, data modeling), data analysis (provide BI automation), and some data science (optimization). So I suppose you‚Äôre correct",1
1quv70v,1,Make a list of companies you want to work at and apply directly. LinkedIn and other ‚Äújob posting‚Äù sites are just data harvesting for HR,5
1quv70v,2,"Try hiring cafe, indeed, and built in . LinkedIn premium absolutely not worth it",3
1quv70v,3,Doesnt matter all the jobs are fake. Ive been applying for 4 months using LinkedIn and HiringCafe. The only interviews i actually get are from recruiters reaching out on LinkedIn.¬†,3
1quv70v,4,"Remote rocketship worked very well for me, although my most successful processes have been referral by colleagues.",1
1quv70v,5,"linkedin premium is really only good if you are messaging people. doesn't give you a ton of message credits, but if you want to ping hiring managers you can do it even if they are third connections.",1
1quv70v,6,Welcometothejungle (previously Otta) was the best resource in the my last job search. Terrible name but it had good quality jobs and I had the best callback rate on that platform.,1
1qut137,1,"Not possible, on purpose. Secure your URL with the secret.",2
1qunozk,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qunozk,2,I've seen multiple people have tremendous success just asking Gemini to come up with a learning plan,33
1qunozk,3,[https://roadmap.sh/data-engineer](https://roadmap.sh/data-engineer),9
1qunozk,4,https://github.com/DataTalksClub/data-engineering-zoomcamp,5
1qunozk,5,I like [vu trinh](https://vutr.substack.com/). Some of his stuff is behind a paywall though,4
1qunhxb,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qunhxb,2,"So many people trying to get into DE when experienced and fresh grads are struggling to land jobs. This isn't the market to switch unless you're getting pushed down and somehow your skills line up. 

Engineers without python or cloud --> data analytics/business intelligence 

Data analytics without much SQL --> spec definitions 

I'm curious what other transitions people have seen. I'm one of the DE without python and its looking more and more like it will be easier to slide into BI and learn those tools vs relearn data engineering..",9
1qunhxb,3,"Hopping from one hot job to another isn‚Äôt going to get you ahead in career. Sit down and focus on what is valuable in your current profession, then go and learn it/pursue self development. AI/ML is super hot right now in Data Science so I would suggest that. As the previous poster said, the market is tough for people with actual experience so it would be easier to stay in your current role. 

It seems like these people all gravitated to DS because it was the hot thing when DE was always there but not as sexy.",6
1qunhxb,4,"Good luck, market sucks right now.",2
1qunhxb,5,"I switched from DS to DE this year! The market is tough and I had a good connection (where my hiring manager and I were already friends fr a past employer together) so I managed to do it. 

You might start with data analytics engineer roles (which is what mine technically is despite being called DE) where you would need to know Snowflake or another data warehousing tool, SQL, dbt potentially, git, and orchestration (maybe Apache Airflow). You would need python and SQL and I enjoyed learning on data lemur.

I would say if this is a passion of yours, work towards it, but always keep DS as a backup plan. I like to remember the authors of the Fundamentals of Data Engineering textbook were ex-DS so if they can do it so can we!",2
1qunhxb,6,"Historically I think it would have been more possible but the market is not good right now. The people applying to DE positions probably include DAs, DSs, DEs, and SEs. Among those I would say that preference from a hiring perspective would be DE >= SE > DS > DA.

The issue with DE is that there's overlap with software engineering. So it's easier for SEs to transfer to DE and you're also competing with them. DS is a more specialized role and typically requires a strong background in math. So it has less competition.",1
1qumuao,1,"How do you want your agent to use it? 
And by agent do you mean a chatbot or an autonomous agent. 
Are the court judgments all in English or multiple languages?",3
1qumuao,2,"I work in the Brazilian judicial system, and the path to obtaining this isn't simple. You need to extract the data from the PDF and perform some pre-processing (cleaning, data preparation, etc.). 
After that you need to chunk and encode than send to a RAG.",3
1qumuao,3,[https://www.docetl.org/](https://www.docetl.org/),0
1qulhge,1,"If you can‚Äôt look into things at all on your own and use critical thinking to make your own conclusions, any field will be difficult.",35
1qulhge,2,"This is mostly an US based sub so marked is different in india. Also, search the sub, theres many questions like this",8
1qulhge,3,"Not true it‚Äôs just changing. Not dying.

It wouldn‚Äôt be ‚Äúespecially data engineers‚Äù it‚Äôs all of software that is changing - if anything was at risk I‚Äôd say pure software engineering is at highest risk. But to be honest things are changing not dying",3
1qulhge,4,"It may not die but what I do see coming, is massive depression in wages for cost center roles like IT because the barrier to produce functional code is lower, though individuals who can debug, and improve quality will always be around.",3
1qulhge,5,"Engineering with no business context will die. Engineers who can communicate well, model, optimise, architect, refine and deliver on business requirements will always be busy. Agree the first break might be hard... But don't limit yourself to engineering¬†",4
1qulhge,6,">¬†tech and data industry is dying, especially data engineering

Well duh, the specialty that now gets tasked with setting up the data pipelines using the LLMs is obviously the one on the chopping block",1
1qukzwj,1,"I did this! Key: lobby for data-adjacent tasks in your dev job (APIs, analytics). Then pivot hard with CourseraML + a killer GitHub repo.",1
1qukzwj,2,"Of course it‚Äôs possible. In my world, DE and SE both exist equally. Just remember with full stack, always a bit of a jack, so keep your focus on the back. Other than that, never hurts to learn all sides of dev. Just pick your niche. It sounds like you already have and see a path forward. I personally approve of your path, as I consider SE fundamental to a lot of DE work, but ymmv depending on the company a LOT",1
1qukzhw,1,You should link to the docs and source code.,8
1qukzhw,2,this doesn't support combinations of columns for primary key?,4
1qukzhw,3,"Hi did you think about getting primary keys of the compared tables by querying the metadata tables instead of using a required parameter ? I know it's doable in PostgreSQL, no idea about other engines",2
1qukzhw,4,Is it https://libraries.io/pypi/tablediff-cli?,2
1qukzhw,5,"This is a great side-project - many have been created, but they never get old.  A few suggestions:

   * Rather than a single primary key I suggest you support compound unique keys
   * Allow users to define either non-key columns they want compared - or non-key columns they want excluded
   * I would also include rows-in-a-only & rows-in-b-only
   * It's also helpful to know exactly which columns have diffs
   * It's also helpful to actually see the changed rows",2
1qukzhw,6,Good one. I tried reladiff and it had missing support for SQL Server which our Org uses. I latched back to archived project data-diff by datafold.,2
1qukzhw,7,"nice, that is really useful, I used to have a similar sql-based job to detect such differences before big ETL processes were executed and automatically alerted my team and paused execution, saved some big troubles when changes were pushed to production without notifying data engineering team, maybe that could be a cool feature!",1
1qujowc,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qujowc,2,Assuming a connections to wherescrape can be made via usual db con methods you can use a tool like sling or dlt to migrate. For a paid service datafold.,1
1qujowc,3,"I read about WhereScape quickly, so I may be wrong, but dbt does not connect different servers. It transform data between tables/views/etc in the same service only. Different logical DBs can be used, but not different services, afaik. If it's an orchestrator you need, check Airflow instead.",0
1quddio,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1quddio,2,"Become a senior BIE 
Or DE 
You can still very much get data roles

You‚Äôre just gonna have to apply 10x more than what you thought you would",40
1quddio,3,"If DE is the career you want then you need to fight for it. Apply, apply, apply! Fix your resume if you can't get an interview. Fix your interview skills if you can't convert interviews to offers. Always upskill. Stay persistent and pound the pavement. I graduated during the Financial Crisis with a totally different background. Bad markets happen, but this is a high demand profession. If you can't land roles with decent experience then don't expect to walk into another profession and score jobs. If DE is what you want then you need to plant your flag.",15
1quddio,4,"IMO if you were to transition to ML/AI engineering, you would be fighting another steeper and longer uphill battle. In your current situation you are already climbing a hill, but atleast you're on your way up. It would make more sense to build out your skills in DE and grow from there.",9
1quddio,5,"Dude what's your outdated tech your so worried about? Access?

I use Databricks everyday, and its still just SQL, with Python for the complicated or production bits.

If you know how to use pandas, you're going to be fine.

Don't let these guys fool you with their whacky stacks. Get yourself a spark docker image, maybe with dbt, or a free databricks account and just start messing with it.",8
1quddio,6,"Unemployed senior data engineer here. Over 10 years of experience building data pipelines in Python SQL, and golang. I applied for over 200 jobs. Got plenty of interviews but no takers. I finally got a non-paying gig building data pipelines for non-profits as a volunteer. I love data engineering so much I just can't give it up. So now I just do it for free.",5
1quddio,7,Trying to figure this out as well. I have 8 years. experience with most the popular DE tools and still cant land a job anywhere.¬†,12
1quddio,8,"If you have experience, I would say even as a mid it's hard not to find a job in this market, even if you have to wait a little bit. DEs are literally the gold for the AI hype",3
1quddio,9,I have been wanting a data engineering job but unfortunately I‚Äôm stuck in .net dev at the moment.,5
1quddio,10,"sometimes overlooked, but, given your data expertise, you have already learned a lot of insights into the business, so switching to a technical project manager role is an option I've seen some peers succeed at, better paid even",3
1qub19h,1,"I think it's much more important to understand the primary concepts of SQL than worry about the small differences in ""dialect"", unless you're applying for a position that explicitly states one particular version and insists on knowing the details. Learn the fundamentals of efficient querying and RDBMS logic before worrying about platform/vendor particulars.",6
1qub19h,2,group by id with select \* on MySQL can result in different results depending on the order of rows fetch in the engine. Postgresql is more strict and prevents you from making this mistake.,2
1qub19h,3,"being deliberate about which columns are used in a group by can help show your interviewer you understand the aggregation you're trying to make. 

also helpful for debugging in case you go down the wrong path. what you've labeled as positives for Mysql syntax I think are crutches in an interview environment and are not really difficult to learn..we're talking about a day or two to remember to explicitly label columns when using a group by.

BTW nice thing about snowflake is that it has group by all üòÅ",1
1qub19h,4,">""but the reason I ask is because using MySQL you can group by and SELECT cols without aggregate functions""


I'd be interested to see some cases where this is deliberately a good idea. I've never worked in a mySQL heavy place, but SAS let's you do this too and it definitely caused more problems than it solved.


For a SQL interview it doesn't matter. Unless it's pre-8 mySQL without window functions converting between the dialects is virtually never hard.",1
1qub19h,5,"> but the reason I ask is because using MySQL you can group by and SELECT cols without aggregate functions (which imo makes it WAY easier to solve problems)

This is actually a bad thing.  I like MySQL for other reasons, but I can't think of any circumstance where it is better to leave off the aggregate function (this also goes for ANY_VALUE in Snowflake, ugh). Newer versions of MySQL warn you for doing this and maybe it's even a default error by now, if you don't set a legacy flag in SQL_MODE.

What happens is that you get an arbitrary value from that column, instead of something deterministic (as defined by what aggregate function you choose).  If I saw someone grouping without an aggregate function (or using the accursed ANY_VALUE), to me that would be a negative, suggesting lack of attention to detail, or not understanding how grouping works.",1
1qua7yn,1,The DE I know there just does BI reporting,20
1qua7yn,2,"I worked at Booz Allen for 3 years. I can answer any questions you may have. 

1. Any job is better than no job in this market and you don‚Äôt have many options as a junior. 

2. Is the project guaranteeing you to get a TS/SCI? 

3. The reason that Booz Allen is not too keen in consulting circles is that they usually pay less but they don‚Äôt have an up or out culture. Also, they have good tuition programs for getting a masters.


From my friends who I have talked to there recently, if you‚Äôre on an IC/DoD contract you are safe. If you‚Äôre not, well that‚Äôs going to be rough.

Feel free to DM me",10
1qua7yn,3,"First, I‚Äôve never worked at Booz Allen.  However, I‚Äôve spent quite a few years doing software consulting and even owned my own consulting firm.  I‚Äôm now retired.

Most of their business is in government contracts.  That‚Äôs bad from the standpoint that the business would almost certainly fold if they lost those contracts or a large portion of them.  It‚Äôs far better if a consulting firm has clients spread across several business sectors.  If you have your business spread roughly evenly across five sectors and there is a downturn in one, two or even three sectors, you could survive as a business.  On the other hand, the federal government is frequently good for business.

Honestly, the IRS cut 31 contracts with Booz Allen less than a week ago.  It was over the leaking of IRS wealthy taxpayer data by a Booz Allen employee.  They still have quite a few contracts, most with the DOD.

If you do a good job, it be can be an okay place to work.",6
1qua7yn,4,"It‚Äôs gov contracting, I started my career there (not at Booz, similar company). Gov contracting at the junior level is all about asses in seats and you will have zero control over where you go - it‚Äôs luck of the draw. High likelihood you‚Äôll end up in an IT help desk type position where you write adhoc SQL against on prem oracle and IBM databases for non tech stakeholders. Interesting projects are few and far between and you‚Äôll have to go to happy hours to kiss ass to be put on them.",4
1qua7yn,5,"I was not DE, but I was developer intern working on signal processing for DoD. My experience there was amazing. I don‚Äôt know how much I would attribute to the company vs my team, but I worked with many friendly, encouraging and very smart people. The work was genuinely fun and interesting as well. I would recommend",2
1qua7yn,6,"Depends on if you really want to do data engineering or not. In a lot of cases at this company, data engineers/scientist/analyst are all lumped together and really just do like business intelligence stuff.",1
1qua7yn,7,"I have a friend who worked there and he said nothing but good things. He did cyber security tho, not de",1
1qua7yn,8,"Hell yeah, I love booze",1
1qua7yn,9,"Current employee in that general practice space. If they're sponsoring you for a Security Clearance then you should be joining one of the Defense or Intel related practices which is good since both have been mostly spared from the jobs & contracts bloodletting that hit the Civil practice earlier this year. 

  
Firm culture is fine, my coworkers are generally smarter & more forward thinking than colleagues from previous corporate F500 jobs. If you're in the DMV area internal networking is very important since its easy to get lost in the mass of employees if you have an inattentive CM, this is slightly less of a concern if you're based in a regional office with maybe a couple hundred headcount or have a really good admin group. Pure staff augmentation contracts are fairly rare on the data engineering side, they seem to follow more of a Design & Implementation -> Sustainment & Iterative Development cycle. Staff aug seems more common if you're something like a former intel analyst & moved to the private sector for higher pay.

  
I've generally enjoyed my time here & its pretty neat when you're working on a project that ties into recent geopolitical news headlines.",1
1qu9341,1,"Your post looks like it's related to Data Engineering in India.  You might find posting in [r/dataengineersindia](https://www.reddit.com/r/dataengineersindia/) more helpful to your situation.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qu9341,2,"For broad lineage across ETL, databases, and custom jobs, OpenLineage/Marquez is a solid open‚Äësource foundation, and tools like DataHub or OpenMetadata can build on that; just be prepared to extend connectors for Informatica PowerCenter and stored procedures since out‚Äëof‚Äëthe‚Äëbox support is limited.",2
1qu7dld,1,Python. I can't vouch for them personally but there are multiple libraries for extracting data/text from PDFs like pypdf (text) and camelot (tabular data). I can vouch for xlsxwriter working well for excel exports. Inbetween you can use something like pandas for transformations (which also happens to have a native to\_excel method that uses xlsxwriter or openpyxl). All free / open source.,4
1qu57bm,1,"Have you also considered leveraging the Tableau relationship model? It would allow you to keep the two tables separate while you define a logical join. Then at runtime, the individual vizzes would join in the relevant data, and any viz that doesn't require the joined data won't join anything.

Otherwise, in my experience (and this is obviously dependent on a number of factors), creating a single pre-joined table generally yields better report performance and potentially better dev experience. If this isn't an option, then my instinct leans towards leaving it as denormalized as possible.",10
1qu57bm,2,"It depends on if you have other attributes, and what you want to do if they change. If the account codes for a service change, do you want to have to rebuild the table? Do you want to keep history?¬†",1
1qu57bm,3,"Using relationships would probably be your best bet I think.

I try to keep everything in one flat view when using Tableau since it seems to work the best that way, but I know that isn‚Äôt always possible.",1
1qu57bm,4,"Option D if the business requires answering many questions at a service / account code grain.

Option B if this is a supplementary use case to an existing data model.

These are both standard dimensional modelling practices. I would not consider the other 2 approaches.",1
1qu4btu,1,Well part of the job is being able to quickly determine what the issue is and chart out a path. Of course the answer is important but more important is showing how you think. If you‚Äôre doing a tough problem in front of an interviewer you should verbalize your thought process. I have found that this helps convey that you know how to think like an engineer rather than simply spitting out the perfect answer. I‚Äôd use ChatGPT to practice before your next interview and talk as you do it to explain your thinking process,2
1qu47hc,1,"I never wanted to be data engineer but i give up LOL now almost 8yo, i never had the opportunity to choose the job",29
1qu47hc,2,Be the best prisoner you can be: [Learning to Measure Time in Love and Loss - The New York Times](https://www.nytimes.com/2024/10/18/style/modern-love-classic-learning-to-measure-time-in-love-and-loss.html),8
1qu47hc,3,I feel the same way. I got lumped into Support Engineering roles due to my customer service background and I'm finding it hard to break out. People say 'just work on projects' as if it actually fucking makes a difference. Nobody actually gives a shit about your projects no matter how impressive they are because it doesn't count as 'real experience' to actual hiring managers in this job market. Just sucks man. Not sure what else to really do at this point.,10
1qu47hc,4,"to be clear, data engineering is one of the most turbulent careers you could choose. Except for the part about SAP. I do believe that sap will find a way to survive even when the world ends.",5
1qu47hc,5,Dude it sounds like you‚Äôre just starting. Why the sob story,2
1qu47hc,6,"OP, I feel you. SAP is a horrible beast. It‚Äôs so complex and so huge nobody wants to touch that shit. Hence, a lot of money can be made, though.",2
1qu47hc,7,Get into any technical role closer to data and move from there,1
1qu21jd,1,Do both. You can do both. The only reason you only have time for one certification is if youre time limited.,9
1qu21jd,2,Id start with databricks just because its essier/less extensive than AWS. But the plus side if you do they AWS Data Engineer you cant get the AWS SAA with just a bit more study,5
1qu21jd,3,Id say databricks personally. The concepts will transfer to the AWS cert. Someone correct me if I'm wrong but I regret doing AWS first. Well see how fast databricks/snowflake goes. Maybe it transfers the other way too. Also depends on how much you understand the new architecture (which actually means agile makes every decision),7
1qu21jd,4,"I have Databricks Apache Spark Associate and I recommend passing it. Not too hard if you already know Spark, and it's appreciated by recruiters",2
1qu21jd,5,both,2
1qu21jd,6,Databricks Associate DE is much easier than AWS DE. It took me about 2 weeks and 2 month respectively to prepare.,2
1qu12iq,1,Read the wiki. Reading is really key in Data Engineering.,5
1qu12iq,2,"I was active enlisted -> used my GI Bill to study statistics-> now a software developer working with data and an officer as a reservist. Secret doesn‚Äôt seem to be too valuable from my experience 

Are you army? Green to gold might be worth it. I was infantry, but was into programming and learned to automate tasks with VBA, when I was put into a desk job‚Ä¶Python can‚Äôt be downloaded to any government computer and VBA lives in Microsoft applications. Still an option as a senior NCO in staff 

It‚Äôd add some time, but better pension as an E03. Plenty of opportunities to play with smaller data sets if you can be on an officer",1
1qu12iq,3,"yes a degree in CS is a good idea.

you have the GI bill; go enjoy getting paid to go to school for a few years and collect your retirement check... and see what the jobs look like on the other side.",1
1qu12iq,4,Learn SQL and Python. Learn how to create and query databases. Do a (coding) boot camp.,1
1qtxjdg,1,Pre signed S3 URLs,19
1qtxjdg,2,"Honestly, the least annoying combo I‚Äôve found is:

For one‚Äëoff client deliveries:  
use something like Dropbox / Google Drive / OneDrive, but generate a direct link with  
‚Äúanyone with the link can view‚Äù and turn off everything like ‚Äúrequire sign‚Äëin‚Äù and ‚Äúallow download requests.‚Äù Put the link behind a short, normal looking URL if you‚Äôre worried it looks spammy.

For bigger / more sensitive stuff:  
Set up a super simple branded download page (your logo, a short message, one big ‚ÄúDownload‚Äù button) and just host the file behind it. You can do that on your own site or with something like Cloudflare R2 + a little static page. To the client it just feels like ‚Äúclick link, click button, done,‚Äù no portals, no weird ads.

If your clients are a bit more tech friendly, WeTransfer Pro is still decent. The free version is where you start to get the sketchy vibe and limits.",4
1qtxjdg,3,Just give Internxt a go,3
1qtxjdg,4,Encode the data in an audio stream and upload to Spotify.,2
1qtxjdg,5,Sharepoint or drive.,1
1qtxjdg,6,S3 is your best bet.,1
1qtxjdg,7,"we ran into the same thing with non technical clients. the biggest friction was always accounts and ‚Äúone more step‚Äù emails. what worked best for us was boring and obvious to the client, basically a single download link with a clear expiry and size limit. anything that required explaining or reassurance ended up costing more time than it saved. the less it feels like a tool, the smoother it usually goes.",1
1qtxjdg,8,"Try out my [https://aero.zip](https://aero.zip) \- no ads, no required sign-ups, end-to-end encrypted (so only the intended recipient can see what is it that you're sending). Sending up to 2 GB is free, but let me know if you'd like a trial for 100 GB plan.",1
1qtxjdg,9,Most of the time I send a pigeon with an SD card. Never had any complains,1
1qtwrba,1,"Kimball's 3rd edition updates the toolkit for modern cloud stuff - 2nd is still solid but misses Snowflake patterns and dbt workflows. Grab the 3rd if you're buying new, it's worth the extra cash for current relevance. I reference both but lean on 3rd now",11
1qtwrba,2,Ah awesome I had the same question!,1
1qtwrba,3,Just for the record‚Ä¶. https://ia801609.us.archive.org/14/items/the-data-warehouse-toolkit-kimball/The%20Data%20Warehouse%20Toolkit%20-%20Kimball.pdf,0
1qtwdcf,1,The ones that pay way less than you‚Äôre used to.,110
1qtwdcf,2,"I work as mid level DE at a F500 insurance company. My organization in particular has a lot of ambitious folks. There are definitely people who have ‚Äòsettled‚Äô but most of them are not seniors. The seniors have high expectations here. That being said, insurance in general is pretty ‚Äòchill‚Äô.",32
1qtwdcf,3,"I've only been to a few companies and I honestly think work life balance is like 50/50 between the company and yourself 

I work at a startup type environment now and I'm just ok with being one of only a couple people on the team that doesn't volunteer for everything and stick their nose in everything lol 

I don't rush to deliver every single project and fall for their manufactured urgency 

Then again, my role isn't some mission critical thing. I produce reports so it's a little different 

I worked at a huge fintech and they were pretty dang slow to do anything so work life balance was decent but you get bored not getting anything done",17
1qtwdcf,4,"Established insurance companies usually rock. They're incredibly risk-averse by nature, so there are tons of lifers here.

I'm a mid-level/senior-ish DE at a F500 insurer these days, after I spent time in higher education, B4 consulting, and then built the data team at a PE-backed startup in the healthcare sector. I've done the stress of consulting and PE, and I can't imagine leaving insurance these days unless it was to go back to higher ed, but the pay there doesn't compete with the pay in insurance.

Insurance in general tends to have a lot of lifers, my team these days has 14 folks, with an average age of 51. Five of us are younger than 40, seven folks are older than 50, and one gentleman still plugging away at 73. Only two of us have been here less than 8 years, and six have been here 15+ years.

I was at USAA for about a year before this, and they also had a ton of long-timers. My team there was \~8 folks and only two of us had been there less than a decade, lots of folks had started in other jobs at the company and then cross-trained over into DE as the company built their org. They want to keep folks who really get the insurance business, that domain knowledge is really helpful.

Not sure if I'd recommend USAA right now, though. They're going through a whole lot of flux these last few years, and I think I'd steer clear until they figure things out, maybe 2028\~2030 or so. The thing that hurt USAA was that they got out over their skis after their big hiring burst during Covid, so they tried to thin the workforce with a ton of early retirement incentives in 2023, and then still had to do their first-ever wave of layoffs in 2024. Morale went from sky-high to the absolute gutter after the layoffs, so a lot of us bounced in the aftermath of that.

Once you've seen how stressful consulting and other industries can be, then how unerringly *stable* insurance is, it's hard to want to leave.",9
1qtwdcf,5,"As per usual, location really helps people here as nobody knows where you are and it might differ from country to country.

In the UK, traditional industries are pretty good.  Legal, finance, banking etc.  Not the most interesting roles although you 100% get work-life balance.  Not sure if that translates to where you are, mind.",6
1qtwdcf,6,"Just about any large non-tech company has a high probability of being more chill than a startup. Retailers, manufacturers, insurers, financial, etc.

But as others have said, salaries tend to be lower.",5
1qtwdcf,7,"I‚Äôm a lead at large health insurance company. The pay is good, not FAANG level by any means, but I‚Äôm quite comfortable. The work-life balance though is out of this world. Days are low stress. I can count on one hand the amount of days I have to work late in a year and they have vary generous pto.",5
1qtwdcf,8,Can confirm insurance is pretty chill. I worked with a large health insurance company and worked about 3 hours a day. Two of my colleagues each work for different insurance companies and same story.,3
1qtwdcf,9,Banks!,3
1qtwdcf,10,"I've always worked in mid-sized, non-tech startups. I was originally in marketing and then moved to data engineering about 6-7 years ago. I'm now a staff data engineer at a mid-size biotech company. 

There's a point around the $10-$30 million annual revenue mark where companies start to really need to step up their data and analytics practices and will start investing more in personnel and technology. These are great moments to get hired.

In my experience, these companies often pay above average and offer a lot of learning opportunities. Since there is always so much that needs to be done, I can often pick and choose what I want to work on. If you're in early enough, you get to do a lot of the foundational design of whatever system you need to build. 

I prefer this to jumping into a well-established team at a large company where the work is just delegated to you, you have to completely conform to the existing paradigm, and potential for skill or career progression is possibly a little more constrained.",2
1qtnonk,1,"Great
But I'd just use inline DuckDB for that",3
1qtfbhx,1,This is just a gold table view built in snowflake. Idk what engineering is actually needed here.,2
1qtfbhx,2,"What is the end goal here? Are you trying to visualize those aggregated performance numbers in a web page, power a BI dashboard, generate a static report, or just run raw queries? The end goal is important. 

The first thing I would do is trash your lambda fetch. Even a hot-start lambda is going to be slower than a dedicated API layer running in something like ECS. Further, you need to refine your data layer to include a layer optimized for data fetches for your specific needs. Something that could be useful would be building an OLAP layer, with is (more or less) just a vectorized representation of your data for rapid dynamic aggregation. Something like the [elasticube](https://pypi.org/project/elasticube/) library. That might be too much for your specific needs, though, so wait until you add that.

In summary:
- Get rid of the Lambda fetch in favor of a dedicated API layer that automatically handles authentication
- Evaluate your database for an analytical layer to support data fetches (add indexes, split into dim/fact, optimize tables/partitions for the most common queries)
- Look into OLAP cubes for better performance if query lag time is still an issue",1
1qtfbhx,3,"\> We had a senior data engineer who designed a sparse GSI schema for dynamodb where the agent metrics were dimensionalized such that i can query a specific GSI to see how an agent ranks on a leader board for a specific zip code/state/county etc. This architecture presents the problem that we can only compare agents based on 1 dimension. (We trade flexibility over speed). However, we want to be able to filter on multiple filters.

how many table rows do you expect to have?",1
1qtdnvi,1,"You often don‚Äôt NEED to if you just need to get something quick and ad-hoc that‚Äôs not going to be used again. 

You nearly always SHOULD if you are actually building out an enterprise solution that‚Äôs intended to scale and support flexible reporting over the long term.",38
1qtdnvi,2,"Generally you build a Kimball-style dimensional model for long-term analytics. Semantic models are generally for self-service. Operational analytics (that is, real time decision-making) is generally where modeling is not quite as strong. You‚Äôre still going to make a model, but integrating them across systems is harder.

There are also operational data engineering tasks that don‚Äôt care at all about models. E.g., get some data from a database and upload to some API every night.",11
1qtdnvi,3,"Stakeholder requests,Query patterns for them, Recency,DB overhead should be the guiding light to build it",4
1qtdnvi,4,"For anything reasonable analytic related- especially for scale you need a well thought through data model. 

However for operational data processing or perhaps even operational analytics you often can relax modeling and governance requirements as the data is not repurposed for varied analytical consumption- rather single use for a well defined operational process",3
1qtdnvi,5,"A dimensional model is a means to an end, and generally speaking, the end is that consumers need an organized and optimal way to gain insights into the data that's being collected. 

Simply put, if the data is already organized, accessible, and every business question can be easily answered, then there is no need for a dimensional model. 

As an example, I have worked for a few SaaS companies where the data collected was never viewed across clients and client insights only required a semantic layer for self service analytics. There is no need for a dimensional model because using an OBT model in a medallion architecture (or whatever you choose to call it) can do the job required with limited development.",4
1qtdnvi,6,"I've been questioning this myself as I've been tasked with building a pipeline for csv sales data that we will only get 3 times a year and I need to use medallion architecture to make incremental dim and fact that tables, but the client doesn't know exactly what he needs just yet, why make these tables and not just give him most the fields from csvs in power bi and let him decide what is kept after making a report, make dim and fact after.",1
1qtdnvi,7,"I dont know if I will repeat some points of other commenters but I was taught to return OBT (one big table) straight from bronze only when:
1. Domain is specific for people who barely know SQL (OPS, sales support) that use objrcts to automate their Daily work (easier to query without knowing keys /joins).¬†
2. One off requests in some niche part of Analytics.¬†
3. Although I've never done that myself OBT with requested features should be delivered to data scientists. It does depend on a process but after explolatory phase they Could request for your help with optimizing data ingest and there you usually dont return dims and facts but just big tables with features and their states that they analyse rn",1
1qtdnvi,8,"Unless you have software that still uses ad-hoc queries and you are limited on storage and compute you can use any modelling way you want. Dimensional modeling was a way to historically report on data with limiting the amount of joins.    
Nowadays you can go relational as you have plenty of compute, or OBT becaause you have plenty of storage, or dimensional if that is the preference. Just stay consisteny",1
1qtdnvi,9,Monitoring some temperature or something. Like a live data stream with very narrow dataset.,1
1qtdnvi,10,"I built a solution where the purpose is generating extracts. Business users when required update some of the mappings. There is no need for a dimensional model to run pipeline to generate files (even though the data is also stored in Snowflake tables).

I think when there is a very specific (narrow) purpose in the ""consumption"" layer - dimensional model might not always justified.

When I model, I ask myself and stakeholder two questions:  
1. how data will be queried  
2. how data will be updated (=>complexity, cost, resilience)

The model is the balance of the two above. If there is an existing DW/DM then consistency is also an important factor.",1
1qtc0ag,1,Start combining year with id as your new id as a start.,4
1qtc0ag,2,"It‚Äôs not clear, but why do you have to reload files from previous years? 

An issue with doing one big file is you‚Äôre overprocessing data which (I presume) is not, and should not change. 

Someone messes the file up and you‚Äôve potentially corrupted historic data.",1
1qtc0ag,3,Sounds sensible. You could always change to what I call a partition swap (no idea if it has a proper name). The year or a primary key becomes the partition key. For every year found in the file loaded these will be swapped/replaced in your staging table. Then data providers have flexibility in the number of years to upload (in case they don‚Äôt fancy maintaining a full file with all years). They can upload one or many years. You can still track changes if that is useful.,1
1qt95p7,1,"record the session, have a lot of questions for him to answer before he leaves. document everything. do as much as you can while he is still there. it's a tough situation many of us have been in and there is no one right way to do it. try your best",39
1qt95p7,2,"That's the reality in many companies.

As he said, look at the code, you'll figure it out. This is a great learning experience for you. It will be frustrating, but being able to tame messy code from other people without any help is a super valuable skill. 

Hard times make you a great engineer, enjoy the struggle :)",15
1qt95p7,3,"Ok, I'll go first.

If possible, take him for a coffee, establish a rapport before you get into the details.  Set him at ease away from any 'external influences'. 

Get the high-level starting points.  Servers.  Processes.  Credentials.  You can - yes, follow the code, but without knowing where the trail starts and what unlocks the gate you're a bit screwed.

Welcome to the magical land of fuckery.  Hope you like reverse-engineering shit. üòÅ",21
1qt95p7,4,"I'm going to go in a very different direction.¬†


Your being set up to fail. They needed to hire an experienced engineer.


This is your first job out of college. You should be working at a shop where a senior person can explain things to you. Both how the code works and data engineer concepts.¬†


Go ahead and do your best, but I'd really recommend you keep looking for a different job.",9
1qt95p7,5,"Start getting used to situations like this. Isn‚Äôt the first or last time it‚Äôll happen. 

Document everything, if you have it available record sessions, AI transcripts, take copious notes. If allowed use AI and/or ripgrep to figure out the code.

The more information you have the better.",4
1qt95p7,6,"Loom record
Document 
Send
If they have questions questions 
Hop on 15 minute call
Update the doc while explaining anything AND record that session as well)",2
1qt95p7,7,"Definitely record the meeting for reference later, there will always be stuff you won‚Äôt understand until you‚Äôre stuck into it. Also, in this case where the former person had some friction, please feel confident that if you deviate from their method and create your own approach, that might actually be good for you and your reputation with this new (to you) stakeholder ‚ò∫Ô∏è",2
1qt95p7,8,Focus on what to do if it breaks while he is still there and then figure out the rest later,1
1qt95p7,9,"Honestly, this is a tough spot for your first role. 1.5 weeks with a guy like that is brutal. Don't even try to understand every line of code; you'll burn out.  
  
Get him to talk through the \*critical data paths\*. Which tables are consumed by the most important downstream systems? Ask:  
1. What are the inputs for X pipeline? Where do they come from?  
2. What are the expected outputs? Who uses them?  
3. How do you know when it breaks? What's the runbook for common failures?  
  
Draw diagrams as he talks; even rough ones. Focus on the data contracts. Who relies on this data, and what format do they expect? Don't leave without clear answers on failure modes; that's what'll bite you in production. You'll figure out the ""how"" later.",1
1qt95p7,10,Feels like something you can ask Claude. Just audit this codebase and give me an entry point.,1
1qt4isy,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qt4isy,2,lost me at data engineer + data scientist in one person. they are looking for a slave who does it all because they are too cheap to pay for both roles. have seen this happen before and it never worked out. it's like asking your brick layer to also do the plumbing. they most likely have no idea what either role really does or how vast both fields are.,6
1qt4isy,3,"Birlasoft is most probably giving you a carrot. It‚Äôs very difficult for the L1 to be approved, they will hire you at low salary saying that you will be travelling anyways, then once the L1 is rejected, they will say they tried. You will be stuck working like a slave, and they get a single person for what should actually be 2 separate positions. What is your level of experience?",2
1qt4isy,4,is this role based in India?,1
1qt4isy,5,"Yes, Pune.",1
1qt3vtb,1,"1) I started DE ZoomCamp Cohort (was confused with other bootcamps but I wanted to learn Google cloud anyways).
2) Starting Senior DE role this month (Due to Layoff and social isolation I was so under confident and lost hope in this job market).
3) Non-technical: Learned that my attitude towards the issue makes a difference, I though I don‚Äôt know anything about DE even after working on Pipelines and Airflow for years, now after getting the job suddenly I remember all the DE tasks I have done in my years of experience.",3
1qt3vtb,2,Working on building an ETL pipeline for our new project.  super excited about it tbh.,1
1qt1543,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qt1543,2,"Sounds like throwing resources at the problem without understanding the root cause. I've seen similar issues with poorly optimized data partitioning leading to massive shuffle spills, even on smaller datasets ‚Äì try reducing the initial number of partitions drastically, sometimes less is more. What kind of data skew are you dealing with?",2
1qsuov1,1,Up-voting as I'm in a similar position and would love some pointers on how to do things on the side!,2
1qsuov1,2,Have you ever considered participating in open-source projects? It can be fancy unless you‚Äôre specifically looking for an additional source of income rather than a way to work on new interesting tech,2
1qsuov1,3,"Might want to try r/overemployed 

As a very casual reader of that sub, one regular piece of advice is that you, ideally, do not tell anybody you are running two jobs/contracts at the same time for obvious reasons.  So, anybody who is is unlikely to be telling the internet, or even a very popular sub of a relatively niche community, that they are working two jobs.

I'd also say for anybody else reading this post, common questions which have the feel of ""how do I earn more money by doing X"" isn't going to be well explained because you are increasing a lot more competition to your successful way of making money.  Again, I'd expect reluctance but could be wrong.",3
1qsqhzm,1,"Seniors can:
- Solve problems on their own
- Mentor juniors
- Understand not just a single piece but how it relates to the overall architecture
- Be aware of the cost/benefit analysis of various solutions
- Propose new VALID design patterns (not just ‚Äúwe should refactor to Snowflake‚Äù or ‚Äúlet‚Äôs migrate everything to Dagster‚Äù)
- And, most importantly for most companies, communicate effectively with non-technical and differently-technical stakeholders and executives",71
1qsqhzm,2,"People here are pulling a leg out to tell you what to do. The reality is: You are Junior DE. Only Senior leaves. You are Senior now. That's like 80% of cases.
That's how it fucking works and never let anyone tell you something else. But you need to be able to handle the shit show.

The other way is to tell your boss what you have to do to become senior. Define clear tangible Targets. If you meet those targets you should become Senior. If not hand in your resignation. Either you are valuable and they will reward you with the role or you were not valuable (just a cost center) in your role, which is just a time bomb anyway.

Nobody is waiting to award you with a senior title. They will try to keep you on a lower paycheck as long as possible.",9
1qsqhzm,3,"I really want to help you, but it‚Äôs the kind of question where if you are asking, it means you‚Äôre nowhere near ready. But as a general rule, the difference between any individual contributor (IC) role and a senior IC role is the demonstrable ability to basically manage yourself. Identify and execute projects that support company objectives, develop relationships within the business, that sort of thing. 

This is why: the next step from senior is often a lead or manager, so to go from junior to senior, you need to demonstrate capability, then when you‚Äôre a senior you‚Äôll be refining your capability and starting to demonstrate that you can apply your skill to leading others, which gets you to the next place.

And obviously that varies wildly from company to company üòÖ",19
1qsqhzm,4,"You have a leg up - you've interviewed!

First, some practical advice - you sitting in an interview means on paper you're already qualified. They're getting to know you. 

Second, some actionable advice - you have experienced the questions and have a newfound understanding of what the industry is looking for. Be good at those things and be the expert so you can answer confidently. Lack of confidence is usually a lack of preparation. 

Finally - you will be hit with imposter syndrome your entire career. Embrace the suck.",7
1qsqhzm,5,"When a technical problem gets escalated, and there is no one to escalate higher, whether you know the solution or not, you're senior. 

Or, whenever your bosses changes your title. Either way.",3
1qsqhzm,6,"Many people has answered, just want to add: Can architecture design, and explain why to do that; understand that there are more than just technical about this role.",2
1qsqhzm,7,"Business impact 

Everyone whose already a level 2 or 3 know the basics and tech stuff 

Are you a leader with a business mind is the question",4
1qsqhzm,8,"Most of the real skills one learns in the job. Once you know the basics of SQL/Python/basic Data Engineering fundamentals (CDC/Data Governance/OOP/Security/Statistic/DevOps/BI/etc), the questions one can be asked in this industry is so vast that practicing for it may be in vain. Esp in the high pressure screen share environment of difficult questions one might be asked.¬†

The longer you work at that job the more senior one can become. Avoid working in/applying to industries/for companies that intentionally use older technologies. By that I mean if they list older technologies in their job positing or the company inherently keeps themselves ancient like Intel/IBM/some pharmaceutical companies/some government jobs/etc.¬†

This approach may not be for everyone but I applied to hundreds, if not thousands of jobs in a year. Which, like reading 20 pages of a book a day, can be the simple consistency of a few jobs a day. And when my current job stopped challenging me enough during the pandemic I joined a start-up to challenge myself further. They just wanted SQL which is an easy requisite and through that process I learned Snowflake, and through that start-up/Snowflake experience I later got another job in Snowflake/GBQ/GCP, and through that job I later got a job in AWS/Databricks.

I never practiced code tests for any job. Many jobs didn‚Äôt want me for many reasons outside my control and it still hurt on a deep personal level. Many jobs I didn‚Äôt want because the interviewer was rude/the company didn‚Äôt seem fun. But for the few jobs that worked out I have seen a bunch of different problems in a bunch of different contexts in a bunch of different languages in a bunch of different industries. Those things, and factors about one‚Äôs personality, seem to govern good Senior/Staff level positions. It‚Äôs more about the patterns one has seen in real scenario's and one‚Äôs temperament - that‚Äôs hard to train for.",1
1qsqhzm,9,"Learn to explain things well.

You don‚Äôt even outline your background, so people are giving you blind advice.

‚ÄúI want to be sr‚Äù can be totally different if you‚Äôre a 5yoe intermediate vs a new grad who has 6 months of experience.",3
1qsqhzm,10,"Once you see data engineering as the ‚Äùthree body problem‚Äù and can navigate that I‚Äôd say you are senior. 

Juniors fixate on a specific aspect of the solution as a whole while seniors know there is more to things then eg perfect code. 

Also you have seen and probably contributed to a couple of failed/sucessful projects end to end and had to do some maintenance of it after you are ‚Äùdone‚Äù. The maintenance is very important because long term that is where most costs are generated. This is why lots of senior devs are a bit sceptical about ai and the maintenance costs it will generate.

So to answer your question: make sure you are exposed to all aspects of a project lifecycle, not just greenfield nice solutions where you have free hands. Also spend time with mid career seasoned guys and listen to them rant.",2
1qsm2gq,1,I start with functions to do what I need to do. One at a time. After a while I have a lot of functions that use the same parameters. That's when I think I have a good candidate for building a class. I just do it to keep my own work organized.,61
1qsm2gq,2,"I feel functional fits DE much more, never really use classes.",65
1qsm2gq,3,"I try not to get too carried away with it because it can be easy to over engineer things. 
We use oop to write reusable source gateway and downloader classes.",24
1qsm2gq,4,"OOP directly maps to table schemas. You can try to represent tables you work with as classes and rows as objects. 

Then you can try to play around with inheritance, interfaces etc. if you have some relationships. Or try to apply language features depending on which one you are using.

But simply mapping data from tables to defined classes puts you ahead of the curve tbh.",14
1qsm2gq,5,"We build a lot of python libraries that help automate certain DE tasks:
- table metadata (DDLs, table management)
- workflow orchestration (we use maestro) 
- data diff tooling

So all of the above are OOP, so not necessarily the data transformation itself",6
1qsm2gq,6,"You don‚Äôt need to learn it in DE context just pick up a book on Python OOP. 

I like https://www.cosmicpython.com because it‚Äôs practical and not dogmatic about OOP which is how most Python is written anyway.",4
1qsm2gq,7,"I‚Äôve lately been using classes a lot supported by protocols in Python to standardize the methods in the classes. This has been helpful when I have multiple sources with different source types, schemas and/or formats. 

This allows me in a main function to simply do something like:

for source in sources:

    source.extract()
    source.transform()
    source.load()


Sorry for the formatting, writing this from my phone!",5
1qsm2gq,8,OOP is anti pattern,6
1qsm2gq,9,"You don't need to.
OOP makes sense when you need to store state, for example if you have a bunch of functions that can do multiple things with tables, you might like to make them methods of a class so you don't have to configure them individually.",3
1qsm2gq,10,"Have a look at this https://github.com/tushar5353/sports_analysis

I‚Äôve created this pipeline just to show how can we leverage classes in ETL.

Also, to show modularised approach.

I know there things because I‚Äôve also worked as SE.",4
1qskarh,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qskarh,2,"A certification might help you get interviews. You probably just need to shortlist what languages and platform you want to use at your next job and get rolling. This will help your long-term career outlook. There is an over abundance of tools out there, but SQL + python (or pyspark) seems like it will be the clear winner for the foreseeable future. I also would stress becoming conversant with DevOps. You got this!",2
1qskarh,3,"It would be definitely good for you to understand what you want in your next role before you start applying. That way you'll have a better understanding of what skills to learn and hone. Also, it would be good to pick Python if you haven't already.",1
1qsg6tc,1,"Some distributed computing architecture does seem murky to me, so I would be happy to use it once you make the code available.

Wish I had more to offer, but as someone who is a severe Visual Learner, this sounds great üëç.",5
1qsg6tc,2,I'd love to try something like this out ü•≥,4
1qscp60,1,"My entire career is basically been data analytics and engineering for marketing teams, and this is also my experience. 

If I could sum it up, I'd say marketing folks act like subject matter experts in marketing but a lot of successful marketing is the result of statistical decision making which marketers do not understand. So 99% of the time marketing analytics involves validating someone's decision rather than a hypothesis. As in, someone has already made up their mind about what they want to do, they just need to prove that it was the right decision in hindsight.",23
1qscp60,2,I have not seen the lack of data get blamed but I see a lot of times they blame the data team in general because they think anomalies in an ad campaign or something is because of a bug on our end. No. Sometimes people just aren‚Äôt clicking on your ad that day or visiting the site.,8
1qscp60,3,"Marketing is only second to FP&A teams in my experience for worst stakeholders to have to regular support from an analytics/data standpoint. Similar to what u/TheGrapez said, Marketing and FP&A teams usually have the most disparity when it comes to how they use/interact with data and how much they think they know about data.

Marketing in the sense like others have said in that they use data as a means to justify their biases or simply blame the data team if results don‚Äôt turn out the way they like. FP&A in my experience is awful because you get a bunch of excel jockeys who think that because they can maintain a rats nest of insane excel macros and formulas that they know how to ‚Äúdo data engineering‚Äù. 

I‚Äôm obviously generalizing, but in working in data for almost 10 years, this has been my experience working with several marketing/Finance teams across several companies",6
1qscp60,4,"I love working with marketing teams because the numbers don‚Äôt matter just the velocity and the direction. Vibes rule. 

Finance and Supply Chain on the other hand can go and do one.",2
1qscp60,5,"In my experience marketing requires a loooot of communication and clearly set rules before working together. 

It's easy to say they're always blaming others but let's be real, without them the company would earn way less and you wouldn't have a job there. So let's make their work as easy as possible.

\- EVERYONE in their team should be aware of your current data infrastructure + its limitations. They should know your roadmap/sprint board/whatever, so they don't think you're sitting on your asses and do nothing.  
\- If they have recurring issues, create a ticket, let them follow it. Whenever they try to blame, ask them to tell their lead to discuss with your data lead to prioritize it. If your lead declines, they won't annoy you because they know why they don't get it.  
\- Block urgent ad-hoc requests. Every request has to be a ticket with a clear description + impact + what happens if they don't have it within the next <time period>. With the last question a lot of urgent things will not be urgent anymore. Most of the time it's just a blocker they want to get rid off asap.   
\- Sometimes they actually don't have the data to do their jobs. Can you provide an interim solution by pulling raw data without implementing it into your data model? So they can run simple queries.

Know your people, try to be seen as an overworked ally who wants to unblock them. Data Engineering in business teams is one of the most stakeholder-management intense jobs. The better you get in that, the less stressful your job will be",1
1qscp60,6,"They‚Äôve been the worst at math, in my experience. Say the cost was $3000 and we acquired 23 customers, not one would know how to work out the cost per customer. It was a lot of vibes. A new person would come in and contradict the strategy of their predecessor.¬†",1
1qscp60,7,"I work in Marketing Operations, and I find the challenges I deal with day-to-day are underestimated by management.

1. I often find that I don‚Äôt have the data to do what is being asked of me. Most commonly, this happens when there is a campaign where the target audience is brand new. I cannot tell you how many times I find myself caught in email chains with compliance and legal where their segmentation criteria requirements specify data points that the company does not even have and has no way of obtaining in a timely or reasonable way. Many times, I will point this out to my superiors who will just ignore me and leave me to figure it out on my own because they have no idea what to do. This literally happened to me last quarter. ü§™ 

I work closely with the team that does operational comms, and it is similarly stressful because they do not always know what list criteria they will need or when. They often deal with Sudoku like puzzles where they have to build lists with pieces of information, from multiple applications and then find a method to validate the results.

Even coordinating sends becomes challenging because of how dynamic the production process is. Multiple rounds of revisions means delayed approval which can push past the expected send date, causing sending conflicts that now have to be sorted out with multiple teams that often believe their message should be priority.

2. Yes. ‚ÄúEverything is a fire.‚Äù üî• Things break unexpectedly all the time, and we are constantly conducting an orchestra in front of a live audience. Marketing comes to you complaining of ‚Äúbad‚Äù data, but keep in mind that often we are fielding complaints from our stakeholders, triggering the inquiry. (FWIW Typically, I only flag anomalies after multiple observations or complaints. I also do some initial investigating on my end, like reviewing audit and sync error logs to rule out the obvious.)

Successfully capturing UTM parameters is kind of like walking a tightrope if you have a bloated tech stack and duplicate records. Yes, collection CAN be the issue, but I‚Äôm 15 years into a career as an IC in digital marketing now, and I mostly pull my hair out from poor data practices upstream in the CRM (e.g; users deleting emails and creating a new record, duplicates merged where UTMs aren‚Äôt preserved, poor field level security or outdated/inappropriate user permissions, etc.) I‚Äôve had an experience just in the last few years where the internal IT team decided to pull Marketing‚Äôs access to the native UTM fields (after 8+ years of having access) because a newer and more senior product hire had this as a requirement. It broke all of the UTM tracking from the website and some other applications, and I had to spend a year arguing with them to get it fixed üôÑ

The practices I see in Salesforce in the last 5 years truly feel like a major regression from the ways that I learned to manage data and technology early on in my career. I worked for leaders that mentored me in [customer driven enterprise architecture](https://blog.opengroup.org/2021/01/28/providing-customer-driven-value-with-a-togaf-based-enterprise-architecture/). It feels like it‚Äôs become a foreign concept with what I see now. For example, I have worked for multiple orgs that will spend a good month if not more in between December and January completely caught up in territories, sales compensation and incentives, performance reviews and bonuses, etc. Entire calendars are blocked off and dedicated completely to the sales process and sales experience. I‚Äôve seen how entire companies are stalled as they wait for decisions to be made. 

Lastly, sometimes we work for people that will use their authority to put pressure on us or our managers to force us to do things we know are stupid. üôÑü§¶üèº‚Äç‚ôÄÔ∏è It‚Äôs riskier to say this if you work remotely, but when I worked in person, it‚Äôs the kind of thing I absolutely would‚Äôve acknowledged in a whisper ü§´",1
1qscp60,8,"Let me tell you my experience working for an ISP.

I have a degree in Telecommunications and decided to go for a Master in Big Data. I always liked the field, a lot of people in engineering like to code, or make autocad prints, or maybe do a CCNA.But a a few people understand that understanding KPI or statistics to solve a problem. Even my more 'telecom' job was being a RF engineer for a Vendor, a position that I feel that was more data focused than other things.

Now, I ended getting this job after being lay off from my last job as a presales, and they put me directly in Sales Department, my coordinator was a long time Marketing people (he is algo an engineer, but always have been in sales or marketing); and this means that I began to panick. To think that my path from this point is being in sales (That I highly hate), but at the end I understand that i am not being less technical, my position is still Data Specialist; my job is still do some stuff in python and PowerBI, and at the end I understand that Marketing is the department that appreciate you ETL to get the stuff they needs to sell. Of course, I always have in mind my projects of Net Automation that I sometimes can mention to the NOC, but this does not means that I am not doing what I like. Also Marketing is less stuborn with the way you do your things that the all tome NOC Specialist or IT Guys, that thinks they way of doing things is the only good way, so...",1
1qscp60,9,"Damn , are you me? I work closely with marketing and finance team. 

The thing with marketing team is they work on hunch rather than using the data. 

Their requirements are vague. They want to do A/B testing and when I suggested groups they were like no this is not what we want. 

They sometimes forget their own KPI doesn‚Äôt have complete understanding of how the ERP system works and how to leverage sales and do marketing based on it. 

Overall if they dint know what they are doing. Our life can be hell.

Also they sometimes ask very basic questions which pisses  me off",1
1qsccxn,1,"What do you mean display it? If you're trying to like, print it out in the terminal, and it's way too big, it might just be the actually printing it part that is taking forever. Scan CSV is lazy meaning it doesn't load anything until you call other methods. If you're trying  .collect on 100GB if data that's a lot to download over wifi and certainly bigger than your computer's memory.

If you want to just like, look at it to check that the column mapping makes sense, I recommend sampling a small (and thus managable) set of rows.

If network speed is the issue it can help to run your code in AWS, like on an EC2 instance. That's not extra tooling like Athena, it's just running your code ""closer"" to the data.",19
1qsccxn,2,"Perhaps you could consider running a job on an EC2 machine (or any service of your liking) to transform those CSV files to parquet, which will compress and have informative headers that Polars can use to read the files in an optimized manner. Otherwise you'll have to download a large chunk of those files to determine the schema, which means your laptop has to pull a lot of data over the internet, which is slow.",5
1qsccxn,3,"once they land in s3 it‚Äôs better to work on them in the cloud. you can try a glue job or,depending on the individual size of each file, kick off a lambda function that does validation when an item is created in the s3 bucket. you have the start of a robust data pipeline.",4
1qsccxn,4,"It‚Äôs over 100GB of data that you‚Äôre trying to download to your laptop. You‚Äôre going to be bottlenecked by network i/o. Either filter it, run the code closer to the data eg on a cloud VM, or accept that 100GB takes a moment to download locally. Also be aware that if you don‚Äôt use streaming mode on .collect(), once the data downloads your machine will likely OOM.",3
1qsccxn,5,"You should have done parquet. Polars is columnar first, it is fast and can lazy load limited columns easily with a columnar format like parquet. Csv is only possible to be loaded in full to memory.  I would recommend trying to use duckdb to rewrite the data in a modern extensible format. Csv at that volume is asking for errors and inefficiencies.",3
1qsccxn,6,"CSV files are at the moment first downloaded to local disk before processed, so this is indeed slow. We will do that streaming in the future.

If you have the opportunity to convert these files to parquet or ipc files, Polars will stream them directly from s3.",3
1qsccxn,7,"literally just did a contract for a business where there was 100gb of parquet and csv in S3.

Duckdb saved my life on my local computer. It has a very very good csv reader.",3
1qsccxn,8,Sample. Although given its already in S3 I don't understand why you're avoiding Athena that's like 50 cents. Trying to use Polars to access 100GB on S3 is... a choice I don't think you've thought through. Are you going to to spend 30m (50MB/s) to hours moving these locally?,2
1qsccxn,9,Athena is the tool designed exactly for such tasks like yours.,2
1qsccxn,10,Use duckdb with httpfs extension,3
1qsc10c,1,can you use [faker](https://faker.readthedocs.io/en/master/) to avoid data breach of PII?,3
1qsc10c,2,"This is kind of generic and vague without context but...

My first thought is create a sandbox environment for testing and just dont expose it in a way that would risk breach. That seems like the most straightforward.

If you NEED to share with others, I would make sure all PII is in dim tables so your core dataset only has ids where things like email or name might be (better system design anyway). If you have secondary dims like company association/email domain, then make sure data is extracted to another dim table. Then you could just perform analysis using unique keys rather than pii fields and then arbitrarily assign labels to the ids like companyA, participant01798, country12, regionB or whatever the pii granularity you need to analyze but not display might be.

If you are talking about actually exposing the dataset to parties that are a pii leak risk... then the dim approach would help with cogently mapping people/companies/addresses to nonidentifiable labels that maintain full data integrity and the relationships between samples.",2
1qsaom3,1,"1. Seems true, take your time there, try to check other tools as well
2.any 3 major certs, don't focus too much as those are only confirmation that you know the basics and there is major certs inflation focus only on major aws ones. Udemy certs are a joke in medium seniority level. Consider digging deep into business problems for once and try being data product owner. Harsh experience my dude, but worth it",3
1qsaom3,2,"Congrats on your new job!

1. I'm not too deep into Palantir. Obviously, knowing only one tool sucks. But you can try to figure out all moving parts behind it. I just checked some architecture diagrams on google and they seem to do ever data engineer's day-to-day. In your free time you could try to copy the Foundry with Open Source tools to understand what is going on step by step: Ingestion -> Transformation -> Analytics -> BI Dashboards (+ Orchestration) etc.

  
2. A basic understanding is enough in 9/10 cases. Focus on getting sensationally good in what actually matters: PySpark, SQL, Transformation/Testing Frameworks (dbt), Deployments, Architecture Decisions etc. I you want to stay in consulting, ask your manager what they'd like to see for your certification. Afaik your company gets paid more if you have specific certs. Outside of consulting, nobody gives a shit about them. Work experience is the only thing that matters. Just get good without relying on AI.",2
1qsaey4,1,Did you pass in first attempt?,1
1qsaey4,2,Can you list all the resources you used to prepare?,1
1qs44qq,1,"Go to companies websites 

We send out 10k applications monthly for our clients and we analyzed which have highest chances of jobs 

LinkedIn was the worst. Most were expired 

My guess, LI was recycling jobs for SEO",2
1qs44qq,2,"Couple things from someone who was in a very similar ‚ÄúI like cloud + data but entry level feels impossible‚Äù spot:

First, your background is actually a plus. Biomedical + CS + interest in AWS is a really solid combo for any company doing healthcare / medtech / clinical analytics. You‚Äôre not ‚Äújust another CS grad‚Äù and you should lean into that in your branding.

Job search stuff:

You‚Äôre not going to win by only applying through LinkedIn / Indeed. Use them to find roles, but assume most of your ‚Äúwins‚Äù will come from:
- Direct company career pages (especially mid‚Äësized boring‚Äësounding companies, not just the big names)
- Alumni network from your undergrad and your MS program
- Local meetups / online meetups (data engineering, cloud, healthcare analytics)

On LinkedIn specifically:
Search for ‚Äúdata engineer‚Äù, filter by Experience ‚Üí Entry level / Associate, and also try ‚Äúanalytics engineer‚Äù and ‚Äúdata analyst‚Äù with SQL + Python + cloud. A lot of ‚Äúdata engineer‚Äù work is hiding under those titles and is more friendly to juniors.

Then for roles you like:
Check who works there as a data engineer / analytics engineer / dev on LinkedIn. Add them with a short note like:
‚ÄúHey, saw you work on X at Y. I‚Äôm a biomedical eng + CS grad student getting into data engineering (SQL/Python/AWS, a few personal projects). Would love to ask 2‚Äì3 quick questions about how you got started there if you have time.‚Äù
You‚Äôre not asking for a job, you‚Äôre asking for info. Some % of those chats naturally turn into referrals.

Targeting-wise, with your background I‚Äôd look at:
Health tech startups, EHR vendors, medical device companies, hospital systems, insurers, pharma / biotech. They all have messy data and love someone who actually understands the domain.

Also, consider ‚Äúbridge‚Äù roles:
Data analyst, BI dev, ‚Äúreporting engineer,‚Äù even some backend roles with a data flavor. If they let you work with SQL, warehouses, ETL, and AWS, that‚Äôs a path into a pure DE title later. First job title matters less than ‚ÄúI have 1‚Äì2 years of real data work on my resume.‚Äù

Projects:
Make 2‚Äì3 small but real projects that scream ‚ÄúI can do data engineering‚Äù and ‚ÄúI know healthcare‚Äù.
Example: pull some public health dataset, build a basic pipeline:
ingest ‚Üí clean ‚Üí store ‚Üí query ‚Üí simple dashboard.
Put it on GitHub, write a 1‚Äëpage README that explains the problem, tech stack, and what you did. This is gold in interviews.

If you want to play with realistic internal‚Äëtool type stuff: spin up a small web UI on top of that data so someone non‚Äëtechnical could view / edit it. Tools like uibakery / Retool / Appsmith are handy for this, because you can show ‚ÄúI know how to wire APIs + DB + UI together‚Äù without spending months learning frontend. That‚Äôs closer to what a lot of internal data teams actually do.

Mindset part:
The ‚Äúno light at the end of the tunnel‚Äù feeling is super normal. Most people don‚Äôt talk about the 200 ignored applications, just the 1 offer. If you‚Äôre sending out apps into a void with no feedback, shift some of that time into:
- Reaching out to humans
- Tightening 1‚Äì2 standout projects
- Practicing SQL + basic data modeling + simple dbt / Airflow patterns so you crush technical screens

You‚Äôre not incapable. The market is just noisy and bad at surfacing juniors. Your combo of domain + CS + cloud interest is good. If you keep iterating on outreach + projects and stay flexible on titles, you‚Äôll get a crack at it.",1
1qs44qq,3,"I also completed a Bachelor's in Biomedical Engineering before I pivoted to data engineering, so feel free to ask questions. Personally, I got my first junior role by applying to an opening on LinkedIn so nothing special on that part. But I can offer insight on what is expected as a junior, and what I did to land the role. First thing's first, you need a decent understanding of Python and SQL, these are your core languages for almost everything. Next, learn the basics of how to handle data, build pipelines, understand the basics of cloud in terms of DE. That's it, don't drown yourself in learning everything, that is not what employers want or expect. Just understand the basics well, and look for opportunities to open up.",1
1qs44qq,4,"Although I'm no healthcare data expert, I think learning and highlighting in your resume the industry standards for healthcare data (HL7 and FHIR) and medical data privacy regulations (HIPAA and GDPR) might be beneficial to stand out over a ""generic"" data engineer profile in that specific field. GIven your background, you might already be pretty familiar with those, sometimes you just have to do a better self marketing to get through ATS filters. Just my two cents best of luck!",1
1qs44qq,5,"I am not sure where you live but if you are in a metro area I would recommend looking for related meetup groups where you can attend in person. Most will have some mingling and a great way to meet people. 

Also look for conferences in your area. Many will have low or no cost admission options you can apply for, for new DEs.",1
1qs3tbl,1,"* yes, you can call yourself a Data Engineer.

* if you're not tasked with redesigning the pipeline and just migrating, yes it's an entry level.

* as for the current economy, stay longer if possible. Unless there is a company that proactively offers you a position.

* I always say it to my juniors: ""don't learn the tools, learn the fundamentals"" (i.e. data designs and theories. Also calculus and statistics). Tools and languages are always evolving, but fundamentals are rarely advanced unless someone makes a breakthrough. You're forgetting past tools is normal.",18
1qs3tbl,2,My first job was doing ETL and I barely knew SQL. You‚Äôre good bro.,12
1qs3tbl,3,"go look at job descriptions, figure out what you dont have... gain that.

you are ready for the job when someone will hire you.

apply before you think you're fully qualified... and let them decide",3
1qs3tbl,4,"Sounds to me like you‚Äôre doing data engineering so calling yourself a data engineer just makes sense. You might not be building new systems, but it‚Äôs not a bad first job at all.

Don‚Äôt have the mindset of leaving in the next 12 months, have the goal of impressing them so much that they move you into more advanced work. Then even if you leave you‚Äôll have a better resume due to that mindset. I‚Äôm getting interviews in this market even without a degree just because I‚Äôve always had the mindset to deliver extra value, which helped me build trust and get to take on responsibilities that nobody at my experience level should have.

For education and advancing skill, read books, and do some real world practice. Depending on what you actually end up doing at work, you might get plenty of practice there.",3
1qs3tbl,5,"Why are you so set on leaving in a year? In most positions, you don‚Äôt actually have a solid grasp of the job and what you‚Äôre doing for at least 6 months.",3
1qs3tbl,6,"My first DE job was just grabbing csvs and excel sheets and ingesting them. Then they wanted reports to be made on them and didn't have anyone available so did that (Data analyst stuff) Then they needed more and to encorporate that (Back to DE). Its less about job title at a certain point if you find the ebb and flow and can learn tools to do what the business needs. I did that for 2 years before I considered my self good enough to be called a professional and not a junior. Maybe you move faster than that. Also Migrating from an old system to a new system is a part of the cycle. Snowflake and Fabric are current generation cloud tech, so you are getting a free education on how to use them, take advantage of it!

Ask yourself, How would you build this whole system? What does the whole system look like? Data engineering isn't just ETL, its a big part of it but the data modeling and being able to serve the overall system is the important part. Do you understand the whole system?

Timeline to understand when to move is kind of irrelavent. Actually understanding and being able to apply those skills is when you make a move. You don't just set that to arbitrary 6/12/24 month timelines. When you gain the capability and understanding thats when you can think about moving to the next thing. There is always a next thing.

  
Back when I started there was only MSQL 2000 and Crystal reports or SSRS. Now there are 50 different technologies to do the same with python , Java, or C#. So tools have gotten more sophisticated. Pick a path, learn that path, maybe use claude code/Code assistant of choice to help you build one, then read and understand how its built. Look up tutorials from youtube and compare that to your solution. Read the libraries and documentation that make it work. Then go from there. 

Build a whole program that takes in data to do something useful. Kaggle has a bunch of examples for this.   
  
The only way to get better is to build things.",2
1qs3tbl,7,"Few thoughts 

1) are you making more money? If yes, is this just about status and ego in that you have an ‚Äúentry level title?‚Äù

In other words do u care more about the money or the title 

2) might as well always be learning - so yes study system design 

https://drive.google.com/file/d/1PaU1PA7j948Y8t5jG8t5QxwGb0VJMs4Z/view?usp=sharing

This book will help^

Entirely free

No optin required 

Thank me later",1
1qs3tbl,8,"A good mindset to develop is to get a bit meta. If you're doing the same thing over and over, ask yourself ""what's the pattern here?"" and think about ways to make your own life easier.

Over time, you'll come across pain-points and figure out or adopt established ways to structure what you're doing to deal with them, and get a feel for how different tasks fall into broadly similar frames - it's this experience that will serve you later on as you move up the ladder. 

This will include things like version control, CI/CD, leveraging models, automation, effective requirements gathering and so on - and often it's this broader set of skills that an employer are going to look for in addition to specific tool knowledge.",1
1qs3tbl,9,It‚Äôll look better on your resume if you stick around for 2 years since it‚Äôs your first gig.,1
1qs3tbl,10,"Alright, listen. Your current job \*is\* data engineering, especially for entry-level. Migrating legacy pipelines to Snowflake/Fabric, that's production data work. You're building pipelines.  
  
Staying 1-2 years is fine if you're learning how to build \*reliable\* and \*cost-effective\* pipelines, not just how to copy tables. To upgrade, focus on monitoring, error handling, performance tuning, and optimizing cloud spend. That's what differentiates someone just moving data from a real DE.  
  
System design is essential for anything beyond basic ETL. Start now. And no, don't fake tool experience. It shows fast when things break. Learn the concepts, then apply them to whatever tool you get. Practical experience on stability and cost is way more valuable than just a list of tools.",1
1qrzk69,1,"We have been using Airbyte OSS for the last year and have had issues from the beginning. Primarily, it doesn‚Äôt scale well. We originally used abctl on a VM and that maxed out with a few db to db cdc connections. Now using it on k8 with a dedicated Postgres db and blob storage for logs. Performance is better but not much. 

It‚Äôs honestly been a very janky product. Random bugs, successful runs that silently failed, sporadic OOM errors when there is 64gb of memory available, and the list goes on. Shoot we are on azure and abctl would randomly crap out because of a missing AWS env var. It also didn‚Äôt integrate well with the rest of our open source stack - dagster, dbt, open metadata

I don‚Äôt know if I could recommend it for anything other than db to db CDC syncs. It‚Äôs been problematic at best. We are in the process of migrating the workloads to dagster python using debezium.",8
1qrzk69,2,"Yes, you'll be using Airbyte.

Seriously, you may as well get Claude to generate the python scripts you need and run them with cron. Airbyte is junk.",15
1qrzk69,3,"I also have speed concerns on my self hosted Airbyte. We run it on k8s and sometimes an incremental sync job from a Postgres DB takes 5 mn with actually no data being loaded, but also sometimes it takes only 1:30 mn with 10-50 MB of data.
Not sure if Airbyte is responsible but I also regularly get gateway errors (502 & 504) when using the API",3
1qrzk69,4,"Airbyte is‚Ä¶not good...but I can‚Äôt think of a good alternative that isn‚Äôt managed/expensive

Depends on the size of your data and team. 

We‚Äôve had a lot of problem scaling and we split our jobs into many streams for our larger data sets even then it falls over a lot but it works fine for small ones.",3
1qrzk69,5,Try dlthub.com,4
1qrzk69,6,Airbyte uses k8s under the hood and it's very slow. It's much faster to write your own scripts (LLM will help with that and use lightweight tools like Airflow or Kestra for orchestration),2
1qrzk69,7,"What you are seeing is expected. Airbyte OSS gets expensive and fragile once you scale because every sync spins containers and leans heavily on Kubernetes. That leads to slow startups, memory spikes, random OOMs and silent failures unless you massively overprovision. At hundreds of pipelines you are basically running a K8s platform. Not just ETL.

The real tradeoff is connector breadth vs operational stability. Airbyte has lots of connectors but many are uneven quality and you own the blast radius when APIs change. If you are hititng wall either slim down to custom scripts or Meltano or move critical pipelines to managed tools like Integrate or Fivetran where throughput, retries and monitoring are solved problems. Self hosted Airbyte can work but you are signing up for infra work long term. No reason to lie to yourself.",2
1qrzk69,8,Why do you want Airbyte? What problems are you trying to solve with it?,1
1qrzk69,9,And how does Meltano compare to Airbyte. I am trying to decide between the two?,1
1qrzk69,10,"We have a self-managed Airbyte OSS set up on AWS EKS. Similar to most other complaints, the connectors are slow and run into OOM issues A LOT! We initially wanted to move away completely from a managed service (Hevo) but quickly realised any high volume connections will fail repeatedly.

For instance, CDC syncs from a production Postgres DB to Snowflake, Airbyte Postgres source was just too slow and WAL would quickly grow, so we created multiple streams. But even then, the Snowflake destination connector would be so slow to write, running into a bunch of timeout issues.

Ultimately, we decided to keep it for lower volume connections e.g. consuming Zendesk data. Even then, we had to use very huge EC2 instances to still avoid issues (r8g.2xlarge).

Ultimately, there aren't a lot of good free solutions that don't involve orchestrating a bunch of Python scripts.",1
1qrqz6i,1,are you already very good at SQL,3
1qrqz6i,2,"I went for BigQuery to DE (in title only) to Cloud Architect supporting a team of DE. If you are starting the Datalake from scratch, use terraform if possible. Focus on permissions and CICD. Let the other DE or analytics engineers focus on the pipelines.

If you have good executive presence, focus documentation that aligns the datalake goals with company strategy.",1
1qrqz6i,3,I had a similar post to yours. Following!,1
1qrqz6i,4,"Are you going to be a team of 1 or will you have support from other DEs?

Sorry I just realized you answered this already. I will say you are going to have a tough road and you are going to make a lot of mistakes along the way.  I would talk with your boss and see if they are at least open to connecting with a consultant to help you architect something out. But make sure it is solid architecture and not ‚Äúslide ware‚Äù (I.e. consultant PowerPoint slides with no meat)",1
1qrmmjl,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qrmmjl,2,"Data analysis is boring, but is the only entry job you can get, unless you are some sort of math genius in which case I guess you could apply to DS roles or Quant research. But for entry roles data analysis is saturated, precisely with people in your same situation, and some other that made a dashboard once in Power BI and they believe their BI experts now.",6
1qrmmjl,3,"I'm currently working at the intersection of DE & Finance.

Just DM'ed you.",3
1qrmmjl,4,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qrmmjl,5,"AI has been actively making things worse at my company.  And I hear the same from a lot of my network of peers in the industry as well.  So I'm not sure what you're on about with that.  But either way, the entry level (data analyst) is pretty saturated, and data engineering is not an entry level role.  I found success career switching into this field by spending about 2 years networking internally at my company and really battling for opportunities to do stretch work on projects that other data teams wanted done but never had the capacity to work on.  Eventually they created a role for me on one of the primary data engineering teams.",1
1qrmmjl,6,"Your best bet is positioning yourself as an analytic engineer. Learn some modeling techniques. Pick up some books on data modeling for finance specifically. And time series analysis.

If you understand both finance and SQL people are going to want to hire you. They‚Äôll scoop you right on up. AI is nowhere near replacing analysts. There are going to be companies that try ‚Äî but it‚Äôs not going to work. AI can certainly augment the work of a competent analyst but it‚Äôs not replacing one. Not yet anyway.",1
1qrm6z3,1,"I always add a model between whether it be dimensional, relational, or vault. This adds a layer of abstraction which can help protect from source changes, permits custom optimisations, and allows you to create various business views over the base transactions e.g. financial vs non-financial, Rollins vs rollouts etc.

DE working in large financial company for last 8 years",13
1qrm6z3,2,"I love taking a pile of reports and using them to create a data model. Break them down into facts and dimensions. Unless you're dealing with pipelines where ""agile"" has dominated every decision and then good luck. Maybe you'll hold it together in the Semantic layer",3
1qrm6z3,3,"Always dimensional model.

It allows for much more flexible analysis over the longer term and produces much more predictable results.

Especially when the front end tool is something like Power BI. DAX will turn into an absolute nightmare with flat models. 

If I had a developer on my team just connecting flat tables to Power BI, they‚Äôd be coached once or twice, and if they kept doing it they‚Äôd be fired. 

It‚Äôs just sloppy, bad practice from either citizen developers or backend developers who either don‚Äôt know how data is used on the front end, or who don‚Äôt understand architecture and are happy to spend all their time writing new views every time a user wants to see things at a different level of detail. 

It‚Äôs not ‚Äúspending more time driving value for the business‚Äù vs ‚Äúspending time battling tech‚Äù.

It‚Äôs putting actual thought into your design to your solution can be scalable and flexible to maintain rather than brute forcing a solution that requires you to build another standalone view every time someone needs a new report. 

People who say ‚Äúdimensional models are overkill‚Äù are nearly always people who don‚Äôt understand dimensional modeling.",10
1qrm6z3,4,"Normally you build those on a star schema. PowerBI works best that way.


Id say such a model should be in/ be based on gold. Never bronze.",0
1qrlhoj,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qrlhoj,2,"FWIW, when i interviewed with Shopify they seemed to care more about the thought process/collaboration than correctness.",30
1qrlhoj,3,Window functions!,21
1qrlhoj,4,I got better by doing leetcode sql questions myself and then asking llm if there were better syntax options for my solution or just reading top voted answers. I ended up picking up a lot of new syntax that made my queries more concise and which I use most days,56
1qrlhoj,5,Datalemur.com,11
1qrlhoj,6,"For fun, you can do the SQL murder mystery, and the SQL squid game (google for these). I would say if you can do these fluently you are good, esp the Squid game one is rather challenging.",6
1qrlhoj,7,Shopify‚Äôs SQL interview process sucked or at least it did for me with the interviewer I had. He was hung up on a small syntax nuance for no reason and even though I told him I can give him multiple ways of doing the same thing he wasn‚Äôt happy. Pretty reflective of their toxic culture I believe,6
1qrlhoj,8,Years of pain is the secret to anything,5
1qrlhoj,9,"‚Äã20+ years in data/ETL here. Beyond just practicing SQL syntax, focus on these data engineering-specific concepts for Shopify:

1.‚ÄãPerformance thinking: When you write queries during the assessment, always consider ""how would this perform on millions of rows?"" Shopify deals with massive scale. Use EXPLAIN plans, avoid SELECT *, and think about index usage.

2. ‚ÄãData quality patterns: Practice SQL for data validation, deduplication (ROW_NUMBER() OVER PARTITION BY), and identifying data anomalies. Real data engineering involves catching bad data before it breaks pipelines.

3. ‚ÄãIncremental processing: Since you mentioned building pipelines mostly in Python, practice SQL patterns for incremental loads‚Äîusing timestamps, watermarks, and merge/upsert logic. Think ""how do I process only new/changed data efficiently?""

4.‚ÄãSet-based thinking: Coming from Python, you might be used to loops. SQL is set-based. Practice writing queries that transform entire datasets at once rather than row-by-row logic.

5. ‚ÄãReal-world scenarios: Go beyond LeetCode. Practice queries like:
  ‚ÄãDetecting duplicate orders
  ‚ÄãCalculating running totals/moving averages
  ‚ÄãHandling NULL values and edge cases
  ‚ÄãTransforming nested/JSON data

‚ÄãFor Shopify specifically: They care about how you communicate your approach. Talk through your thinking: ""I‚Äôm using a CTE here for readability"" or ""This JOIN might be slow, but we could index X‚Ä¶""

‚ÄãGood luck!",13
1qrlhoj,10,"where do these people keep coming from, who focused so much on python and completely neglected SQL? why are there so many of them?",7
1qrk9lj,1,"CDC should be your first step in almost every scenario. Don‚Äôt do ETL until it‚Äôs in raw, then validate to get it to bronze. Only merge into bronze with basic validation checks on your jobs metadata. Truly, don‚Äôt check the data at all, just end the connection as fast as possible.",6
1qrk9lj,2,"How Gold tables already exist in SAP ? 

IMO, SAP data quality is not enough and you have much cleaning and modeling to do to get Silver and Gold.",5
1qrk9lj,3,How much data do you process daily?,1
1qrk9lj,4,"Didn‚Äôt this post just appear on r/databricks ?

Something that I don‚Äôt think was made clear on your other post is this native HANA, the ERP, or BW?

Just referring to it as SAP HANA is a little naive",1
1qrk9lj,5,"I am not much familiar with SAP, but Databricks have amped up their partnership, or whatever it is called with SAP, [https://www.databricks.com/blog/announcing-general-availability-sap-databricks-sap-business-data-cloud](https://www.databricks.com/blog/announcing-general-availability-sap-databricks-sap-business-data-cloud) 

  
Don't know if the improved sharing of data is useful in your case, and how your setup is in relation to that, but dropping the link just in case.",1
1qrk9lj,6,"depends on how skilled / resilient you are. 

If you're very much one or the other; airbyte will consume much of your day but cost less. 

For something more pricey Qlik / fivetran will do much of the heavy lifting for you.",1
1qrk9lj,7,cdata,1
1qrk9lj,8,You need a virtualization layer so you dont have to replicate,0
1qrk9lj,9,"lol bro have you ever opened the sap backend tables in your life? There‚Äôs nothing in there that resembles any usable information and you usually need to create CDS views out of multiple tables to start getting somewhat close to bare minimum.

It‚Äôs a nightmare regardless of what you do",0
1qrk9lj,10,Why is not sap Hana dead yet?,-3
1qrhvt7,1,"You can find our open-source project showcase here: https://dataengineering.wiki/Community/Projects

If you would like your project to be featured, submit it here: https://airtable.com/appDgaRSGl09yvjFj/pagmImKixEISPcGQz/form

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qr6cml,1,"Yes I have. 
Why do you need a simulator? It‚Äôs a standard multiple choice proctored exam.",1
1qr4dfn,1,"BigQuery + Looker Studio is a totally reasonable stack for this scale, and putting Pub/Sub in front is a nice safety net if you want durability and smoother ingestion. For device auth, you could use a shared API gateway with per‚Äëdevice keys or tokens rather than individual service accounts, which keeps things secure without turning identity management into a nightmare.",3
1qr4dfn,2,You can do all data handling and processing using SQL Server and SSIS. You can even run an instance in the cloud if you like.,-1
1qr1ah1,1,">What's the motivation to still pursue this field ?

As somebody who went into this field from a different field, if you work in IT/programming, don't suck at it, and work for a decent company, GOOD LORD is it a sweet deal.

Life as a chemist:

* There is no such thing as remote work.  You are on-site 100% of the time.  I used to have to commute 1.5 hours one way if it was a quick journey.  Made me give up all of my hobbies post work.  Standard working day was around 12 hours (wake up at 6, get home at around 6).  Yeah, I could have moved closer but I'm glad I didn't because it meant I'd have moved to bum fuck nowhere for a job which ended my contract during the pandemic
* Labs are often in shitty locations.  Good luck if you want to live near a city and have a reasonable commute time.  Didn't pack a lunch? Budget 20 minutes of your lunch time getting to and from somewhere to get something to eat
* People think lab work is a refined art where people stand around in pristine labs and discuss how the universe works.  In reality, the labs are freezing cold in winter, swelteringly hot in summer, and poorly maintained.  Half of me isn't surprised because if there is an explosion/fire, it's not a huge amount of damage cost.
* No windows can be quite common in buildings.  Somewhere I used to work, during winter there were days I'd see no natural light (get in before sun rise, sun sets at around 4 pm)
* It's also a lot more physical than people give credit for.  Lifting large/heavy vessels into/out of your fume cupboard as well as transporting large quantities of materials isn't easy on your back as you get older
* I worked in organic synthesis where if you don't have a PhD in chemistry, it's incredibly hard to not only break into the field but succeed because you're talking about the vast majority of people have a PhD, thus use it as a stick to beat you with (quite fairly, tbh).  I do not have a PhD and was usually the only person in the lab who didn't have a PhD
* The work environment is a lot less forgiving than tech.  You can't google and hit the right answer easily.  You really have to know what you are asking the internet.  Even then, the answers are likely not widely published.  People are a lot smarter on average and a lot less willing to accept bullshit answers
* Salaries are not remotely comparable although you do get a much better pension.  For reference, I used the chemical companies used to pretty much double your contributions.  I have never seen that figure in IT

Life as a DE:

* Work from home is an option.  If you have to turn up, at least your office is likely somewhere accessible and temperature controlled
* A much lower barrier to entry.  The idea you ""need"" a Masters degree is, in my opinion, complete bollocks.  DE education requirements are not standardised across the industry whereas chemistry absolutely is.  It's not possible to get into chemistry without a chemistry degree, although you can definitely get into data without a CS degree
* Difficulty isn't anywhere near as close.  If I ruin a reaction in the lab, good luck getting that shit back.  I guess it's like dropping a DB without a back up, although it's much easier to back something up than it is to make an extra X amount of material in case, I don't know, you drop your flask on the floor, somebody knocks it over etc.  You live in a world of glass so breakages are inevitable
* General working is a lot easier.  Synthesis of a product can take a long time.  A long one is multiple days if not weeks.  You can turn something around quickly and have almost unlimited tries at it in DE
* Stakes being lower means it's not as stressful although it does mean you get a lot more bozos because people are less careful.  The introduction of AI has massively exposed who is and isn't a fraud (anybody who struggles without AI or ""has to"" lie on their CV is absolutely a fraud)
* Zero manual labour.  Much much lower chances of dying - I used to make novel pharmaceutical compounds which were definitely biologically active although since it has never been made or tested before (publicly), nobody has any idea how toxic the compound is.  So, it could either do nothing or murder you on the spot.  Fun fact: I once made something which was over 30 times more potent than fentanyl
* Salaries are way higher on average.  After 2-3 years of being in DE, I earnt double what I did as my final salary as a chemist after around 10 years in the industry

So what I'm saying is life is relative.  If you have only ever worked in tech, then yeah, the grass will always seem greener.  As somebody coming in from the other way in, it's definitely greener within the IT industry.",46
1qr1ah1,2,"Ehhh. I mean, imo there's always going to be stuff ai can't do, at least without an informed human nudging it in the right direction and keeping an eye on the outputs at some point? And we don't even know what *new* data work is going to be opened up/made necessary by ai at this point either really. If you enjoy the work and you're good at figuring new stuff out I'd say there's no need to jump ship to full-time goose husbandry just yet, trust yourself and your abilities and keep following your nose about what's useful/interesting to learn and work on. Yeah a lot of the straightforward stuff can be handled by an agentic approach, but in my experience there's always going to be weird new stuff cropping up and that often needs a meat-based neural net aka a human to figure out üòú",15
1qr1ah1,3,"I can't give advice, but can tell you (shared this another sub) that my company is aiming to reduce ALL IT staff to me. Can they pull this off? Not sure, but that's their ye goal.

They think with a combination of AI tools I'll be ableto do everything that our staff does. Even I'm kinda like, ""Am I about to be irrelevant soon?""

So, tell me about goose farming :)",5
1qr1ah1,4,">There is joy in browsing the internet without prompts and scrolling across website. There is joy in navigating UIs, drop downs and looking at the love they have put in. There is joy in minimizing the annoying chat pop that open ups at the website.

You may not find it a perfect metaphor but I'm sure people preferred going to the library and checking out old newspapers and encyclopedias to do research even after the internet and Wikipedia came around too. 

I get that it's a huge paradigm shift, and a lot of our careers are in danger at the moment. But (imo) AI is proving to be the ultimate floor-raiser so we as engineers are only shooting ourselves in the foot by not trying to integrate it as best as possible into our toolkit. 

AI should help us with all of our lower level tasks (boilerplate code, cursory analysis, test case writing). All of the other tasks that come from experience (sniffing out bad code, identifying bad architecture, optimization) will grow to be even more valuable.",6
1qr1ah1,5,"I‚Äôm concerned about A.I. removing a lot of the joy from work but it also adds it as well.

Like the data engineering field requires working with so many potentials tools because the market is so fragmented. ¬†It can be frustrating picking up something and trying to work out how to do what you want to do.

So in that scenario LLM AI is like a buddy that can point me where I want to go when I want too and it‚Äôs easy to work out if it‚Äôs offering poorly made up advice.

Now for setting up agentic AI and improving it, I mean for me that‚Äôs all too early to see if making good agents is actually worthwhile.

I do agree that A.I. is challenging the joy of work. ¬† Slop is a thing and it‚Äôs no good.",3
1qr1ah1,6,"I was an aircraft technician before this. Airline travel perks are amazing, I‚Äôm actually in first class writing this waiting to depart. I‚Äôm thinking of going back for similar reasons to you. Don‚Äôt feel like this job even produces anything meaningful most days.",2
1qr1ah1,7,I remember Bill Gates saying this somewhere 'AI is really doing a fantastic job but not as great as humans',2
1qr1ah1,8,Remind me! 2 days,1
1qr1ah1,9,Remind me! 2 days,1
1qr1ah1,10,Farming,1
1qqyv6f,1,"We run incremental run with offset (ie last 2 days of data until today), append everything to a landing table, and process from there. Each source has a different timezone, so the downstream will process and standardize all timestamp to timestamp_tz data type (we use Snowflake). From there we do the downstream analytics.

Thanks to timestamp_tz data type the timezone is always clear so we know how to control the time correctly.",3
1qqyv6f,2,"Why not just make the stupid thing hourly? Then when client_x‚Äôs time zone has a full 24, it can kick off?",1
1qqyv6f,3,"yeah this kinda pain usually ain‚Äôt just pipeline logic, it‚Äôs how time is modeled biting back. once you start mixin job time, event time, client local time, all in one place, everything get confusing real fast. helps a lot to separate it out like keep processing time (UTC) and business time (client timezone) as diff things in the model. trying to solve timezone stuff only in the scheduler is where sanity start slippin lol. cleaner time dims + clear grain makes incremental logic way less stressful. They talk about this kinda headache in **r/agiledatamodeling**.",1
1qqyv6f,4,A date dimension can help,0
1qqy67i,1,Ad,47
1qqy67i,2,catalog chaos is real,1
1qqy67i,3,gravitino sounds intriguing. how's it with data lineage?,-10
1qqy67i,4,gravitino sounds cool. how's its support community?,-14
1qqy67i,5,"That‚Äôs awesome to hear. Out of curiosity, have you guys also explored Apache Polaris which is also OS as well? Any particular reason for choosing Gravitino instead?",-3
1qqx556,1,"like this one? 

[https://www.reddit.com/r/dataengineering/comments/1ov1ug0/introducing\_open\_transformation\_specification\_ots/](https://www.reddit.com/r/dataengineering/comments/1ov1ug0/introducing_open_transformation_specification_ots/)",1
1qqx556,2,Because you‚Äôre pretty much asking to automating l verification of business logic right via MCP.,1
1qqx556,3,[standards are great](https://xkcd.com/927/),1
1qqx556,4,"Because for 1-20% of the fields it won't work.

This is going back to case-tool ETL tooling of the 1990s.",1
1qqvw49,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qqvw49,2,"learn to write sql,most people can't",21
1qqvw49,3,"You are on a right path and well aware, the market is brutal. We've had a long hiring phase and let me tell you what I've seen from interviewing:

\- Most larger companies get \~1000 CVs in a day. Most of them are trash tbf, but recrutiers won't take much time to scan your resume. Make sure it's easy to read. Make sure there is impressive stuff in there. Look at your CV and compare yourself with 1000 random people. Is there any way you can change something to make it better than your competitors?

\- THEN work on the technical details. You'll most likely have some of these 3 interviews: SQL/Python, Data Modeling, Data Architecture/System Design. If it's not a startup they'll most likely ask you concepts and not directly questions related to a platform like AWS. So understand frameworks: Orchestration, Deployment, Quality Testing, Monitoring etc. You can learn those directly with Airflow, dbt etc. or even build it by yourself in Python. Cool project to show btw.

\- At any seniority, people suck at SQL. At least for an Analytics Engineering role, this should be your best mastered skill. JOINs, GROUP BYs, Aggregations are the most basic skills. People failed so frequently on rolling averages, knowing when to use a window function and when not, how to filter by year or month, not knowing anything about how queries execute etc. I still can't believe I've rejected \~70% in a simple SQL round. Here's your first spot to shine.

\- Be a cool person. If we had a laugh and I'm between reject and pass, you'll likely pass.

\- For Data Architecture questions, learn tradeoffs: Correctness vs Latency, Storage vs Compute, Reliability vs Velocity, Efficiency vs Capabilities, Cap Theorem etc. When to use what + example use cases.

\- For Data Modeling its the same answer: Ingestion Layer (Event Logs, Snapshots) -> Facts (out of events)/Dimensions (out of snapshots) -> Aggregations/Data Marts -> KPIs. Understand what happens in every step. This is a usual dbt workflow, learn it by just moving some messy tables to create a final cool KPI.

\- If you want to add a personal project: Have a URL, let me click it and let me be able to explore it and think ""cool"". I won't ever use your project. But having something I could use already would make you stand out by a lot because you prooved you can deploy stuff.",3
1qqvw49,4,"Learn sql, for web I use data lemur, for mobile practice, i use the app query dojo¬†",3
1qqvw49,5,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qqvw49,6,The biggest skill will be learning how to interviewing well.,1
1qqvfu4,1,Good luck! None of us know why we ever got hired for our first job.,9
1qqvfu4,2,"They know your work experience. What you need to show is good personality, critical thinking, just be you! Don't be afraid to say I don't know but am sure I can figure it out.",2
1qqvfu4,3,"Don't sweat it. Everyone feels that way. You'll learn and get more confidence then realize you still have so much more to learn. That's life. 

In my first ever interview, they asked me to draw out a process on a white board. I was so nervous I ended up asking right there in the interview ""was I right?"". The interviewer said he doesn't know, he's not an expert in the field. He just wanted to see how I handled it. I ended up getting the job. You'll be fine.",1
1qqvfu4,4,"It‚Äôs important if you get stuck somewhere or start to run out of time to talk through what you WOULD do if you had more time and what considerations you‚Äôre getting hung up on. They want to know how you process problems. If you can demonstrate that you have the right mindset to solve them, it will matter less whether you actually produce a solution that‚Äôs 100% correct.",1
1qqsfmm,1,"Standalone orchestration like Airflow is nice to have when you are orchestrating many different technologies and need to link dependencies. It‚Äôs also nice when you need to ‚Äúreplay‚Äù data assuming you wrote your dags properly.

My company uses GCP‚Äôs managed Airflow called Composer and it works great and doesn‚Äôt cost too much.",157
1qqsfmm,2,Someone forgot to tell my $10b company,96
1qqsfmm,3,"Not sure why you concatenated Airflow and Hadoop into a single expression because they are entirely different tools. ¬†You ask about orchestration tools, of which Hadoop is not but‚Ä¶ airflow is still INCREDIBLY common, and growing.

Nobody is really writing net new greenfield code in hadoop anymore though.

and for the record, because this sub REALLY seems to struggle to understand:

AIRFLOW IS NOT AN ‚ÄúETL TOOL‚Äù

‚ÄúBut I can do etl on it‚Äù. You can also do ETL on a raspberry pi or your iPhone with the appropriate amount of motivation. Airflow is for orchestration and it orchestrates pretty much anything- ¬†not just ETL!",94
1qqsfmm,4,Last 3 companies I worked for use airflow.,21
1qqsfmm,5,"Airflow is still very prevalent and growing. Also, not sure why Hadoop was included in that concatenation. Very different tools. 

Airflow / Dagster - orchestration tools. These excel in orchestrating the flow of data between various systems. Think website -> api -> database -> analytics report. Hadoop is a ‚Äúdead‚Äù technology. Essentially makes no sense for greenfield but some companies have legacy platforms that still need support.",12
1qqsfmm,6,"Marketing is a powerful tool, I was talking with some early career data engineers and they didn‚Äôt know that you can run spark outside databricks.",8
1qqsfmm,7,"Databricks is literally managed spark. I will leave it to the imagination where spark comes from. I don't know of any company not using airflow, even if they use it poorly and pretend it's just roidrage chron.

HDFS should be going away, since so few companies ever actually built a lake with unstructured data, but I still see it out there all the time.",6
1qqsfmm,8,"Here you are,
https://airflowsummit.org/sessions/2025/airflow-openai/
Anthropic(Claude) also ask for airflow in their analytics role",5
1qqsfmm,9,"What we see these days is the ""managed version"" of open source frameworks and technologies. Dataproc for Spark, Cloud Composer for Airflow, Pub/Sub for Kafka etc etc...

Hadoop has been replaced by Spark, I agree. But Airflow I reckon is still used for scalable complex workflows. I use it for my work on the Linux Mint platform, works well.",4
1qqsfmm,10,"I work at a serious large company that's a bunch of sub companies with every new product being spun up by a team that's clever and picked a new data backend from the other 100 teams. 

My team does a lot of skunk works 'oh no we didn't plan for the data ' kinda projects and have landed in airflow for all of it. 

There are absolutely better products that are more specialized but we have wrote our own modular dag generators and about 2/3 of our dags are 10 line yaml files now and we can easily restate data.

It's honestly super cool to be able to sensor for s3 files, kick off a pipe, sensor for its end, kick off dbt, all from the same 50-100 lines of easy to read python.

I've recently also started to look at kicking emr jobs off for some of the huge pipes that need external compute.",9
1qqoayv,1,">¬†I‚Äôd like to strengthen my long-term position, fill in some theory gaps, and - now that I have a young family - set a good example by continuing my education.

The questions to ask are two-fold here:

* What do you think you'd learn from doing a degree?
* Is this a balanced professional-personal decision?

I see this a lot with ""already working but considering further education"" posts - there is having a degree for tangible benefits (if you imagine it's very difficult to be a doctor or lawyer without an actual degree) and there is having a degree for personal reasons (some people want a degree simply to have a degree).  

Nothing wrong with the latter, although it's one of those where if you're in a position to obtain a degree with little to no consequence to yourself (I'd say this describes option 2 accurately), and don't mind sticking the time in, then by all means go ahead.

Conversely, if you're already working, and have been working, then additional education, unless you have been told with absolute certainty that's what's holding you back, doesn't hold much value.  You'd learn infinitely more on the job, anything you don't know you can probably teach yourself, all the while you have the flexibility of being paid and choosing when you study - something I can imagine is a lot more valuable when you have a young family rather than being locked 3-6 years into a degree you feel obligated to finish.

Between the two choices, option 2 to get it over and done with although would say none of those options are really worth the time investment and you'd get a better return spending time with your family and pursuing your hobbies.",1
1qqoayv,2,"I started a degree apprenticeship (data science) having been working as an analyst for a few years. And moved to an engineering role within the same organisation about halfway through the course.

I did find that the breadth and depth wasn't particularly great and getting the degree ended up being more evidencing that I knew stuff I already knew than actually teaching new things. 

My organisation did give me a pay bump on graduating though. So worth doing for that.",1
1qqoayv,3,"So, please don‚Äôt take this personally, but I honestly believe that Degree Apprenticeships should be for school age levers. 

I‚Äôve worked with apprentices for a good amount of time and even had responsibility for a cohort of around 130 of them a few years ago (in the Data space) - and seeing people with a slightly less traditional background go out and nail both a degree and their professional life is hugely gratifying. 

That said with 13 years under your belt you would likely be very under challenged.  

Personally I‚Äôd see if you could get on a Masters course of some way (possibly with a conversion). 

Also: I‚Äôve seen some senior people go for apprenticeship and ‚Ä¶ it‚Äôs a doss, way of doing less at work and taking advantage of the system. Put it this way, I‚Äôve not seen many/any top quality people putting themselves for DA‚Äôs - even ones without university educations.",1
1qqo2n7,1,"You can find our open-source project showcase here: https://dataengineering.wiki/Community/Projects

If you would like your project to be featured, submit it here: https://airtable.com/appDgaRSGl09yvjFj/pagmImKixEISPcGQz/form

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qqo2n7,2,"hmm, looks like it didn't save the picture. give me a second.",1
1qqndyj,1,"As a former software engineer turned data science student, id say backend python/sql is transferable but theres a lot of concepts and tools that are foreign to SWE‚Äôs that are outside their skillset. Id recommend reading up on the data lifecycle, modern tools, data system design before you fully jump ship",6
1qqndyj,2,">With frontend, backend, and full stack SWE roles becoming saturated and AI improving, I want to future-proof my career. I‚Äôve been considering a pivot to Data Engineering.

>Is the Data Engineering job market any better than the SWE job market? Would you recommend the switch from SWE to Data Engineering? 

Moving from SWE to DE strictly to future-proof your career is not a good idea in my opinion. The same issues you mentioned are impacting the DE space, and I'd argue the DE was already starting to get saturated over the past few years as people from other fields wanted to pivot.

>Will my 3 years of SWE experience allow me to break into a data engineering role?

Your experience could help, with a big asterisk here. Tech-forward companies who see DE as a specialization of SWE will value your background and may hire you for an entry level DE position. But generally speaking DE requires a breadth of knowledge, so while it's possible I think you might have a difficult time breaking through without any experience with (or directly adjacent to) DE.",7
1qqndyj,3,"Honestly i would advice against being a data engineer in this market.
We are seen as a cost center , basically what that means is the most orgs have a bad work culture compounded by the fact that every andy thinks LLM can get the job done .
So I would not get into it right now.
Stick to web dev but go full stack.
Right now the biggest issue in orgs I see is that the AI code is bloaty and you need specific niche react knowledge for there over engineered stuff.
Tooling is where companies are going right now",2
1qqndyj,4,Here‚Äôs the thing. Are you able to articulate your company‚Äôs business model and understand your position and ability to affect the bottom line?  Figure that. Out and work from there. I keep seeing technical people sticking to niche positions when you should be looking at the bigger picture. Rant over,1
1qqndyj,5,"I see front end web development nearly vaporizing in 5 years. Yes, go into DE. While AI is still a threat it's possible DE will be safe longer.",0
1qqm5vk,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qqm5vk,2,"Automate as much of your job as you can, then start actively seeking out people's pain points and solving them with data. Keyword is ""active"" here, i.e. talk to people, chat it up. Once you feel like you've established yourself as more of a problem solver who's an asset to the business and less of a ""data guy"", ask for a sizable raise and pull out your list of solved business problems.

If you don't get your way, start looking for somewhere else to go and take that big list of wins into an interview. Do that and you'll move in very much the right direction.",134
1qqm5vk,3,">I feel like I do data engineering, analytics engineering, and data analytics. Is this what the industry is now?

You wear more hats at smaller companies and on smaller teams. GenAI is also enabling a lot of people to attempt to expand their skillsets and take on tasks that would've previously been outside of their wheelhouse.

>But for long-term career growth and salary, I don't know what to do.

In a vacuum, if you're strictly talking about growth and salary you probably want to consider pivoting into DE from DA. In fact if you search the subreddit you'll find a ton of posts from people trying to do the same. In some ways you're in an advantageous spot because you're already getting hands on with more technical tasks.",31
1qqm5vk,4,">60k

Currency and location really helps people.",16
1qqm5vk,5,"When you say ETL pipes, what are you using to build, test, and deploy?",8
1qqm5vk,6,I do everything you say for what it‚Äôs worth to you,5
1qqm5vk,7,This all seems about right. They got you doing all the work cuz you‚Äôre the JR with talent. The better you are at your job the more work you will have to do. Keep this up so they see you as a all star and try to get to the point where you can be Sr. then you can slack off,3
1qqm5vk,8,Yes this is the industry now. They expect you to be a super hero while they follow garbage manual processes. Make sure you are being paid  handsomely for it or leave.,3
1qqm5vk,9,"If you're at a regular company, almost no one else at your business has done anything half as cognitively demanding as building out end to end business reporting. And you're getting paid peanuts.",2
1qqm5vk,10,"Sounds like youre up for a promotion. Here's the thing though, they'll never see you as the engineer once they paid you like an analyst.


Best thing I've seen some folks do is hop to consulting for a year and then come back if they'll have you, at a properly market adjusted rate.",2
1qqm3io,1,Wish them the best and move on my friend.,85
1qqm3io,2,Just connect excel directly to the OLTP and call it a day,68
1qqm3io,3,"LMAO ... I am a senior manager of two teams of five engineers each, and it took us eight months to build a consolidated data source from about eight databases, two CRMs, three ERPs, three license servers,post-merger.  We now know more about the acquired company's license server than their own people ever did.

Why are there employers who even approach a single engineer thinking this is doable in less than six months minimum?

I would have laughed out loud at their senior leaders.",53
1qqm3io,4,"When I take my car to the shop, I always tell the mechanic how long it should take to fix my car.",27
1qqm3io,5,"Did you confirm you are both using the same shared understanding of what a cube is, and the steps to build?

On the surface, this sounds like a case where the TPM or Data Analyst layer is missing.",11
1qqm3io,6,If it‚Äôs that easy you should probably just do it yourself.,7
1qqm3io,7,"I got told to make a cube in a week once, the first week at that job. I told them that was ridiculous because I knew nothing about their KPIs. They insisted, so I threw an mdx cube together in a week that counted keys and summed the decimals.

It was entirely useless but I worked there for 7.5 years and rebuilt their entire backend from scratch.",5
1qqm3io,8,"""data is clean"" usually means ""it's clean now but the old stuff is a nightmare. Good luck""

Just tell them they can have it right or they can have it right now. It's going to take x time to do it right",3
1qqm3io,9,"I'll go off the client pissing train.


Your client probably wants to have something tangible, fast, to prove the value.


It's obvious that you don't have the customer's trust, and probably you haven't attempted to gain any yet. You simply follow what you personally think is right, without trying to solve the problems for the client.


In short, you have not proven yourself yet, to be the trusted advisor who can burn their budget at will.


You could offer to do quick and dirty raw -> cube with disclaimer that it's not for long term and should be revised later once the use cases are clearer.


This this basic software consulting.",3
1qqm3io,10,"Well in fairness, I can build a basic cube in a few hours using ancient old Cognos software, I did a super basic one for someone yesterday in under an hour. Maybe they‚Äôve had experience with something like that, and are defining a cube as different to your definition?  Could just be an interpretation error.",4
1qqlucd,1,Lol! Though think you need some more rows/combinations on the bottom one,2
1qqlucd,2,"Self Join: Keeping it simple üòÖ, Cross Self Join: Well, that's a whole other level! üôÉ",1
1qqlucd,3,Loll,1
1qqjh1h,1,">Or what do you think the essentials for a data engineer ‚Äústarter pack‚Äù should contain?

Starter pack for anybody who has just got a job is chill tf out.  Plenty of time to be stressed later.

Genuinely, nobody can tell you what you're going to be working with apart from your new/current team.  Asking us what to learn might be a massive waste of time.  I completely get that most of these questions come from a place of anxiety and desire to do well (I asked exactly the same question when I got my first job).

You'd be much better off coming in eager to learn than already pre-burnt out.  Nobody expects anybody to be shit hot when they start although absolutely everybody expects improvement.  A sustainable career is one where you can turn up every day and keep going.",2
1qqjh1h,2,"Sounds like more dtaabricks 
Ec2 sagemaker DBT airflow as a start

Do you know what they use?",1
1qqi4gz,1,I use AI to generate documentation including markdowns and mermaid flow charts. It might not be perfect but it's a hell of a lot better than doing everything from scratch.,5
1qqi4gz,2,"write for your intended audience and not just for yourself.

remember there's mamy forms of documenting information - tutorials, how-to guides, reference material etc

https://engineering.squarespace.com/blog/2023/part-1-learn-the-different-types-of-technical-documentation",2
1qqi4gz,3,"Sounds like you are only a few steps away from using sphinx docs. You can auto document and create docs from the source code, then your documentation can be compiled into a PDF, static site, confluence docs, almost anything.",1
1qqi4gz,4,Had a very high up manager insist the code is the documentation at every project meeting. You bet your ass our whole shop had next to no documentation.,1
1qqhvik,1,You should probably just send it to a deadletter queue.,1
1qqhdks,1,Isn't this the standard way of doing things? You see it everywhere with ADF. Honestly all you need for a lot of companies.,29
1qqhdks,2,"I don‚Äôt know if what we do is ‚Äúmetadata driven ingestion‚Äù but we just write our pipelines in Python and define source tables, fields, descriptions, target schemas, etc in a config file. Script reads from this, creates the tables if they don‚Äôt exist, and runs the ETL.",11
1qqhdks,3,"I have seen metadata-driven ingestion work well, but only when people are honest about what it is good at and what it is not. In my current setup (airflow + spark sql) we built a framework for the boring 80% of ingestion. Basically ever source gets a config file that defines things like:

* source location and fromat
* expected schema and key fields
* load mode (append vs merge)
* basic column-level transforamtions
* partitioning and retention rules
* data quality checks such as nulls, uniqueness, etc

Airflow just reads the metadata and generates the DAG tasks dynamically so adding a new table is as good as adding a new config and redeploy. That works actually, better than what I was expecting.

Where these frameworks usually die is when people try to stretch them into solving the hard 20% - business logic, weird timestamps, broken upstream APIs, one-off edge cases,. At that point you are building a DSL inside YAML, and nobody wants to debug YAML on friday.

The happy middle ground is metadtaa drives ingestion and standard bronze/silver handling. Anythign complex gets real python module with tests. Configs stay declarative so cannot do programming in configs (which is good).

Funny enough, I‚Äôd love to open source what we built but cororate legal treats sharing code like i am smuggling beer.

So yeah - do it as long as you are focusing on the repeatable 80% boring stuff. Good luck!",9
1qqhdks,4,"We implemented something similar, I think the issue is that it is so daunting to normalize the sources and how they come in. I felt we constantly had to add new fields/rules to the confit to handle. It gets to a point where it feels more complicated than clean code.

So it really depends on the scale of this data and the sources the data comes from. If they all come from csv or txt, that makes it a lot more feasible. If it comes via API, well that is much more difficult to normalize as authentication, pagination rules, etc can be so different. You suddenly need multiple points of,low within the config and it can get unreadable.",3
1qqhdks,5,I did for ingesting and shredding out JSON documents into normalised tables. It worked because I spent a lot of time thinking about the design and about how to populate the metadata in the 1st place. If you go off half cocked on either one you'll go down a rabbit hole,2
1qqhdks,6,"We are working on a metadriven databricks python medaillion implementation at the moment.
The initial setup costs relatively a lot of effort, but it scales really well once the foundation is there.
I would say, you need at least one experienced programmerer in the team to set this up, because if you dont follow good programming principels, things get complex and ugly quickly. 
After finishing our pilot, we had quite a lot of rework.
So far I really like our setup and I see lots of potential for the future.",2
1qqhdks,7,"Dlt dbt model generator sort of does this; generates a project structure with staging and int models etc included based on schema of known sources (ingested via dlt). 

https://dlthub.com/docs/hub/features/transformations/dbt-transformations

Though it requires a paid licence and I also haven't tried it.
Not sure how configurable it is, not much based on scanning the docs.",2
1qqhdks,8,Use dbt,2
1qqhdks,9,"The more dynamic approach you want to implement the longer you have to prepare your input data and think about architecture. Recently, I‚Äôve open sourced one of such tools. It can produce output SQL models based on the YML config but someone still needs to link sources to targets either in GUI or directly in YML file. It‚Äôs an easy thing to do if you have strict rules/architecture. However, it becomes much harder and probably won‚Äôt work if you model your data in chaotic way",1
1qqhdks,10,"Yes, I've used some ""out of the box"" solutions and have built my own. 

What environment and tools are you working with?",1
1qqgdkj,1,Start to?,28
1qqgdkj,2,Snowflake has already started this with [Manufacturing](https://www.snowflake.com/en/news/press-releases/snowflake-launches-manufacturing-data-cloud-to-improve-supply-chain-performance-and-power-smart-manufacturing/) and [Healthcare](https://www.snowflake.com/en/blog/snowflake-new-healthcare-life-sciences-data-cloud/),4
1qqgdkj,3,"> Snowflake and Databricks have achieved steady growth, in large part due to their consistently high customer expansion (NRR) at about 140% and 125% respectively. 

You've got the numbers backwards. It's 140%+ for Databricks. The chart below that paragraph has the correct numbers displayed.",1
1qqgdkj,4,It‚Äôs already happening,1
1qqgdkj,5,"I think you‚Äôre right about their strategy, but I‚Äôm skeptical they can actually pull it off. At their core, both are infrastructure companies, and it‚Äôs incredibly difficult to pivot from building developer tools to selling verticalized business solutions.

Their entire reputation is built on technical leadership. They won originally by outperforming legacy players. Think about how they effectively outmoded Redshift and Vertica by simply having a more modern architecture. But the most successful ""vertical"" companies aren't actually technology leaders. Nobody uses Salesforce, Adobe, Intuit, or Docusign because they have the best code. They use them because those companies understand specific business workflows better than anyone else.

I'm skeptical that companies that big can change their product DNA that drastically. It is hard to move to selling finished houses when your entire culture is designed to build the world's best power tools. This reads like they are overly capitalized and their core businesses aren't growing fast enough to justify valuations. imo shareholder value will probably get destroyed as they chase revenue in spaces they don't understand deeply.",1
1qqfdx8,1,"I think the future for this field is bright but I do see that landing your first job is becoming increasingly harder, so I would worry more about how you are going to thicken your resume.",1
1qqe8e7,1,"because I don't need to spend 30-45 minutes in a car to drive to a building to log on to a laptop and remote into a server and do some work. I can just do that at home. Everyone I work with is in different cities across the US, and sometimes different countries. What's the point of the cloud if we all go to a building to work on it?",28
1qqe8e7,2,"I am currently in a job search for hybrid, and my friend is looking for remote only. I would say we are seeing about the same amount, but I am looking at basically any city. If you are looking at just one smaller city, yeah remote will for sure overshadow those.",6
1qqe8e7,3,"Senior manager here.

Fivetran, dbt and snowflake are cloud based.  AWS EC2 is cloud.  Why do I need anyone to be physically in an office versus leveraging the best talent I can find anywhere in the world?",6
1qqe8e7,4,"I could be totally wrong because it depends on many factors, but because Data Engineers are so highly paid, companies in high cost-of-living places (like the US cities LA, NYC, SF, Seattle, Chicago, and DC) look for remote workers in lower cost-of-living places because they will accept a smaller salary.",10
1qqe8e7,5,"Office space is expensive and if you don‚Äôt need to pay for it, or can pay for less of it than everyone else, you‚Äôre at an advantage to competitors who do.",2
1qqe8e7,6,"I‚Äôd say it‚Äôs a combination of a lot of things.. cloud based tools, largely independent work and projects, ability to pull from talent across the country (or world), and personalities for a lot of DEs. Companies have realized they can get better talent while reducing required space and salary expenses in some cases while getting the work done",2
1qqe8e7,7,"Its a mid-level and up role, and it requires focus and less ongoing collaborative face to face work than some other positions. So it tailors to remote nicely. The other factor is it takes a certain scale of company to need a de, so unless you are in a large metro area there likely aren't going to be a ton of on prem listings for your area because the size of company that needs de isn't as common around you.

Also its an employers job market right now, with a lot of tech layoffs, by widening the geographic net on a backend role they are more likely to get a larger pool of qualified candidates to pick from. 

And the biggest thing is a lot of orgs are spread out these days and a de is integrated with large range of departments in many cases, so it doesnt really matter to be on prem if on Monday you are talking to someone in California and Tuesday someone from Montreal and Wednesday someone from Sweden, or if you are like me all and more in the same day",1
1qqe8e7,8,Where can I find remote jobs ? I have been laid off recently and want a remote job,1
1qqe8e7,9,Why is my lobster so buttery?,1
1qqe5up,1,"Learning new tooling is not a big deal. If they use computerized screening process, you will never be able to satisfy their demands. That is a loosing game. Instead, try to speak to actual humans who will understand your skillset and expertise.",2
1qqdpgu,1,"Might sound cliche but the hardest part is consistency, not intent. If 2 people answer the same request 2 different ways, it creates unnecessary risk and it'll make you look bad even if both are true. Having one approved narrative per system type (prod/logs/backups/vendors) helps.",6
1qqdp7l,1,"Every 6 months, there is a new tool in the market and it has been hard to keep up.",142
1qqdp7l,2,"What exactly is implied by generalist in terms of data engineering?

Let's be honest aside from the obvious things like SQL, Python and Modelling, most engineers of doing about 20-30 other skills or tool sets as it is.

We're effectively already in a role that's the ""Jack of all"" trades, and I prefer the industry doesn't add to that role by being ""a master of none"". I want to work with other professionals actually who know what the fuck they're doing.

Although I do feel like this role exists in some places, for this reason I honestly don't see full stack data engineers as a realistic pathway. It's a huge issue in the industry already that the roles of data professionals are not adequately defined and we're just expected take on everything.

But that's just my honest opinion.",77
1qqdp7l,3,"I always just go back to the basics of computing. Any full stack tool is just an abstraction over that. The important things to understand are always data structures, OOP, and algorithms such that you can write pseudocode to solve a problem and not depend on a single language. Be an expert in SQL. Understand what memory, CPUs, and disk space are in a single machine. It‚Äôs good to know how computers work in general. Understand distributed computing and the Spark framework, so you can compute large datasets across many machines. Understand CICD with git and Jenkins. Understand the fundamentals of GenAI and know what it‚Äôs good at (summarizing and analyzing large text or logs / finding patterns in data points, deciding next steps in ambiguous situation, generating boilerplate code) and know what it‚Äôs not good at (it often will produce incorrect code and may hallucinate so always triple check its work, and does not need to be used to do things that are deterministic - I see a lot of overkill with GenAI which wastes money and time). 

Once you have the foundation, you can adapt to any tool.",25
1qqdp7l,4,AI is only good with good data. Im starting the pivot from data to AI engineering because I think people with a background in data will have an advantage in that job market,16
1qqdp7l,5,"Stick it on your CV I guess and charge a lot of money for it???

To be truthful, there is very little on that info graphic that I do not have experience with",11
1qqdp7l,6,"Meanwhile: PMs ask you daily, How are we doing today? 
The tension is to start ingesting more requirements to lower part of the chain while wiping out the middle managers which doesn‚Äôt make any value of it. 
Soon will be an AI checking on the daily‚Äôs.
More people burnt out",6
1qqdp7l,7,"I've always been a full stack data engineer tbh. From ideation to ml production as well as everything in between. Including building frameworks, reports, dashboards, eda, dbt projects, ingestion pipelines, cicd, etc.

My educational background is a blend of econ and cs if curious. I also just wore a lot of hats and at small companies before I got to where I'm at. At small companies you always kinda have to be full stack.",6
1qqdp7l,8,"The expectations are getting pretty insane. Echoing another redditor, DEs are already learning so many things that this shift honestly devalues the skill of a specialist Data Engineer. DEs need to be able to communicate expectations on what is reasonable for a single person to do and advocate for additional specialist DE roles because this wont be sustainable nor will there be a premium because if companies find the output of a generalist DE is the same as a specialist DE, it discourages people to specialize which is bad for our craft.",4
1qqdp7l,9,"We‚Äôve been using the full stack dev concept for many years. Our tech stack is intentionally simple: SQL, Tableau, some Python for automation / GenAI and DataRobot for ML. We are a large healthcare provider, so the subject matter and data engineering are tough. You lose some efficiency by not specializing, but gain a ton in work fulfillment and elimination of handoffs. I‚Äôm a big fan of the concept, but this would be hard to do if you have massive tech sprawl.",4
1qqdp7l,10,"As someone who has basically been shoved into ‚Äúfull stack‚Äù 

There are too many damn products and ecosystems to keep up with. We know enough to make problems that then the specialists fix. 

My work life is always a series outrageous asks that are given the same timelines as a specialist. Example ‚Äúingest, organize, document, clean, and insight all of this data we got from our intern who learned how to do a mass export and we pay $30 an hour to do‚Ä¶. No no. Buying a connector is too expensive. Her job is to extract, manually rename, and drop files to this s3 bucket. Yes they are some insane format. Work with it. And at the end I want a dashboard that tells me the exact reason why sales were low‚Ä¶. Oh and make another version with an LLM I can talk to about my data. No I haven‚Äôt thought about questions, I just wanna talk to it‚Äù

I hate what I‚Äôve become. I hate that executives see me as some golden cow. I hate that they think this is normal. 

Can I make that? Yes. Will it be good? Fuck no. It will be taped together with duct tape and anger.",4
1qqdoh4,1,"Pretty difficult if English isn't your first language learning here although we do have resources within the sub.
Our first recommendation is search through the community [Wiki](https://dataengineering.wiki/Guides/Getting+Started+With+Data+Engineering) which answers a lot of general questions about a career in data engineering.
Secondly, try the [search function](https://www.reddit.com/r/dataengineering/search/?) by typing the topic of your question into the search bar.",3
1qqcuy7,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qqcuy7,2,">Like, are they actually enough on their own to get entry level jobs?¬†

It has been a really long time since doing ""a course"" is enough to get a job.  Courses are an introduction to a topic.  Not a complete solution.  You do not do a course and become a complete programmer in the same way graduating with a medicine degree makes you ready to carry out complex, life threatening surgery.

A common misconception is that all you need is a course.  Personally, I did an online courses.  I did a few actually.  6-8 months later, I got my first DE role with no experience whatsoever.  Without any further detail, this makes getting into DE sound really fucking easy.

What isn't included is all of the work outside of those courses.  I was literally writing code, watching videos, reading blogs, writing stuff down, and generally thinking about programming for 60-70 hours per week every week for around 6 months.  And when I say writing code, I wasn't following tutorials.  I was googling the fuck out of the internet on how to try and do what I wanted to do.  As you can tell, that's a lot more than 70-90 hours.

On top of that, what's also not mentioned is my career before being in DE involved interpreting data on pretty much a daily basis.  Sometimes, stuff which you can't even measure, test, or see.  This makes working with data which travels between two places quite straightforward relative to somebody who hasn't had the same background as me.

>I need to know so I know if I'm simply wasting my time. If they are nice additions that reflect skill, but at the end of the day, not enough on their own, and businesses really want a college degree.

I'd be lying if I said a university degree doesn't make your life easier.  Of course it does.

That being said, you are only wasting your time if you think the course is ""enough"".  There are lot of soft skills which you can't really measure and they are a lot more valuable than the degree itself.  You have the massive benefit of not being time gated with what and how you learn which is balanced against not having a traditional degree.

My recommendation is work very heavily on your soft skills.  How you come across, how you analyse problems, present findings, and ultimately implement solutions will take you somewhere.  How long it will take, I can't possibly say.",5
1qqcuy7,3,"Less likely but possible. I have seen an entry level software engineer hired in with no college experience and self taught. But briefly working with them, I didn't see how much better or on par he was with others.

I know there are plenty of engineers that never went to college, but they were hired in years ago pre covid. I am sure recruiting practices and circumstances are vastly different.

I assume recruiters are going to weed out non college grads unless you have something going for you. Like internship experience at a decent or good company. So unless you have something that stands out from the typical compsci college grad, why shouldnt a recruit pick the college grad over someone that went through workshops and training courses?",3
1qqcuy7,4,"I got hired because of the personal projects I built and put on GitHub, not because of the online cert got that taught me the basics.

The cert got me started but the projects were a lot of hard work and I learned more doing those than in the course.",1
1qqcuy7,5,Honestly no.,1
1qqcuy7,6,"The certifications are worth nothing because anyone can skip through the course and get them.

What matters is what you learned from it and if you can apply it. Showcase proof that you understood the subject. Create a project that helps you in your day-to-day life.

E.g. for me I once conncted my nutrition app and fitbit data to a google sheet and analyzed the data in there. How do calories influence my sleep etc. That sounds 1000x better than most course projects because you have an actual story to share around it.

If you can showcase the applied knowledege, you're good to go :)",1
1qqcuy7,7,"Understand programming and data, and build some personal projects showcasing that. If you dont have a degree then it's going to be harder but not impossible. But in the end, learning and trying is completely free so just do it.",1
1qqcuy7,8,"It's a good idea to concentrate on soft skills, and online courses can undoubtedly aid in their development. Additionally, Pathway programs have a strong emphasis on the integration of faith and professional development, which may provide benefits beyond technical proficiency.",1
1qqbp2q,1,"It can only improve your resume and might increase your chances, I don't see a reason to not get it in your situation.",1
1qqbp2q,2,You will be preferred by service based orgs if you have certs,1
1qqandr,1,"Your team should have onboarding documentation for these scenarios. If not, create one. VPs drool over new hires being proactive",54
1qqandr,2,"At the very least, what's needed for development (programs and versions) should be documented as well as any recommended ways or known conflicts in utilities.",13
1qqandr,3,"Even at places with onboarding documentation for new team members the documentation is usually out of date unless they are constantly onboarding new team mates.

So yeah I‚Äôd say this is a common pain point at most orgs. That being said if there isn‚Äôt any or it‚Äôs out of date your team mates should be helping you.",13
1qqandr,4,"In my experience, yes. That was the hardest part for me was doing the setup. I documented all my troubles along the way to help future devs.",6
1qqandr,5,No I expect on boarding documentation however I also do know what I want on my machine and so I have a good view of what I will need to install and set up in order to do my job the specifics of the organisation though should be provided with step-by-step instructions,5
1qqandr,6,"It is a mix.  There should be an expectation that you will need to be provided with details like project names, access to the approved tools repository (if you don't have admin access to install any tools you want), and that authorizations will be created for you (or you will be given a list of authorizations to request)

But, you should come in with an preferred set of tools to use.  When you are doing RDBMS work, DBeaver. VSCode for an IDE. Knowing how to download and install the SDK for your given cloud provider. Notepad++ for your text editor, etc.  

With the expectation from above, that if there is an approved list, you'll use the approved tools in place of your preferred tools.

Once you have the authentication/access details, there should be an expectation that you can then configure the machine yourself with minimal assistance.  So, a mix of self-help and shared details.  

The bigger the shop, the more the expectation should be that there is a documented (if out of date) way to be onboarded and a list of preferred tools, that you are capable of getting working given the correct details.  Smaller shops, it is a bit looser, but still expecting you can get the tools up and running and will just need the configuration details to plug in.  And, you will be expected to be able to verify access and troubleshoot issues to some extent (either yourself, or with your team/the corporate help desk, depending on the issue)",3
1qqandr,7,"They feel caught that there is no onboarding material + they usually only remember half the stuff because it‚Äôs a one time setup that they did months or years ago for themselves.

It should be normal to onboard a ne colleague. Even with cloud providers, setups can differ between companies so none should expect that the new hire knows everything environment specific from the start.",3
1qqandr,8,"Shows lack of maturity of the company.¬† Id be wary.¬† They should have a wiki setup, companies have used Confluence where Ive worked.¬† It could be a good opportunity for you as the person to establish good documention or an eventual nightmare.",1
1qqandr,9,"I think specific to your point - setting up VS Code - is your problem. Thats your chosen IDE and you should research and setup your IDE. There are often many ways to skin a cat in VS Code, its down to you to decide each extension.

What SHOULD be available is configuration info; connection strings, what security info you might need (for example if you should use elevated account privs at any point.)",1
1qqandr,10,"You‚Äôll have time to ramp up for you to figure it out

There will be internal documents for you to look at

Worst case, ask someone and just show them you‚Äôre eager to learn 

They might judge a little bit the alternative (staying quiet) is a lot worse",2
1qq9o22,1,When you director wants to become big data based system‚Äôs leader,125
1qq9o22,2,"An RDBMS stores data, Spark jobs process data - they are not the same type of thing",39
1qq9o22,3,"focus on the ideas for now. e.g you have tools to handle massive data and tools to handle smaller sized data. 

Having experience in both is important on the long run, simply because small data can sometimes have tons of insights, and massive data can be filled with noise.

and most importantly in data engineering, never underestimate how many people think that they need massive data tools when they have small data and VICE VERSA... e.g companies with massive data trying to fit it all in pandas with 8gb of ram",14
1qq9o22,4,"Most small-medium sized companies, and large companies with low data maturity don‚Äôt need spark or distributed processing. But once you start getting into TB/PB territory it becomes critical. A lot of it is industry dependent. My current company is advertising tech and it‚Äôs critical because we process hundreds of millions of events per day. Compare that to when I used to work at a regional bank and the biggest table we had was like 30M records, so we could do all processing in SQL server itself.",13
1qq9o22,5,"It basically comes down to OLTP vs OLAP needs

RDBMS are optimized for OLTP, which is coherence, precise small scope fast fetching, small precise fast joining and processing, but they also do an excelllent job at small sized OLAP workflows.

OLAP systems are optimized for large fetches, large joins, large processing etc, and do not require as much speed for small fetches, they don't usually involve thousands of concurrent edits so coherence is less complex and less costly to maintain, and most importantly, they scale well with size, they usually* use cold storage and distributed processing.",10
1qq9o22,6,"Think of data stores being in 3 separate categories:

1. Traditional SQL databases (RDBMS) - MySQL, Postgres, SQL Server, etc.
2. NoSQL - MongoDB, DynamoDB, Neo4J, etc.
3. Warehouses and data lakes - Azure ADLS, Amazon S3, Redshift, etc.

They all store data, but for different purposes. RDBMS systems are used for storing application data that gets read and updated by users all the time (think of user profiles, posts, comments to posts, orders, etc.), and these store data in relational row-based form under the hood, and have their own internal mechanisms for data processing related to application business logic, constraint and transaction enforcement (OLTP for application data).

NoSQL systems also store application business data, they're preferred in the use cases where RDBMS' strict constraint, transaction, schema enforcement and strong consistency become a bottleneck - mainly, data partitioning. It's a big and complex topic to explain in one sentence, but generally speaking, NoSQL systems sacrifice those perks to some degree to achieve native and easy horizontal scalability across multiple servers in a cluster (don't confuse with read replicas), which is a tradeoff that's OK for many modern use cases.

Warehouses and data lakes is where that application data later lands to from the main database(s), for the purposes of analytical workloads, BI, AI/ML (OLAP workloads). The reason data is placed in a different place for these workloads is because it needs to be organized differently under the hood to be efficiently queried for these purposes (keywords to look up - columnar data format, parquet, OLAP) - while application database needs to be fast for lots of small individual writes and reads per row, warehouses need to be very fast at batch querying, big scans and throughput. And Spark is a data processing tool, that does distributed processing for warehouses and data lakes, not for OLTP workloads. Also to keep in mind, if your data is not that big and you can process it on one server without any problem, you don't need Spark, a custom Python script using e.g. Pandas dataframes for transformations will do the job without the overhead of setting up a cluster.",4
1qq9o22,7,"I want to add to what others have said, one thing that your standard OLTP monolith needs is more management. You have to worry about indexing and fragmentation, amongst other things, that require upkeep. The analytical databases usually don't need that, so you generally pay more for them but you also don't need DBAs to manage them. Spark is overkill for the majority of people who use it, but spark allows software devs to not sit in SQL all day, if they don't want to.",3
1qq9o22,8,"When your RDBMS system takes 6 hours to return a query result (it fails at 2 hours) 

When the RDBMS has pipelines so complex that no one understands the whole thing, even with decent documentation.",3
1qq9o22,9,"RDBMS are built for different workload, next question?

Surprised that didn‚Äôt cover that in the book tbh.. but then, I‚Äôve not read it :/",2
1qq9o22,10,It‚Äôs a generic book that doesn‚Äôt actually teach you how to do anything.,3
1qq98ku,1,"Still seeing dbt tests as the baseline but they catch most issues in my experience. The ""weekly slip-through"" you mentioned tracks with what I've seen.

For observability layer, Monte Carlo if you have budget, Metaplane if you don't. I'm building AnomalyArmor (bias obvious) but happy to talk about what we're seeing work across the space.

Agents for data quality is a bleeding edge pattern. I like it, but I'm also an engineer that embraces AI technology.

Like most things, you get out what you put into it. If you're technical enough to create agents and skills, then build it in-house and don't pay for it.",2
1qq98ku,2,"QA is and should be changing, it's been a drag on engineering teams for too long. Check out duku ai",1
1qq98ku,3,"Dbt test are baseline, if you have unreliable source systems testing against verified backups is the best way, at least for us.

We have seed files plus snapshots for major numbers and test for any changes against that. This flags a lot of things normal tests are not able
To catch and are critical for the company",1
1qq8g3e,1,Have you tried asking it to 'make no mistakes'?¬†,1
1qq69k3,1,"If you use Databricks, look at clean rooms",2
1qq69k3,2,"I'm not sure this quite fits what you're after but both BigQuery and Snowflake have this concept of external ""marts"" (I forget what their proper product name is for each) where you can configure certain external data sets for limited consumption by external users or even the public.",1
1qq69k3,3,"Who exactly are you finding these commonalities for?

I don't know what country you're in but it sounds like a GDPR violation waiting to happen.

I honestly wouldn't be going anywhere near project, unless there's some legal agreement between the two companies, it's not your issue",1
1qq5oso,1,"I would strongly advise a buy-not-build approach here - especially for DAM.

Consider the likes of Adobe, Assetbank, Bynder etc - as that will have AI embedded anyway, but has the right workflows for artwork and the users.",3
1qq5oso,2,"Do you need a data lake, why not just a database?",1
1qq5oso,3,"Echoing someone else in this thread, many DAM platforms are steadily integrating quite good AI features into their platforms to improve functions like searching , content management (sorting/tagging), and workflows which sounds similar to what you may be trying to achieve. 

Reinventing the wheel likely will take significantly more resources on your end than may be worth it. Some other DAM solutions that have implemented AI include Canto, Acquia, and Frontify.",1
1qq5oso,4,"Jezz, what world we are living in? Experienced people can't land a job while someone with zero clue what is unstructured data, taking architecture decisions.

You don't need a lake, put metadata into DB and link your files stored in an object storage.",1
1qq5oso,5,"A big factor for me was what processing engine would you be using. Spark? Polars? AWS Athena SQL queries? This narrows down your options. For example AWS Athena doesn't Integration with DeltaLake to well. You can read but you can't manage the tables, like alter, delete. We are using polars and this means that for management tasks we have to use delta-rs, which is a package I like. But we tried Iceberg first, and hated pyiceberg package so much we decided on DeltaLake. Spark works with everything but is a truck of an engine. If you would be only processing gigabytes or low terabytes daily it's probably overkill. Stuff like AWS glue and similar are quite expensive for what they are (IMO)",1
1qq5902,1,Its an absolute crap.,23
1qq5902,2,"Fabric is not production-ready. It may be ready in four years, but for now it has many bugs and missing features.",101
1qq5902,3,The answer you‚Äôll get here is ‚Äúno‚Äù.,42
1qq5902,4,Fabric literally the worst data platform solution out there,68
1qq5902,5,"no, not unless you‚Äôre contracting it out to a team of very cheap idiots to maintain, who have all the time in the world to burn on microsoft support and working around fabrics constant issues",31
1qq5902,6,I‚Äôm a fan of powerBi and some of features in fabric from a reporting standpoint. But I would not use it for data engineering. We are switching from sqlserver to databricks for our engineering workloads. I personally hate the fixed consumption model and like the pay as you go approach,9
1qq5902,7,"From a data engineering skillset perspective which parts do you think are not transferrable to other platforms?¬†


Fabric runs on Python, Pyspark, SQL pretty much the same as Databricks or snowflake.


Generally if I am looking to hire a data engineer I am not looking to hire a ""fabric data engineer"" I want to know you understand the fundamental concepts and techniques of getting business value out of data",8
1qq5902,8,Interesting to know if it is since I‚Äôm learning the stack,8
1qq5902,9,I tried it in the company I work for and its not worthy mostly cause is quite at an experimental stage and too pricey,5
1qq5902,10,"Fabric is MS's answer to Databricks, Snowflake. An attempt to bundle every tool they have into a new shiny platform that you can do everything. Unfortunately, as you might have seen or read on this group, it's not really that good yet. I think Fabric is ""recent"" and not widely adopted to see companies hiring roles for it.",9
1qq4zrw,1,"Can you please just stop with these advertorials for your product? They are misleading at best, maliciously incorrect at worst.",3
1qq4aw3,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qq4aw3,2,"Hi, a few things to consider.

\- Data Talks Zoom Camp uses GCP, whereas Data Engineering by Joe Reis uses AWS. Even though the cloud technology you use doesn't matter much to learn the concepts, it's still better to start with what you are interested in.  
\- Zoom Camp is semi-structured and felt all over the place with some GitHub content, YouTube videos, past Cohort links, etc. For someone like me, it's very easy to deviate and lose interest.  
\- Joe Reis's course is well structured with a lot of optional material and paired with the Fundamentals of Data Engineering as the course's textbook. The assignments follow a similar structure to other Andrew NG's courses. Not too tough to pass. But, still a lot to explore and learn.  
\- ZoomCamp is free but uses a sponsored orchestrator. The deeplearning course uses Airflow, which is considered standard. Of course, there is Dagster and a few other tools gaining traction. So, this is not too significant either.  
\- ZoomCamp is tweaked and updated every year. But the DeepLearning course stays the same. But since it's a very recent release, it still covers the most popular tools and concepts, such as Lakehouse architecture, dbt, text data processing for AIML workloads, etc.  
\- ZoomCamp has a strict deadline to get the certificate. You will still achieve it if you start now. I assume the score weightage is mostly on the capstone project. For the Deep Learning one, you can keep paying and extending as long as needed.

Overall, I tried to finish Zoomcamp multiple times and was distracted, but I am in the final module of the Deep Learning course.",6
1qq4aw3,3,"I like the DataTalks Zoomcamp a lot and am about to go through it again actually.  Alexey ~~Andrei~~ is an awesome instructor for his portions.  Some of the others are less so, but overall it's pretty solid.  

I'm half way through Joe Reis's Deeplearning and I'm torn on whether to finish it or not.  A lot of the videos are reiterating a lot of content from his book (Fundamentals of Data Engineering).  It goes over a lot of the soft skills that a good DE needs, but less so on the technical side.  

His class spends a lot of time within AWS and you do pick up some skills with it, so it's not too bad, but I don't feel like it was sticking with me after the class was done.

Personally, do a few of the DataTalks zoomcamps - it has you writing and doing more than the other courses.  And since the zoomcamps can occasionally miss some steps, it will force you to either interact with the other students or hone your research skills in order to make things work correctly.

Lastly, you can go back through the DataTalks github and look at previous cohorts - they change up their tech stack some.  One year used Airflow for their orchestration, the next used Dagster and they might be using another one now.  Most orchestration tools are similar, so getting the basics done and then being familiar with other tools is a good thing.",4
1qq4aw3,4,"> what exactly would I be missing if I start the DataTalks Zoomcamp today since the start date has long passed already

Pretty much assignment 1. I'm redoing it, because they finally started reshooting key videos. Previously they switched tool so many times you would be trying to figure out where you used tool X and it was impossible because they pivoted to Y 2 years ago.",3
1qq4aw3,5,"Restating what others said ‚Äî zoomcamp is self paced and always there. If you care about a certificate, I believe you can still get it with completion of the final project! It is my first time doing it and I‚Äôve liked it so far.",3
1qq4aw3,6,"U can catch up, zoomcamp only matters for the final project to get the cert",2
1qq4aw3,7,RemindMe! 1 day,1
1qq39sr,1,Let‚Äôs make the numbers as opaque as possible to cover the lack of Fabric adoption. Still believe it was a poor strategic decision to merge PBI with the rest of the suite..,37
1qq39sr,2,"Power Platform/fabric/powerbi are literally the worst part of my job.

I al going to have to do sole stuff with SharePoint soon, and I do not have high hopes for that either.",13
1qq39sr,3,"In big part, yes. Microsoft forced all existing Power BI premium capacity subscribers to move to Fabric by simply retiring Power BI premium capacity licence, which is more expensive, but you have to pay unless you want to lose the premium features or move to PPU licences which for larger companies is even more expensive",5
1qq39sr,4,"> Power BI subscriptions still rule :)

My humble opinion is PBI has had its day and fabric bundling may be the beginning of the end for it.

In a lot of my customers, they are looking elsewhere because they feel fabric is being shoved down their throat.",5
1qq39sr,5,"IIRC they did the same thing when Azure was new. The industry was all about AWS and Microsoft needed it to look like people were rapidly adopting Azure as well. So they moved unrelated, previously adopted products under the Azure umbrella and said ""see how fast people are adopting Azure!""  To their credit, they kept investing in Azure and it's a great product today. But they needed people to believe it wasn't a fly by night cloud effort that they'd pull the plug on in 6 months if it didn't get the adoption they wanted.",4
1qq39sr,6,Yes 100 percent,1
1qq39sr,7,Power Platform is not included in Fabric financials.,1
1qpv93e,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qpv93e,2,Why not just do it locally if it is just for a portfolio project? It's really just a config change to point it to another DB.,4
1qpv93e,3,"I just wrapped up a project using data from Kaggle. I built an ETL pipeline with AWS (Glue, Step Functions, RDS, EC2, S3, SNS for alerts etc) and provisioned the infrastructure using terraform. All in, all it cost me under 35 dollars CAD. 

You can do the same.",6
1qpv93e,4,"Best case Nasa free api 
Which provides various dataset via Api, it will be very usefull and help your friends to practice python scripting for change the datetime to call Api to get raw using scheduling and DAG, with his data they don't restricted with Basic ETL with SQL or Python they can utilise and learn new tools such as Airflow and additionally if they can claim AWS or GCP free tier to practice on cloud environment so that they get understand how Clouds data system mapping and ingestion to cloud then stream it for processing and perform transfermation and then load again as Glod layer then if want they even stream the gold data into a Bi dashboard using quick sight or PowerBi they can learn this like how they going to be work on real rathar then localhost ports.",2
1qpurc0,1,"Actually been discussing this at work. Are you on snowflake? We use streamlit in snowflake.

Some are arguing we use just the native git integration only which I think could wind up exploding. I'm trying to argue in favor of snowflake cli which would also open the door to automate some of our manual sql scripts we have that doesn't fit within dbt.

In any case, have you asked Claude to give any suggestions with pros/cons? Usually gives a decent baseline of avenues to explore.",12
1qpurc0,2,Use an actual web framework to begin with - skip the streamlit stage if vibe coding imo,6
1qpurc0,3,"I don‚Äôt know why streamlit is the go to choice for so many people, they should be using fast API to serve HTML/JS/CSS. The LLM knowledge is much deeper.",14
1qpurc0,4,Do people use streamlit for production????,3
1qpurc0,5,"When that one breaks, just build a new one!",2
1qpurc0,6,Vibecode a React frontend with D3.js and Python backends instead. Or just use Retool or a normal BI tool that supports dynamic stuff. Streamlit is a fun toy. Do not put anything even remotely serious in there. It's a total mess.,2
1qpurc0,7,"We have built a full fledged analytics app with streamlit and cortex agents. It's sort of like chatgpt but with a very strict workflow and for healthcare data.

I feel Streamlit is great for POCs or pilot projects, but it feels incredibly unstable and unscalable.",1
1qpurc0,8,"streamlit is fun...initially. Then it is just terrible mess for anything more than just a few charts. CRUD becomes nightmare due to Streamlit's refresh all page model. So 1000s of streamlit apps does sound like a nightmare. 

But then I was chatting to a friend who was very proud of building a web app on a weekend with AI. When I asked him what tech is used, he had no idea. I guess this is our new reality",1
1qprsx8,1,"I added the ability for the remote agent, or the SaaS worker, to run dbt core. [https://saddledata.com/blog/announcing-remote-agent/](https://saddledata.com/blog/announcing-remote-agent/)",1
1qprrd8,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qprrd8,2,"Only you can say what the money means to you.


For what it's worth I've done both and you'd have to pay me 50% more at least to go back to stuff aug/consulting. Maybe more.


I'd take the better prospects and stable home every time.",50
1qprrd8,3,"I get that you want to leave, but all my friends that left consulting did so at a significant salary increase. I recommend you ask some of your ex-colleagues. I know the job market is rough, but a 12% pay cut is significant...",20
1qprrd8,4,Negotiate for them to pay for a Databricks certification in your offer,6
1qprrd8,5,"Look into what this ‚Äúnew role‚Äù requires. Your current role might involve crunch/more hours than expected. This one may have less real-world hours that you work, changing the equation entirely. Or the opposite can be true- you can be slung into a role that demands 60-80hr workweeks. 

What are the ACTUAL job responsibilities and how do you know you‚Äôre not gonna be stuck doing the job of 3 people with one salary?",4
1qprrd8,6,"Is it truly below market rate or just less than you now make.¬†


Your sanity is worth wondering too.


Ifv you take it, expect your paid market rate and that's what you'll make. Hoping for more in a few years may be foolish depending on your salary and market rates.¬†


We can't tell you what you're worth without knowing your location. Up to you, but sharing the offered salary would also be helpful for saying if your paid competitively.",3
1qprrd8,7,"life is short, enjoy it, 12% pay diff won't make a diff, but what you do 40+h/wk will",3
1qprrd8,8,"I assume that since you are entertaining the idea of a 12.5% cut, you can afford it. Doesn't mean you have to like it, but it is acceptable.

Personally, I see the stability as worth it. Likely with some ladder to climb too. Sounds like you are trying to convince yourself. 

Take the job, learn something new.

It will make you more valuable in consulting, too.",3
1qprrd8,9,"I‚Äôd negotiate for more than what you‚Äôre making if possible. If not possible, I would still take it but with a mindset of learning everything in the shortest amount of time then find better opportunities, 12% for practical and high demand skill is not bad imo.",2
1qprrd8,10,Earn or learn. Preferably both. At least one.,2
1qpom8i,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qpom8i,2,"I think you miss the point of FGA. If you grant someone access to level2, it inherits to all lower level resources. Or not, of course, depending on your authorization model. 

It sounds to me that FGA is actually perfect for your use case, because it allows for very advanced authorization models. Yes, you would have to sync *some* data (as tuples) but I don‚Äôt see why you would have to add or remove thousands. To me, that seems like your authorization model is not correct. I am eying OpenFGA by the way, maybe you can try that out (Auth0 FGA is based on that), so you can experiment a bit. I found the documentation extremely useful, it really helped me opening my mind to modelling a proper authorization setup that scales, without millions of tuples to maintain.",3
1qpom8i,3,"> It would require strict synchronization and cleanup

See [https://authzed.com/blog/the-dual-write-problem](https://authzed.com/blog/the-dual-write-problem)

> integrating a third-party authorization service appears to introduce significant complexity

It would introduce the complexity of running another service.

> and may negatively impact performance in my case.
> Additionally, retrieving large allow-lists for filtering and search operations may be impractical or become a performance bottleneck.

The people building authz services have realized that authz needs to be performant, so you shouldn't have issues in that regard :)",1
1qplapl,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qplapl,2,Databricks Free,23
1qplapl,3,A cointainer can help on that. Free and local.,17
1qplapl,4,"this repo has been pretty good for getting some challenge problems ‚Äî helped me practice! 

https://zillacode.com/home

https://github.com/davidzajac1/zillacode",5
1qplapl,5,"Local spark session, jupyter notebook, databricks free edition, azure/aws, online pyspark editors, hackerrank or leercode maybe or something like that, claude/gemini, etc.",2
1qplapl,6,"I would suggest a self managed local setup preferably on Linux (WSL2 if you are on Windows). The installation is a little tricky because of dependencies and version conflicts. But trust me there is no better way to learn Spark. When you learn local installation on your own, it's easy to switch to managed services like Dataproc, EMR and Databricks. Practice Pyspark and if possible Scala Spark (for native performance benefits)",2
1qplapl,7,sparkplayground,1
1qplapl,8,You can try Spark Playground‚Äôs online compiler,1
1qplapl,9,stratascratch,1
1qplapl,10,Google colab,1
1qpkei4,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qpkei4,2,"More people use sql than pandas.

To be fair, i never use pandas and suck with it. Have been a DE for 10 years.


Think part of it is how DE came from BI (development).¬†",5
1qpkei4,3,"Lets say you have to get that data every 0.1 seconds 24/7. Theres minor variation in each time you extract it. Thats when you need to put it in a database, and after a while, use SQL to get anything meaningful out of it.",1
1qpkei4,4,"Well you can load your data into Power BI via SQL queries, e.g. with the ODBC connector. On the DA side it‚Äôs more like you are ‚Äúpulling‚Äù the data into your reports from a database. Traditionally these are separate jobs. There are many use cases for keeping data in a DB, not just for dashboard stuff. Also for a reasonable amount of data, you can easily run into limits with what can easily fit into memory with Python. Your question really condenses to ‚Äúwhat‚Äôs the point of databases, anyway?‚Äù Rather than anything to do with structured queries.",1
1qpkei4,5,Integrate a dbt project or use duckDB‚Äôs python api to replace any data processing done via pandas.,1
1qpkei4,6,"This is a de vs da context thing typically. You typically are not going to use both in the way that you currently are trying to. For da ad hoc querying from api and then analyzing data in notebook, Pandas (or better these days to future proof yourself, Polars) is more likely to be a go to. It is a discreet analysis engine. It is great for flattening api json to tables quickly, it is modular so you can do step wise analysis, it is very quick to export from.

But for proper oltp or olap based de pipeline dev with traceability and db centric design, sql will be your bread and butter much of the time. 

I have moved from da to de and have been using more and more duckdb as my sql engine lately. It is olap so it doesn't have schema registration reqs as ready to enforce, but it can lazy process parquet from s3 VERY efficiently without significant memory required. The thing about pandas is that all ops are in memory, which is fine if your dataset is less than your available system RAM, but when you work with larger data or cloud based flows (and associated costs) limiting memory dependency is an important optimization. Polars also can help with this because it has lazy loading ops.

At this point for data collection i pull from api and collect as polars then pass to duck to write raw data as partitioned parquet. If I can do transformations in duck I do it all in duck directly querying the parquet. If there is something messy that I have difficulty doing in duck, I will do what I can with duck and then take it back to polars for lazy in memory vector ops. If I need to quickly inspect data I use pandas converted tables because I am more well versed on pandas. I also use pandas where iteration line by line is necessary (like api lookup). Ultimately I will typically then write processed flattened consumable data as partitioned parquet. Then depending on case I use duck to query for analysis myself, or I serve flattened data as csv somewhere that it can be consumed by end usage app like power bi.",1
1qpkei4,7,"Bruh you just take your excel convert to csv, then upload csv to like PostgreSQL you can download visual studio to use it it‚Äôs not that hard, duck db might be easier  too",0
1qpii5e,1,This sounds like an ad lol,13
1qpii5e,2,"Did you check run times? For same job , I could see some of the jobs taking less than half of the time it previously used to take, no code changes, no optimization,  just upgrading glue from 2.0 to 4.0, obviously this means there is some backend optimization. Have not delved into details since the priority is to upgrade glue jobs, but I think some spark optimization is at play, also with increase in glue version also the spark version gets upgraded. Will check this whenever I can, but surely some optimization is at play.",2
1qpii5e,3,"Interesing. AWS is less likely to invest significant new updates (features, major changes) into the Glue 4.0 image now that Glue 5.x has been out and it makes sense to make improvements on newer 5.x versions.

  
One guess I can have is that there were improvements in Glue‚Äôs cluster management which would be independent of the image Glue is using, though that alone wouldn‚Äôt necessarily explain faster execution time (though it would mprove overall runtime). Inspecting Spark UI metrics before/after to see if anything changed under the hood (shuffle, IO, executor behavior, etc.) would probly the next step to take.",2
1qpe75w,1,"Sounds like a full stack software engineering job not DE, even if your not working on the front end yourself you will still need to learn some to connect your apis, unless that foundry does it for you, never heard of it.

Learning many skills at once you will progress slowly,and I would stay away from low code tools at first since you won't learn what's going on beneath them, it could be hard to find work at another employer who does use the same niche tools.

Only a year of experience so maybe I'm completely wrong.",2
1qpe75w,2,"Is this intership still available?  
That's my stack.  
P.S. I would take this offer!",1
1qpe75w,3,"Imho, that's a perfect opportunity to get some experience in coding.",1
1qpe75w,4,If it is a paid internship take it.,1
1qpbkfa,1,"Good utility. Do you plan to maintain it actively? I have faced issues with the utilities I built like this one, end up burdening me with the maintenance tasks",3
1qp9sys,1,Do the data contracts follow the Open Data Contract Standard? I took a quick look but can't find it in the documentation.,3
1qp8tni,1,I don't think it's part of data engineering. But I do think it's widely used. You should get the cert.,28
1qp8tni,2,Knowledge of Kubernetes could absolutely be a factor in getting a certain job or not.,15
1qp8tni,3,"It's Data Engineer adjacent.

Not a core part of the job, but definitely nice to have.

Though it's not something that will ever be required for a junior DE.",10
1qp8tni,4,More for the data platform role.,2
1qp8tni,5,"It‚Äôs a very useful tool to have general knowledge of in your kit.

Our Airflow deployment runs on Kubernetes in multiple clouds. All¬†tasks run on Kubernetes. We aren‚Äôt the primary owners and don‚Äôt interact with it directly (most of the time), but it‚Äôs helpful to have a general understanding of it.

Everything else equal, I‚Äôd absolutely favor hiring someone familiar with K8s over someone who isn‚Äôt, but it wouldn‚Äôt be a dealbreaker if they were the stronger candidate in other areas.",3
1qp8tni,6,"Knowing how to use kubernetes is useful. Knowing how to manage a kubernetes platform is generally out of scope. 

For example, in my current role we use the Kubeflow platform for deploying models and running pipelines so knowing how to containerize code, set pods/cpu/memory/etc and interact with a cluster using basic kubectl commands is a requirement.",3
1qp8tni,7,"Depending on the tech stack of the company, sadly yes.",1
1qp8tni,8,Not relevant in larger properly organised teams,1
1qp8tni,9,"No, but I would rather get someone that knows K8S instead of someone who does not.

Beacause:  
1) that person took the time to learn something that broadens their knowledge. Which is a good indicator of their passion or inclinations.  
2) you never know what happens when it is time to bring your S3, Spark, Iceberg/delta, and whatever else on prem or off commercial data platforms.  
3) you begin to think cloud natively instead of cloud only",0
1qp7ryc,1,"Refer to the [quarterly salary discussion](https://www.reddit.com/r/dataengineering/comments/1pbi5i7/quarterly_salary_discussion_dec_2025/)

Locking this up.",4
1qp6j55,1,"Best choice ‚Üí Math

Math builds strong logical thinking, problem-solving, and algorithmic skills, which directly help with data pipelines, optimization, SQL logic, and system design. It also supports learning distributed systems, performance tuning, and scalable architectures later.

Statistics is very useful but more aligned with Data Science / Analytics / ML, not core Data Engineering. You‚Äôll use some stats, but not deeply in most DE roles.

Economics is great for business understanding, but it won‚Äôt give you the technical depth companies expect from a Data Engineer.",7
1qp6j55,2,"I was a data science major who pivoted into data engineering at my first job 6 years ago.  I would echo the other comment here that says study at much CS as you can.  I focused on statistics and have not used it, whereas I‚Äôve now had to teach myself much of what I would have learned in CS.",4
1qp6j55,3,I know data science is a bigger major now but I feel like that boxes you in. Get a CS degree with a data science minor. You'll open up more avenues in my opinion.,2
1qp5lva,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qp5lva,2,"I'm very interested in knowing what your use case is.

What tortured mind-virus inspired you to create a process so devious that it tortures the spirit of the very hardware it runs on...",2
1qp48tg,1,"ah yes, DuckDB, well known for its ability to handle concurrency. Might want to edit that out of your post.

Last I heard, ClickHouse perf on joins wasn't that great. Can you share a little more about your use case?",3
1qp2xul,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qp2xul,2,Following,2
1qp2xul,3,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qp2xul,4,"It‚Äôs hard to find resources that are not just speculation and hype. The blogosphere is full of slop and self promotion. 

Here‚Äôs a few links I‚Äôve come across from people that are reflecting after actually building ai tools, might give you a sense of where to focus:

https://utkarshkanwat.com/writing/betting-against-agents (ignore the clickbaity title)

https://www.reddit.com/r/ExperiencedDevs/s/KRqDudaVVC",2
1qp2xul,5,Following,1
1qp2xul,6,Following,1
1qp2xul,7,Following,1
1qp2xul,8,Deeplearning.ai,1
1qp2xul,9,Following,1
1qp2xul,10,"Reading star schema book

Zoom camp data engineering 

AWS solution architect udemy course

And building a pet project (focused on data viz)",1
1qp1uqw,1,"Your post looks like it's related to Data Engineering in India.  You might find posting in [r/dataengineersindia](https://www.reddit.com/r/dataengineersindia/) more helpful to your situation.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qp1uqw,2,"I‚Äôm 20 years in tech, safe employment, in leadership, really interesting work and I will tell you that your feelings are valid and being felt by a lot of people right now. The rules don‚Äôt apply any more. 

Here‚Äôs what I would do (as a leader.. this is what I‚Äôm looking for). I don‚Äôt know if its right - only time will tell:-

1 - get really good a defining and documenting for AI and business. How do I get an Agent to accelerate my work via context and not code. How do I show others the value I‚Äôm generating via acceleration of skills and effect using LLMs. Outcomes are all that count - 99% success but no value to business or customers doesn‚Äôt count as a win. 
2 - the technology moats will come down. Oracle tech stack isn‚Äôt going to be a thing in 2 years. You‚Äôre a plumber of data and the stack/implementations matter less as the coast of replacing them is coming down at a huge rate. Tech debt suddenly has way lower interests rates. 
3 - get good at big data again. Small stuff is gonna be usurped - that‚Äôs light work for a LLM. Big data is going to be hard because of cost and time. 

Start with YouTube - it‚Äôs a mess of influencers and hype men but there is some gold in there. 

Microsoft have a bunch of free GitHub learning on AI, ML, Big data on their repos. https://github.com/microsoft/AI-For-Beginners is one. But they gave many. 

One thing to remember - the AI coding hype cycle has been around for 2 or so years of any value. You are still smarter than AI for sure. It can just type and think faster than you. So figure out how to make it your puppet, you will win. 

You WILL have an aha moment after 2-3 months and it will kick in. You will feel like you have a new superpower and feel like you have new energy. Every engineer I work with internally and externally has this moment if they break through the initial inertia - but it‚Äôs exhausting for those who have been grinding for years. 

You got this!",9
1qp1uqw,3,"Databricks is just a platform. Important Core Skills are understanding of Lake based data formats and how efficiently lakes can be used to store and retrieve data (Partitions etc)

I just started working with Databricks and other than platform specific nuances, it‚Äôs like any-other big data platform.

If you don‚Äôt have experience with Python, databricks gives you SQL interface to do all of the engineering stuff. I‚Äôll recommend going through Databricks related free stuff available online.

PS: ~12 YOE",10
1qp1uqw,4,"Same here for me with 13 years of experience, currently working on Snowflake but I feel there is a lot of competition in it, snowflake is easy to learn and cheaper resources are available, not even getting a single call due to 90 days notice period. Feeling stuck in the job 
Not sure what I am supposed to change",5
1qp1uqw,5,At least you're on a traditional stack still used. You'll find something. Everyone talks streaming but a good design still has a data warehouse. Just everything doesn't need to go into facts and dimensions but most of it should,2
1qp1uqw,6,"I made the conversion a few years ago, coming from stacks using Talend or Informatica on top of Oracle or MSSQL to using stacks that leverage Python and Snowflake/Databricks/DuckDB. I much prefer the latter to the former after getting used to how things are different.

You're still doing the same things in the new stacks, such as moving data from a file, API, or a database into a data warehouse (or lake) for further transformation and analysis. The fundamentals are still the same, so once you are able to figure out what the parallels are between the old and new tools you'll be in good shape.

Learning Python is highly recommended, with maybe Go or TypeScript as a backup language. Learning how to use Pandas and later PySpark will help too. Data pipelines are now typically coded by hand in a programming language, or else configured in a tool like DLT or Bruin (ingestion tools) along with other tools like DBT for transformation. Definitely not impossible to transition, but be aware that it's a very different world than ODI, DataSphere, Cognos, Talend, or Informatica.",2
1qp1uqw,7,Thanks for the suggestion. Have to start somewhere for relevance. This is one of those things,1
1qp0f0i,1,The issue is compounded by the fact that pay has remained relatively flat while the skills requirements have increased.,82
1qp0f0i,2,"Probably a minority, but I like it, since it produces variable and interesting challenges and you learn a lot :)",94
1qp0f0i,3,"Head of Data should be replaced with CTO. 

As a Head of Data myself, I would hope data leadership would know better. However, Engineering leadership does not or does not care. 

My current role has a similar situation. We talk about being data driven / data quality, etc. but when push comes to shove, engineering pushes out üí©code and then we‚Äôre told ‚Äúwell, you can just update your pipeline and get it from there‚Äù",11
1qp0f0i,4,"I've run data engineering organizations for a few decades. It's all how you look at it. Going wide is the singular reason data engineers are usually the least impacted by year over year layoffs. It also ensures access to different technologies and problems, which staves off monotony and boredom in role.",21
1qp0f0i,5,"This is why data engineering is not a junior role. Personally I like it. If I wanted to do the same thing every day, I would have become an accountant.


That being said, it's unsustainable to be an expert in everything. You have to rely on others. I don't know much about infrastructure and security, only the basics, which is enough to ask the IT team and the vendors the right questions to get the project built.¬†",8
1qp0f0i,6,"I was assigned a security certificate ticket the other day, ah wtf people!? Oh and password reset for service accounts and stuff. I just send it back but I don't have permission to do so and it's a mess. Come on people, yes it's used for a reporting tool or whatever but that's infrastructure",8
1qp0f0i,7,"This is not new. The smaller the team, the more you have to be a generalist. From personal experience, I was doing all those things back in 2010, from setting up and securing Hadoop clusters to gathering requirements and building reports, as part of a team of five.

As data teams grew into the dozens, people became specialists. They could focus on just doing platform work, or just building data products with a predefined set of tools. Now, we're seeing leaner budgets and smaller teams, with some members moved to AI projects, others laid off, and the remaining staff asked to expand their scope and keep all the plates spinning.

It is a lot, yes. The feeling of being just competent enough to stay afloat can be exhausting after a while. The silver lining is that you are not a cog in a large data organization, but an individual contributor with a wide set of hard-earned skills and institutional knowledge that will be very hard to replace.",6
1qp0f0i,8,"And hired as senior python developer.
We need specific unions.",11
1qp0f0i,9,"Anecdotally this holds true across data professions. You can model that when the data task is described as magic by supervisors or management, because it has ventured beyond their technical skill, you begin seeing the task arrive into the catch all of a data professional nearby. Hey. You know data. Do thing with this.",2
1qp0f0i,10,So true and they wonder why DEs tend to burn out.,2
1qozv1g,1,"I'm gonna repurpose an old meme.

Streaming data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it.",93
1qozv1g,2,"Yes, streaming is high cost for low reward in 90% of cases. Especially from the bi/ops side of de, most jobs are ad hoc and periodic by need and design. Mlops and product facing obviously has a higher need for it, but still, most jobs are not going to be streaming in the day to day.",69
1qozv1g,3,"I think it‚Äôs very niche. 

Data freshness is only as meaningful as how quickly people will react to it. 

If somebody‚Äôs not going to be taking action within seconds of seeing something, probably not worth it. 

Most places in my experience are fine with batch loads and end up getting much better performance etc.

Not saying there aren‚Äôt use cases but it‚Äôs not as common as places would like to think.",25
1qozv1g,4,"Depends on the company and department.

If you're in a technology company full of software engineers streaming is *very* common.  

If you're in a non-technology company, in a team of data engineers that mostly use dbt it's generally off the table.  Not a good fit.

BTW, these days I find more people who are building something in between:  micro-batches of say 5-15 minutes, with event-driven pipelines rather than streams or daily batches.",21
1qozv1g,5,"Hardly any of my job is real time, streaming or automated anything. We‚Äôve got this shitty ass program called ‚ÄúAutomate‚Äù that I think is only used at my company. It‚Äôs wack. We also use SSIS lol.",9
1qozv1g,6,"This just isn't true. It's ignorant.


40% of your time will go to unfucking checkpoints and weird latency issues from a buggy as shit streaming tool. Get hyped.",6
1qozv1g,7,"Most of the job is pulling data in from other databases and processing it, yep.

Very few places *need* streaming data ingestion, and even fewer of the ones who actually *do* use it do so well.",8
1qozv1g,8,"Depends on the place I guess.  
For me and my teams 95% of the work is about real time streaming high volumes of data, yes Kafka is a central part of this.  
We don't do one off ingestion of random files.  
We connect to some databases to get enrichment data, but that will be like a scheduled import (hourly/daily/weekly depending on the kind of data) that is automatically put in to something more performant like redis to then enrich the streaming data with things like user information or geoip data.",3
1qozv1g,9,Yes and the data warehouse didn't go away! If you got rid of it you're heading into a mess.,2
1qozv1g,10,"I prefer to build streaming first.  It's not about the speed for me.  

First, it's about control and localizing failure.  To use your examples; If I break up the CSV putting each row in it's own message, process each row one at a time, and one fails, that row fails, not the whole CSV.  That way I know exactly where the problem is and my users have access to the data that is not wrong while the issue with the failures gets handled.  Slapping a CDC implementation gives me a stream of all the changes to the relational database not just the end state when the incremental job ran.  If the load drastically increases the streaming system gives me a buffer without having to scale my workers unless the load sustains past my data latency requirements.  I've seen incremental jobs like that lock up a worker using all the memory because it was suddenly getting back much more from querying the relational database than it had been designed to handle.  It couldn't process anything until those workers got scaled up.

Second, I think you pointed out something with the fact that you're doing streaming on one system and batch on another.  You now effectively have two different types of systems you're maintaining.  If you go Kappa architecture as opposed to Lambda architecture you only have one type of system and that simplifies your operational overhead.  Even if streaming is harder to operate, you end up ahead.  I've never personally found a streaming system more operationally difficult but people do.  If you're only going to have one kind of system, I think it's easier to put a batch of work on a streaming system than to try and micro-batch your way to approximating a streaming system.

Lastly, and this is more personal, I like to think of data pipelines in a functional programming or even unix shell scripting model.  Mostly stateless operations with pipes in between.  Streaming tooling seems to map to this mental model better for me.",2
1qoz0rq,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qoz0rq,2,"I took it years ago and it was probably the number one thing that helped me gain confidence in data engineering, and eventually land my first DE role.

So yes I‚Äôd recommend. Although you might be too late to join the current live cohort. You‚Äôll need to check.",1
1qoz0rq,3,"I'm not familiar with that Bootcamp. But I conducted a very large amount of interviews in the Data and DevOps space for the contracting division of a large tech company. Bootcamp grads often felt very 'same-y' in their level of experience and superficialness of their answers to questions; I would primarily suggest to spend time breaking things and troubleshooting them. As most of the bootcampers seemed to only be familiar with the 'happy' path. 

If you need a guided education and someone to drive you to complete things by dates, maybe they're good. But they're often not cheap and if you can self direct the amount of free tutorials and project guides online are more than sufficient and often essentially the same as the bootcamps would cover. I personally wouldn't recommend it unless they have job placement services that are well reviewed by past candidates.",1
1qoxz30,1,"I have not done this specifically but have A LOT of experience with the confluence api because my company for reasons beyond me decided to unsubscribe from our content manager and use confluence for our knowledge base drafting area with no plans for how that would move to our customer facing knowledge portal. Queue me getting roped in.

Anyway, the api is pretty simple to pull things from, you can collect based on field changes like status using filtering if you want an update only when changed specification. The content comes out as 'xhtml' which is html with some bespoke xml thrown in for their macros. I have found it pretty benign to reformat for destination requirements using beautiful soup.",2
1qoxz30,2,I believe there is a way to render markdown into confluence pages using quarto,1
1qoxz30,3,"Everything as markdown only. Then pipelines that uses md2cf to auth and upload. Theres some confluence docs that expalins how it renders md.
There are some additional files you can add to tell cf how to orgenize the pages and subpages if I recall . 

Remember to disable manual editing permissions so its only via commit and pipeline.

This will also allow you to apply some tests for markown linting, add spelling ang grammar checks, translation if needed and more to embbed quality into the documentation.

Theres more to do with picture attachments, verifying links, deploying a 'public' /'private' version per excepted audience and more but simple steps first.",1
1qovfio,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qovfio,2,">are there any specific skills i should learn to stay relevant as data engineer?

Unless you intend on working directly with ML, then higher level concepts are much more important rather than directly being involved with optimising and building ML models.

Personally, I took a 20 USD Udemy course on data science and ML 5 years ago and it's still relevant today.  One addition now is understanding how LLMs work under the hood.  The concept of generating and assessing good quality training datasets hasn't really changed that much although with the advent of LLMs, the scales are a lot bigger.",2
1qovfio,3,"As the industry shifts toward agentic workflows the role of Data Engineering is expanding beyond just ""moving data"". I'm starting to focus on AI-augmented data engineering: modern IDE workflow with AI-generated tests, live connectivity between LLM tools and databases via MCP, autonomous data agents for log analysis and debugging and prompt engineering techniques to generate and reason about complex ETL pipelines directly from natural language",1
1qotieb,1,"Look up Slowly Changing Dimensions. The answer is, it depends on what your business needs are. If you need to track historical data, this could just be a SCD2 process that appends a new record with the new status of the applicant, marks it ‚Äúactive‚Äù with a valid_from date of the new record and a valid_to date of null or ‚Äò12/31/9999‚Äô, and marks the old records as ‚Äúinactive‚Äù and marks the valid_to date as whatever the valid_from date is of the new record. 

That‚Äôs a bit simplified of an explanation but this is a common historical data tracking pattern.

Edit: the lookup key to check whether a record needs upserting is the userID in this case.

Edit 2: grammar and spelling üòÖ",11
1qotieb,2,If it's just the status create a candidate_status table and just insert a new row as status changes with a timestamp,2
1qot5ku,1,I know for sure I will not ask LLM for advice. All these systems are being paid to lie day and night.,7
1qot5ku,2,I search Reddit for opinions üòÇ,5
1qot5ku,3,"Do both do what I need them to do at the scale I need?
Which is easiest to use or has the best documentation.?

If the answer isn't clear, find a coin and flip it.  Honestly,  if it turns out to be the wrong choice it'll be far enough in the future that you won't be blamed.",1
1qot5ku,4,"1. What do you need the tool to do ? 

2. Try it ? I was hesitant to try databricks but once I got it setup I can see how much time I wasted. You‚Äôll never learn the tools unless you actually use them",1
1qot5ku,5,"You take a use case that covers the majority of things you need to validate and then build it multiple times in the competing tools. 

Then you determine which ones are capable and of those which ones mesh the best within your environment: team skillset, existing infrastructure, integrations, etc‚Ä¶

Lastly you determine cost. This could be through negotiating with the vendors or pricing out the infrastructure for self hosted platforms.


I don‚Äôt think there is a product to be built to solve it. Even if you build it, it‚Äôs the trust that will be hard to gain. There‚Äôs already websites like G2 or research companies like Gartner and IDC that do this type of thing.",0
1qos08a,1,"If you want to simplify it, you can switch to LocalExecutor using env variables in Docker compose file and get rid of separate Worker, Redis and probably Flower

Some of the projects I‚Äôve worked on successfully did their job using LocalExecutor running up to 100-200 DAGs per instance",1
1qoqc3q,1,"> As you probably know, we haven‚Äôt seen this same capability for data

Um‚Ä¶ I don‚Äôt know that because it‚Äôs definitely not true. A quick google search of ‚Äúgit for data‚Äù comes up with multiple tools that do this sort of thing. One example (which I don‚Äôt know anything about since it was just a quick google search) is lakefs which appears to already be partnered with AWS and Databricks.

I‚Äôm certainly not saying that existing tools are exactly the same as yours, but the claim that these tools don‚Äôt exist at all really takes away from your credibility. In fact, I think a lot of the claims you make on the site also take away from your credibility.

> 40% of time lost to firefighting. Pipeline changes take days, not hours.

How did you measure this? And what kind of changes are you talking about? I have changes all the time that take minutes and some that take weeks.

> dbt, Airflow, observability‚Äîhumans manually stitch workflows across 10+ tools.

What are the other 8? You only named 2. And why are you assuming that data teams are using all of these tools at once?

> One bad query corrupts everything. Backfill campaigns cost $50K‚Äì200K per incident.

HUH??? Where are you getting these numbers? These are just totally made up. If something this catastrophic happened, why not just restore a backup?

> No unified lineage. No real versions. No way for AI to experiment safely.

Why would I want to ‚Äúexperiment‚Äù on production data? If I‚Äôm experimenting, it‚Äôll be in a dev environment, not directly on production.

I love the idea of rolling back data and automatic lineage, but it‚Äôs certainly not a novel concept. Delta  Lake for example already does both of those things quite well.

To be clear, I‚Äôm not criticizing your idea or the tool, it actually looks pretty neat - but the marketing around it really needs work. You‚Äôre marketing to engineers, who are generally pretty smart people. Be intellectually honest with your marketing.",9
1qoqc3q,2,Dvc is git for data,4
1qoqc3q,3,"Are you related to https://www.thenile.dev/? Otherwise, did you do no research before choosing a name?",3
1qoqc3q,4,Project Nessie is like git for data,2
1qoqc3q,5,"Can someone ELI5 me what's git for data?


I tried Google and chatgpt but it sounds quite vague, anything from storing data contracts to data storage pointer...",2
1qoqc3q,6,"I don‚Äôt get the product idea. No existing problem you can not solve with git, backups and slowly changing dimensions etc. 

The features sound generic and promising as all start ups helping with data problems. 

My favourite is the AI code generator, the use cases are so common that you safe one time 5 minutes for the price of missing validation. Afterwards a dashboard is the place to be for it.",1
1qoqc3q,7,lol https://horizonepoch.com,1
1qoqc3q,8,doesn't snowflake have point in time recovery?,1
1qoqc3q,9,Your website is very close to having an unfortunate anagram.,1
1qon39x,1,"This is as stupid as it gets. Like if you have even a shred of understanding you wouldn‚Äôt even try to make a comparison between BQ and Duckdb.

BQ to Athena, fair comparison even if it may not be perfectly apple to apple, BQ to Duckdb, hell no.",10
1qon39x,2,DuckDB ftw,2
1qon39x,3,"Agreed with other comments that this comparison doesn't make much sense, but just to go into a little more detail as to why...

BigQuery and Athena are both serverless cloud offerings. BigQuery abstracts away storage *and* compute. Athena abstracts away only compute, as you need to connect it to your object storage, but fundamentally, it's at least a little similar as an offering. You're not managing infrastructure with either of them, and they'll scale up parallelization to whatever level is necessary to ensure you get query results back in a reasonable amount of time. Because they're both built on serverless billing models, costs are very high on a query-by-query basis, but this can still be competitive in a real-world scenario if you have relatively low daily utilization.

Compare that to DuckDB, where you're using dedicated hardware with an embedded, columnar database. It abstracts away nothing - you're providing the hardware for storage and the hardware for compute. This is radically different. It works well at small scales, and because you're managing everything, it's great for heavy utilization and is extremely cost-effective on a query-by-query basis. It's also not distributed at all. Once you scale up past what a single machine can handle, you have graduated from DuckDB and need to move to something that can distribute the workload and handle scale.

Fundamentally, they're different tools solving different problems. So when you run a benchmark with 20 GB of data, you're using a very small scale of data that DuckDB is built for and which BigQuery/Athena are not. Because there's no downtime in benchmarking, you're also simulating what is effectively 100% utilization, the least favorable scenario for a serverless billing model. Of course DuckDB is going to end up looking good.

The way to meaningfully compare them on cost, at least, if you were for some reason trying to decide between these different options, would be to look at a simulated full month of usage. The hardware you're running DuckDB on costs \~$1500 for a month on EC2, or maybe \~$7500 in up front hardware costs if you have it on-site. You'd need to run \~1700 queries a day on BigQuery to get a similar monthly bill, or \~13500 of the narrow queries on Athena. You could certainly cut your hardware costs for DuckDB by a lot for this workload, but also, are you running 13000 queries a day?",4
1qon39x,4,adblog and you have no idea what their powers are,2
1qon39x,5,Comparing Athena and BigQuery to DuckDB is like comparing a sedan and semi truck for 0-60 times.,1
1qon39x,6,"Duckdb looks nice until you need to process hundreds of gbs of data. I tried to make our pipelines with DuckDB (\~400GB of parquet) and although it worked, the experience of integrating it with most tools was not that great compared to other solutions.",1
1qon39x,7,"Nice write-up. We‚Äôve just added DuckDB to Exasol‚Äôs BenchKit as well, so it might be interesting to compare approaches.
Repo is here if useful: [https://github.com/exasol/benchkit](https://github.com/exasol/benchkit)",-1
1qolskj,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qolskj,2,Take a few weeks to do the dbt training course and take the certification. If you have all the experience just take the time dude.,3
1qolskj,3,"I work for Astronomer, so am biased there, but we offer a couple of Airflow certifications (fundamentals, and DAG authoring). I have heard anecdotal positive feedback in the Airflow community that they can be helpful resume boosters. The prep courses are all free. [https://academy.astronomer.io/page/astronomer-certification](https://academy.astronomer.io/page/astronomer-certification)",3
1qolp4i,1,"It allows connecting to data in other clouds and analyzing it without movement. So you can for example have your delta tables on AWS S3, make shortcuts in your Fabric lakehouse and query them directly without ingestion.",12
1qolp4i,2,Not to mention its not remotely ‚Äúone lake‚Äù at all. A separate ADLS bucket gets deployed with workspace in the region of the attached capacity. You can check the ABFSS url to see.,3
1qolp4i,3,Big query Omni has entered the chat.,2
1qoiat1,1,"The competition is so high that they expect you to be the entire data department. Big Data, Lakehouse, DevOps, Semantic data model, Reports, mlOps... AND be the leetcode keyboard monkey champion",252
1qoiat1,2,"I guess I‚Äôm ahead of the market, been doing 2026 since 2018 ü•≤",51
1qoiat1,3,"Yes. Check LinkedIn. They ask for python developer then the job description says: docker, Kubernetes, ETL... It's just a way to downgrade the salary.
They end up not hiring or hiring the wrong person. Bullshit companies anyway.",136
1qoiat1,4,I do everything in excel but create decks for show and tells telling everyone about other tools. Life is easier this way.,26
1qoiat1,5,"Its basically the only way i keep my job lol

I do some backend, garbage front-end, data platform/architecting/engineering and some devops",20
1qoiat1,6,Yep. Looks like what happened to the full stack developer over the past 20 years. The stack just kept getting fuller and fuller.¬†,16
1qoiat1,7,No I'm doing the MLOps too,11
1qoiat1,8,"A lot of this is due to increase in amount of people with programming and tech skills. There's simply so many more people with those skills today then there was 10-15 years ago.

10-15 years ago, finding someone who could code was more difficult than today, so companies took whoever they could find and gave them a good salary to do a lot of tech work. As smart phones and social media and e-commerce were still relatively new and starting to boom, these companies were just getting off the ground and needed someone, anyone, who could code. Those folks wore many hats in their job.

5 -7 years ago, as more people graduated college with CS degrees or did online bootcamps in hopes of getting these coding jobs, these companies found that they could be more selective in hiring. Okay, they found plenty of people who can code, but how many of them can code and interpret all this data we have? Thus the hiring direction narrows the scope of candidates.

In the last 5 years, we again see so many people getting CS degrees, learning to code, bootcamps etc. that companies can again be even more selective in their hiring. Okay, they found plenty of candidates that can code, can interpret data, and have AWS experience, now can we narrow the list down to candidates that have specifically our industry experience? The answer to that is yes, companies can do that now; there's just so many candidates.

If you've been in the workforce long enough, you'll start to notice trends. The key is identifying the next trend before it happens.",9
1qoiat1,9,"I am just about to jump into the market for a job again.

I‚Äôve normally worked SWE or Embedded, but I studied Data and I‚Äôve done DE work on my own because I think it‚Äôs a safe place to some degree.

I just finished a buildout of an Airtable CRM for a client. I treated it like an engineering project, GitHub repo, everything is JS or AWS Lambda. ETL for transferring legacy data. Unit testing yada yada.

This gig seems like it‚Äôs going to lead into building snapshot storage, data warehouse etc to support the product I built for them.

In data, I think your assessment is correct.

Companies want to see value.
One good hybrid engineer comfortable in SWE, DE, and ML.

Being that person brings direct value and impact to a company. This is the way.",14
1qoiat1,10,Yes and the fact they‚Äôve been trying to sneakily do this without paying up for it is insane. I literally read the JD and I‚Äôm like so you want an all around 1 stop shop for under 100k GL with that and I mean it honestly cause I really wanna know who‚Äôs taking on the unnecessary stress.,5
1qohpkm,1,"This is actually really nicely scoped. Most ‚ÄúSQL challenge‚Äù sites either stay at SELECT * FROM foo or jump straight into leetcode-style puzzles that never show up in real dashboards.

Grounding it in network monitoring data is smart too: lots of joins, time windows, and weird aggregation cases are exactly what people hit in real jobs.

A couple of thoughts / questions:

- Any plans to expose the schema up front in a more ‚Äúdoc‚Äù style way? When I teach juniors, half the battle is helping them read schemas before writing queries.
- Might be cool to add ‚Äúbad but works‚Äù vs ‚Äúidiomatic‚Äù solutions, especially around CTE vs subquery, or when to use window functions instead of GROUP BY gymnastics.
- If you ever expand beyond pure practice, this kind of dataset is perfect for showing how those queries then power internal tools or dashboards that non‚ÄëSQL folks use every day.

Bookmarked. This looks like something I‚Äôd happily throw at new hires for a week.",2
1qobvpg,1,"You can ensure replayability of your pipelines. It requires discipline, and also additional investment of resources every time you make changes.

I found that pipeline replayability value diminishes after three months or so, i.e. it's very rare that you have to replay batches from over three months back. 

It might be different if data is very critical and business want extra layer of insurance to ensure data correctness.",5
1qobvpg,2,"Not expecting this to be used in production or anything just yet, but I posted a library here yesterday, called ""darl"", that among other things gives you exactly this! It builds a computation graph which you can retrieve at any point in time as long as you have it cached somewhere (it caches for you automatically on execution). You can even retrieve the results for each node in the computation graph if they're still in the cache. You can navigate up and down each intermediate node to see what was computed, what was pulled from cache, what didn't run, what errored, etc.

You can see the project under github at mitstake/darl (no link since that triggers automod)

Demo from the docs:

    from darl import Engine
    
    def A(ngn):
        b = ngn.B()
        ngn.collect()
        return b + 1
    
    def B(ngn):
        b = ngn.catch.B2()
        ngn.collect()
        match b:
            case ngn.error():
                raise b.error
            case _:
                return b
    
    def B2(ngn):
        c = ngn.C()
        d = ngn.D()
        ngn.collect()
        return c / d    # raise a ZeroDivisionError
    
    def C(ngn):
        return 1
    
    def D(ngn):
        return 0
    
    ngn = Engine.create([A, B, B2, C, D])
    ngn.D()  # precache D (to see FROM_CACHE status in trace)
    try:
        ngn.A()         # This will and should fail to see ERRORED/NO_RUN statuses
    except:
        pass
    
    tr = ngn.trace()
    print(tr)                      # <Trace: <CallKey(A: {}, ())>, NOT_RUN>
    print(tr.ups[0])               # <Trace: <CallKey(B: {}, ())>, ERRORED>, (0.0 sec)>
    print(tr.ups[0].ups[0])        # <Trace: <CallKey(B2: {}, ())>, CAUGHT_ERROR>, (0.0 sec)>
    print(tr.ups[0].ups[0].ups[0]) # <Trace: <CallKey(C: {}, ())>, COMPUTED>, (0.0 sec)>
    print(tr.ups[0].ups[0].ups[1]) # <Trace: <CallKey(D: {}, ())>, FROM_CACHE>

If you save the graph somewhere and load it you can look at it from a previous run

    graph_build_id = tr.graph.graph_build_id
    
    save_graph_build_id_somewhere(graph_build_id)
    
    # in a new process
    from darl.trace import Trace
    
    graph_build_id = load_graph_build_id_from_somewhere()
    
    tr = Trace.from_graph_build_id(graph_build_id, ngn.cache)  # same functionality as in above snippet

I've used this in graphs with 10s to 100s of thousands of nodes for debugging, profiling and historical investigation.",1
1qoa34u,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qoa34u,2,"It depends on what you want to know? 

But without going to in-depth on the problem, suppose you want a graph of item sales per day. 

* Make an aggregate table with one line per item per day an item got sold. So you simply have a table with a date key, an item key and a number of items sold. The latter always being higher than 0. 
* Now add a table that has a list of all dates your business was open, a table that has all the items and left join these to the aggregate table. 
* Each line without an item has 0 sales. It will be high volume, but still limited to three columns.",4
1qoa34u,3,"If low memory usage and processing time is key, then I'd suggest you try the impact in your sistem for the 2 possible options:

- either you fill missing elements with an array generator or watermarking on your actual storage
- or you store the actual data and just calculate on read the gaps and fill with zeroes (or have a separate index with dates and left join)

There is not right or wrong, so it's more about your constraints and how you query that data. Do you represent it visually? Are you more focused on aggregated kpis? Outliers? 

I'm often a fan of filling in DB because it just makes maintenance simpler and data warehouse storage costs are not crazy. Plus we are talking about numbers, not too heavy either

Please alao bear in mind the type of db you use. Relational tables are good with joins (optionB), non relational might be better off with bigtable (optionA)",1
1qoa34u,4,"It would be nice to explain what tools/stack you're using - but I'm assuming you are processing this locally on your computer and reading from some files. Here's some general thoughts:

  
\- try to tailor your transformations to what the ""end goal"" is here, if its a monthly report then you don't necessarily need to set them to zero since you can choose between safe/unsafe methods for your math functions

\- process the data in smaller batches; if using a database, you can partition the data by date and process smaller partitions at a time; if running locally using python, loop through smaller chunks by date ranges or items",1
1qoa34u,5,"I'm going with the most basic answer that helped me through all use cases before, cross join with date table and generate what's missing. obviously only works if dataset is not humongous",1
1qoa34u,6,"Or you could just create some time variables and use normal regression models instead of time series.

Like week-of-the-month, some one hot encode of day of the week.

Anyway, you probably will miss the day of the sale, so maybe you could aggregate and try to predict sales by week.

Also, kinda counter intuitive to try to predict daily sales when you don't have daily sales.",1
1qoa34u,7,"You said you wanted to switch back to pandas. Have you considered using dask dataframes for processing? 

I know they are compatible with pandas libraries, the only thing to bear in mind is that dask can be slow on a single machine.

For modelling time series it is usually also worth having a date calendar dimension to which you can join to rather than 'generating' the missing rows.",1
1qo864m,1,"I don't know how you're surviving on Redshift.

Clickhouse should be fine for your use-case, Starrocks as well. But you'd need to hire experts or consultancy to design the architecture from your requirements and make it all production ready.

And beware of sql difference between redshift and those two databases, especially Clickhouse.",4
1qo864m,2,"Clickhouse was built specifically for clickstream data, so your use-case fits the bill pretty well.

If you keep it single node, it is rock solid. As far as the gotchas go, two come to mind:

1) Joins are supported, but not preferred. Build your data model in a way where analysts use OBT whenever possible.
2) Syntax is fairly different from Postgres one, there's enough of fun stuff to learn to use CH efficiently.",2
1qo864m,3,"What type of data? Geospatial? Need to run lots of concurrent dashboard/APIs? Ingest latency an issue at that scale? What‚Äôs the source of the data? Stream, batch etc?",1
1qo864m,4,"Gotchas is that unlike snowflake, clickhouse sucks for joins, but it excels at write heavy use case.

So that means ideally unless you can do materialized view as your ETL, you shouldn‚Äôt do any transformation there. 

Another gotcha is that querying performance is very dependent on how you order the data (it‚Äôs one of the required configuration when building the table). For example if you order by ID, but then query by let‚Äôs say first name, performance hit would be be significant.

I would say it‚Äôs a very good and cost effective engine, but even the cloud version, there are several ‚Äúquirks‚Äù that you might stumble upon, compared to like snowflake which I would say works out of the box. You‚Äôll extrapolate from this info what that means if you do self hosted.

It‚Äôs a great, cost effective engine (i can go along way with TB of data with a single 16gb instance), but the skill bar is higher in terms of management.",1
1qo864m,5,"Would you do self hosted CH or CH cloud? Clustering and replicas? Self hosted CH is a bit painful when you need to handle all of that, but cloud is ex pen sive.",1
1qo864m,6,"Are you using distribution keys on redshift? Thats a feature unfortunately literally not seen anywhere in olap systems (and i so yearn for it). If you need to replicate that model then you have to create a sharded clickhouse cluster and store the table as distributed table. 

Curious how much your redshift expenses were. Also you mentioned a PB per day, but what total storage do you do?",1
1qo864m,7,"1 PB or Parquet or 1 PB of something compressible, like CSVs? That can easily be 20x magnitude difference.",1
1qo864m,8,"Caveat : I work for Firebolt.

But we certainly don't suck at Joins and have better ACID compliance, so better consistency.",0
1qo7q0y,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qo7q0y,2,"What exactly do you plan to learn in llm and gen AI?

I suggest learning to use them to do the DE work faster, that's feasible and in scope

but i am certainly biased because this is how we teach nowadays - nobody is doing not-assisted work anymore.",19
1qo7q0y,3,Learn llm and genai. What does this even mean? You‚Äôre going to learn how to use them as tools or learn to keep up with research. The latter is almost a full time job in itself even if you have a strong background in marh or coding.,9
1qo7q0y,4,i swear its always indians with these kind of questions,6
1qo7q0y,5,"""almost 1.9 yoe"" I've read a lot of dumb things but this is the dumbest

This is worse than the parents going ""my baby is 49 months old!"" - she's 4, Jan",4
1qo7q0y,6,![gif](giphy|yDYAHbqe5DfyM),1
1qo6cgm,1,"Yes, NiFi is well-suited for this scenario. It can poll SFTP, parse Excel files, and write to SQL while preserving the original format, and its scheduling handles frequent intervals like every 5 minutes or hourly reliably.",3
1qo6cgm,2,That is an obscure ETL program. You are better off using SSIS which is already included in your SQL Server license.,2
1qo49nt,1,"I generally push for using sql whenever possible but there's still 2 things that we have to use python.

For extractions, I have some helper/utility code that handles generic things like abstracting away some API connections and setting up connections to datalake. The main python asset imports those helpers and then executes the actual logic of batch processing the extraction and loading to our datalake (hive configured).

For transformations, we have mostly moved away from python to sql since we can directly query external tables created from our hive datalake. But we still sometimes use python for the first layer where the data is weird file formats (like grib, netcdf, etc.) that require special processing - in such cases we read the files, convert to a dataframe, and then materialize it in our dwh. Our data platform's own built-in python materialization automatically handles incremental strategies and variable injections.

There are also very rare cases where data needs to be processed in a way thats not possible with sql - for example decoding airport weather reports like metar/taf, which require special python libraries to decode.

To answer your question about frameworks - we write functional code with very minimal object-oriented programming. For my team the rule of thumb is that a single python file/asset should contain all the logic tied to a single data entity/table/model. We never use notebooks (unless for quick local testing and adhoc stuff). In some cases, we extract different data from a single API endpoint, so for those cases we create a separate agent/helper to connect to the API and configure the parameters - this is the only case where we use a bit more oop.",7
1qo49nt,2,Snowflake I just do everything in Snowflake now lol.,2
1qo49nt,3,"I work on analytical workloads.  im using: scikit-learn, pytorch, tensorflow. Mainly notebooks but there are also ways to embed in SQL if i need to serve through an API or to a dashboard /  report.",1
1qo49nt,4,"For transformations, we use dbt python models when necessary (like decryption or forecasting)",0
1qo0c98,1,"the easiest thing to do is not care about the order they're loaded. 

loading data in parallel will cause this to happen.

if the original order matters then you should add an index number onto the data",9
1qnyuju,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qnyuju,2,"I am not a Data Engineer but have been working in Software Industry for a bit (20+ years). My only advice would be that for the next 2-3 years just soak up the knowledge and volunteer for any challenging work you can, ask any question, seek answers that others couldn‚Äôt. This will help you develop a true passion, deep skill and experience in the field which will become the foundation for the next 10 years of your career. Then a time will come where you will have to repeat it but you will know what to do because you have done it once.
All the best !!",16
1qnyuju,3,"First things first learn to ask the right questions to the Business analysts, architects, PM, etc as it is your first job learn to take advice and then work on technical stuff, coding is not the most important thing understanding how everything works and why you need to use one tech over the other and in what scenario will make you a better engineer.",4
1qnyuju,4,"Congrats! I don‚Äôt have any advice, but if you don‚Äôt mind me asking, how did you land your entry level job? LinkedIn? Indeed? Know someone? I‚Äôm currently looking to get my foot in the door for a Data Engineering position with a good skill set but no experience in the field",4
1qnyuju,5,I would suggest you look for a good mentor at your work. If you cannot get one and if everyone is busy look outside. This will help you do the right things and be successful in the role you are hired in.,2
1qnyuju,6,"I‚Äôm the exact way! I stated my first DE job post college 2 months ago and I use Azure databricks, I would say study medallion architecture and doing API to raw with azure storage, and learning dp aka Delta live tables (dlt) is highly important and your gonna be doing a lot of spark",5
1qnyuju,7,Try watching Tybuls videos on Azure on YouTube he's rhe best.,1
1qnyuju,8,"Congrats! I'm unemployed and trying to get that same job. I'm studying a lot and about cloud computing I'm prioritizing AWS S3 (storage), EC2 (compute), RDS (relational db), and Glue (ETL part, for orchestration). I'd study their equivalents on Azure.",2
1qnqbqm,1,">¬†As a hiring manager half-baked personal projects matter way more than certification.

The best I can do is half-baked work projects.",157
1qnqbqm,2,"I‚Äôm not a hiring manager, but I‚Äôve interviewed and assessed candidates. I look at a recent platform cert as a checkbox that verifies ‚Äúthis person can use the interfaces and functionality of this platform.‚Äù It absolutely does not say anything to me about the candidates ability to problem solve and deliver‚Ä¶just that they can use the platform. 

Other types of certs, like a python/sql/etc cert or a generic data engineering cert, or even a boot camp mean next to nothing. You should have a portfolio or be able to walk through some project details to demonstrate the higher level knowledge that would be covered by those certs. 

For me personally, I would pursue a cert as a means to learn something new and to be able to brag about it online afterwardsüòù kinda kidding. 

As an example, I‚Äôm using Fabric for the first time and would consider the cert as a way to guide and test my learning. I‚Äôd put it on my resume to validate that I can use the platform. But if I‚Äôm learning something like data modeling, I will probably try to find a toy project to add to my portfolio instead of a cert. If I saw a candidate with a data modeling cert, they better be able to back it up with some hands on experience otherwise it‚Äôs meaningless by itself.",49
1qnqbqm,3,What do people recommend for hands on projects? Especially in the cloud where it's expensive to do anything. I guess you can get hands on with most of the technologies but it's so much easier in the cloud and if that's what you're targeting only makes sense. I'm in the practice test phase of AWS but also looking to get hands on projects I can add to my profile. This subs wiki is good but always looking for something more,10
1qnqbqm,4,"my advice as someone who‚Äôs been on both sides of the interview:

take the time to learn and get hands on familiarity with the [thing]. do a project that mimics what you would do in a business setting.

the just lie and say you did that at your previous position. if you know if well enough you can answer questions about it, then it doesn‚Äôt matter.",18
1qnqbqm,5,"When I got my entry level Data Engineering job, a couple months ago I had no Databricks experience and or certs or any DE experience at all to be honest, but I made personal projects with REST API integration and using Apache Airflow DAGS that impressed my manager, so some employers do appreciate your creativity to problem solve not how good you are at a test/platform tha almost anyone can take",8
1qnqbqm,6,I have certs and I highly value/ recommend them as I think you become a better engineer. But yes they may not be valuable.,12
1qnqbqm,7,It's wild that a 4 day training from a vendor costs as much as 4 months of University in my country,5
1qnqbqm,8,"Cert are more for partnership requirements. Good for the companies,  good for the candidates",5
1qnqbqm,9,"Get a Cert because it will help you get a job.

But also get a cert for the jargon for an interview, if you want to get into Azure do the fundamental cert because you will be able to 'speak the jargon' in interviews by using the correct buzzwords for the none tech people.

But also remember the tech people will tell you are using buzzwords.",4
1qnqbqm,10,"I don't buy this.

A cert is better than no cert. A cert teaches reasoning with a vendor framework, which usually extrapolates to reasoning about common issues in the industry. 

Snowflake, AWS, and other tool providers solve similar problems to their competitors in similar ways. Knowing one extrapolates.

You aren't wrong that being provably a high agency person with good judgement is more valuable than a cert. I just don't know that that's a valuable comparison. I don't expect most engineers at a mid-level to really have a lot of reasons to spin up a DB for a meaningful personal project that isn't half-assed resume padding. (& I can get value quickly out of seeing a cert)",10
1qnorsn,1,"Those services will cost you 500 per day lol. 

I think it looks good. You could try downsizing the VM perhaps. Also look into converting the excel into csv to speed up processing. Excel can be expensive to load due to memory usage.",17
1qnorsn,2,you could use airflow to orchestrate your already existing python scripts in a more structured way,9
1qnorsn,3,"Is your VM on 24/7?
Modern engineering often separates storage from compute. Allowing you to access data while vm is not running.


There are quite a few free os tools you could take a look at.


But I think a tiny fabric subscription would also work in your case.


Interesting tools:
Duckdb
Apache airflow (scheduling)
Dbt core


Airflow chains tasks, and when one breaks followup tasks are not executed.
You can automate your checks.


Dbt is a great way to manage logic under source control and gives insights in effects of changes you make.


Duckdb is hot because it allows 1 single vm to execute queries very fast.",5
1qnorsn,4,You can use SSIS for your data processing / report generation. SSIS is already included in the SQL Server license.,4
1qnorsn,5,"You‚Äôre missing something. Where are these 50 SQL statements getting the data and why drop/reinsert?

If nothing fancy, I think what you‚Äôre saying you need is as hoc sql endpoint and somewhere to run Python to do your Excel magic. Have you considered renting a database from Azure directly (with some network rules of course) and using Azure Automation for your Python script? I assume you‚Äôre using an Excel library to author new or edit existing files and don‚Äôt need to actually run Excel eg to execute vb script or refresh a connection, and this can be left to the users (if not, you will need your Windows VM).",1
1qnorsn,6,"Not utilizing SQL Server Agent and SSIS is missing out on a lot of great functionality that SQL Server offers. They can create vastly improved execution patterns as well as better methods for handling errors or exceptions.

How do you manage your database backups, maintenance plans, and performance tuning tasks?",1
1qnorsn,7,Reusable Airflow DAGs driven by a YAML config file since you‚Äôre already using python.,1
1qnorsn,8,"SQL Server? SSIS would do the job.

But, i feel like knowing the little I do from what your queries look like: a combination of duckdb, sqlite, or perhaps your own postgres instance with pg_duckdb (if required) would solve a lot 

Although, I mean, if it works...why bother.",1
1qnorsn,9,"It sounds like the data you are processing is relatively small, is that correct?

I'm going to be honest, while overkill, you could do all of the data processing with Databricks for well under $500/month. The question would be whether you would be able to serve the data to PowerBI using Direct Query from Databricks DBSQL without going over budget.  You could also just do import mode to PowerBI as well if the reports are only refreshed daily which would save on costs.",1
1qnorsn,10,Alteryx,0
1qnmyiz,1,"Subscribe to the [TLDR newsletters](https://tldr.tech/), daily summary emails.",2
1qnlysg,1,"Batch processing with a cursor. Unless you really need real-time data, streaming is just not worth the headache or cost.",2
1qnjx8b,1,"When looking into something similar previously, I found the term ""record linkage"" and then `splink` for Python.

It can use DuckDB as the default backend.

- https://dataingovernment.blog.gov.uk/2022/09/23/splink-fast-accurate-and-scalable-record-linkage/
- https://moj-analytical-services.github.io/splink/
- https://github.com/moj-analytical-services/splink
- https://pypi.org/project/splink/",4
1qnjx8b,2,Google entity resolution techniques. Welcome to DE job security.,3
1qnjeht,1,"I feel like I saw a lot more of them in the Hadoop days, but not so much since Spark came around. I'd consider that the ramp to take career-wise, by the way.",1
1qnj3yv,1,"Although MacOS is Unix-based, it still does some things differently. cron is a legacy holdover and while it may be present, it‚Äôs not the recommended scheduler anymore.

The supported scheduler is launchd (LaunchAgents/LaunchDaemons). So changing crontab -e often does nothing. I would use a launchd plist and launchctl instead.

Hope this helps.",1
1qnispr,1,"Just a heads up, you know things.¬† You have skills.¬† Being laid off is a HUGE hit to the ego and can genuinely fuck your confidence in yourself.¬† What they did wasn't personal, it was business.¬† Your value hasn't changed as a person, they just made business decisions that you had no control over.¬† Take a day or two to decompress and then treat filling applications out as your job.¬† If you need help with formatting your resume you can DM me and I'll help.¬† You'll get through this.¬† (2 layoffs in 4 months for myself in 2022 a month after buying a house.¬† I understand the stress.)",152
1qnispr,2,"One piece of advice I‚Äôll give is SIGN UP FOR UNEMPLOYMENT! Many people have this feeling like they don‚Äôt deserve it or they won‚Äôt need it, etc. but it‚Äôs literally there for you to use in this exact situation!",142
1qnispr,3,"Dw, what you‚Äôre describing is something we've seen with Data PMs who‚Äôve spent time in startups. You do have experience and gut instincts, they just never had a chance to solidify into clear frameworks because startups. ;) 

What usually helps isn‚Äôt starting over, but doing a short, structured upskilling pass to organize what you already know.

If you‚Äôre prepping, focus on:

* End-to-end data products: how data moves from source ‚Üí pipeline ‚Üí metric ‚Üí decision
* Metrics: how KPIs are defined, where they break, and how you‚Äôd debug a ‚Äúbad‚Äù metric
* Analytics literacy: enough SQL/analysis to sanity-check results and push back with confidence
* Big data concepts: warehouses, pipelines, batch vs streaming, mental models, not deep engineering

Once these are structured, most people realize they‚Äôve been doing versions of this all along, just without the 100% language and confidence interviews expect.",74
1qnispr,4,"I went thru a layoff a few years ago. I had only been in in a true DE role for about a year. I felt as you feel, worried I didnt have enough skills, and unsure how I was going to proceed. It was quite hard, but I did find a new role. I had to learn a lot of new skills. Things challenged me a lot. All in all, though, I got through it, and you will too. The best advice I can give, aside from what is already here, is to be resilient.",17
1qnispr,5,What‚Äôs a data PM ? First time ive heard of it,10
1qnispr,6,"First of all; it seems that is not your fault; that is the first thing you have to be in your head after being Laid Off.

Then, you count your saving, prepare your CV and began looking for offers. And assure yourself that you are still the best for the position.",8
1qnispr,7,"Hopefully you got a severance of some type or have some savings. ¬†What you need to do is take a week and decompress. ¬†Like literally don‚Äôt think of work. ¬†You then need to start looking for work, don‚Äôt get wound up in knots over ‚Äúwhat you need to know‚Äù as¬†

1. You aren‚Äôt going to get to any level of competency in the gap between jobs (unless you can afford to be unemployed for a long time) and

2. You‚Äôll probably pick the wrong thing (you don‚Äôt know the stack your future employer will use)

Your focus needs to be on finding the next job not training for the job you don‚Äôt have.¬†",20
1qnispr,8,You certainly know more than you think. Working in chaotic environments builds up skill no matter how frustrating it is.,5
1qnispr,9,"To be honest, 4 years is more than enough time to stop 'winging it.' If you feel like you know nothing, it‚Äôs because you‚Äôve been reactive, hiding behind startup chaos and 'Job Title Inflation' instead of building a real technical foundation.

The Data PM role can often be a mix of everything and nothing, but in a stable company, you are expected to be more than a messenger. Start mastering SQL, Data Modeling, and the technical pipeline. You need to be able to explain how data gets from A to B",28
1qnispr,10,"OP, what will help this sub help you is by telling them what you think a data PM does and what your day-to-day role was at the company. This way the community will be able to better direct your misconceptions (if any), and help you upskill based on what you know/don't know/don't know what you don't know. Be as explicit as possible so that this community can help you fill in the gaps. Good luck.",8
1qnfgwo,1,Open source: [https://github.com/databendlabs/snowtree](https://github.com/databendlabs/snowtree),1
1qnf69j,1,What's the benchmark vs DuckDB?,1
1qnf69j,2,">wanted to fetch data directly from disk

Which cylinder do you need? What's the disk geometry?!?

>Memory and Cache-Aware Layouts

>Use EBPF,Direct memory access(DMA)/(DPDK) to skip system calls and hit the disk directly to extract data. Additional exploration of cache-line alignment, prefetching strategies, and vectorized execution paths could further improve performance on modern CPUs.

I am interested in this aspect of the project.",1
1qnf64h,1,"If you have good knowledge base in data foundation dbt is nothing for you. Because dbt will transformation tool for you, you already have knowledge about data modelling, star schema, scd types, fact dimension, semantic model, just you need to build those things with dbt. 

You just need to understand dbt ci habits and components because that features most important things in dbt because most of the data team has a lot of problem in production(they may have a lot garbage SQL models tables in dwh without any review and ci control actions those are the problem which exactly dbt solve) 

Components you need to understand for governance, devops... 

- manifest.json 
- run_result.json
- graphs
- docs

And you need to understand slim-ci methods and devops skills but those are not a big deal.",6
1qnf64h,2,Learn a bit of the dbt concepts it‚Äôs rather easy.,3
1qnf64h,3,"Feel like with a good baseline you can just go on a speed run through all 3 of those tools in a couple of months. 

They each have rabbit holes of course but if you just stick to SQL, data modeling and transformation, materialization, and BI you will go a long long way just understanding how these 3 tools are the same, how they differ, pros, cons, etc.

So tldr do a little of all 3.",3
1qnb2qk,1,"Oh cool.   Is it the metabase serialization API the thing that phones home to give you usage data?  

I don‚Äôt currently use dbt but I would still like column lineage as you‚Äôve described.  I imagine someone could build a column lineage experience purely only with the serialization API?",1
1qn92m3,1,"You already have a foot in the door. Do some reading on architecture, tooling and patterns. If you can also show willingness and ability to learn in interviews you'll be in a good place.",1
1qn8j0h,1,Just use transactions.,14
1qn8j0h,2,"I don't really understand the question because you answered it yourself, more or less.",6
1qn8j0h,3,"For all manual SQL changes we are required to select the changeset into a temp table then insert that into a new backup table, then perform the change. 

If it‚Äôs too large for temp table, then we backup the entire table being changed.

And if it‚Äôs bigger than that, we take a db backup. 


And yes, you have to submit your SQL query , which is then checked and ran by a DBA. Devs don‚Äôt have datawriter permissions in production.",6
1qn8j0h,4,"Between transactions and all the built in data history functionality in most modern data platforms (I.e time travel and undrop in snowflake), I really don‚Äôt see the worry anymore",3
1qn8j0h,5,This post sounds like ai.,3
1qn8j0h,6,"All the time.   But I have 3 rules.

1) Always run a Select with whatever clause you are about to run.

2) Do it in a transaction and make sure it's the same record count as Step 1.

3) Don't fuck with posted transactions that affect GL.  Changing anything like that can make accounts not balance and destroy any data integrity your application promises.  If something was posted wrong they can reverse it in the app and redo it correctly.",2
1qn8j0h,7,"If you're using SSMS you can use [SSMS Boost](https://www.ssmsboost.com/Features/ssms-add-in-fatal-actions-guard). Other than that, check your plan before you run anything on production and see if there's anything unexpected.

    GRANT SHOWPLAN TO user_name;

You can also first check with an equal SELECT statement what your UPDATE/DELETE is going to do.

Your normal user can also have only read permissions for example, and you *have* to use 

    EXECUTE AS super_user_yourname

to run your code on prod.

Last option that I'm a fan of is: Nobody can INSERT/UPDATE/DELETE in prod, any code has to be run as a Post-Deployment script through CICD, that way it's traceable.",1
1qn8j0h,8,"yeah we hit something very similar trying to track this alongside our core system

it started as a quick workaround but paper and excel got messy fast and people stopped trusting it

once we added a small side tool to capture the date once and keep the history things cleaned up quickly

we got better visibility and fewer misses without having to mess with the core system right away",1
1qn8j0h,9,"If you are truly analytical, just recompile data from source again.
If you have some kind of data generation where this not possible (e.g. a source that deletes faster than you or you truly master a source) then have proper procedures in place, from DR to temporal tables.",1
1qn8j0h,10,"Had a DB migration with an empty constraints scripts (missing 2k from the old DB) once. BOY, that was a a day.",1
1qn37zp,1,All sports teams will pay peanuts because it‚Äôs a dream job for many. I would actually love to try one of these jobs one day if I am FI and don‚Äôt really need money.,308
1qn37zp,2,"It's the same thing with gaming related jobs, for many it's a dream so they will accept worse conditions.",86
1qn37zp,3,"I call that the ""passion tax""",23
1qn37zp,4,"Look up ""Why you want a boring job"" on YouTube


These ""dream jobs"" are just breeding grounds for underpay and abuse",19
1qn37zp,5,Welcome to jobs in sports.,9
1qn37zp,6,"Something odd is happening, I'm seeing this more and more. They know how desperate some are and taking advantage. I'm starting to see just how bad it is.",6
1qn37zp,7,Passion tax,6
1qn37zp,8,"salary ranges posted on LI are usually wrong, I'd trust the numbers from the actual MLB site itself if there is a posting",15
1qn37zp,9,"I interviewed with the Minnesota Twins in 2019 as a data scientist, job was to essentially help predict their financials from ticket sales and concessions. Would have needed to take a major paycut and move to Minnesota. Job ended up getting furloughed in the pandemic.",4
1qn37zp,10,"Not related to DE, but I once interviewed with the CEO of Serie A in the US. I told him I expected the higher end of the salary band listed and he said ""sorry we can't do that"". I said why not and he said ""we don't work in sports for money, we work in sports for the love of the game"". I laughed. Did not get the job",3
1qn1vu8,1,"Basically, it sounds like you are leaning towards siloing data. It probably isn't your best move. While many processes take place within a given domain, the really interesting things are between domains. That is extremely fertile ground for data insights.

Federated data warehouses are fundamentally unsound. Think about what has to be done to bounce a 1TB table in one warehouse against a 1TB table in the other. At some point, you will be duplicating the data across a relatively slow medium. The physics work against you. Federated warehouses are really only suitable for researching potential data and, sometimes, low volume OLTP systems.

With what you are proposing, researching financial healthcare charges could very quickly become difficult to do.",16
1qn1vu8,2,"Finance doesn't look at any billing data, even in the aggregate? Patient billing converts into projected reimbursement (based on CMS or negotiated rates), which is the basis for budget planning and forecasting, all of which would typically be performed by Finance.  
  
I think there's a disconnect somewhere. What can you tell us about your source system(s) and the current reporting used by Finance?",2
1qn1vu8,3,"How confident are you that those two data sets will never need to interact? Because I made that bet at a mid-size healthcare rollup firm a few years ago and it went really sideways on me as soon as we started reporting on it.

First the CFO also wanted to see some of the corporate-level finance data reported on parts of the executive dashboard, then he wanted some of it added to the FP&A dashboard so that *they* could see it, and then eventually they wanted to balance some of those financials against the accruals and to-target revenue analysis that FP&A was doing weekly.

I would bet dollars to donuts that *someone* will want to commingle these two data sets sooner rather than later, the question is whether you think that person will be low enough on the hierarchy that you can tell them to just do it in Excel, or whether it‚Äôll be someone high enough that they‚Äôll tell *you* to do it.",2
1qn1vu8,4,"The original definition of data warehouse was that it was ""subject-oriented"".  So, you might have one warehouse for finance, another for marketing, another for product, another for HR, etc, etc.

In my experience this doesn't work in small companies - because you just don't have enough funding to pay for a half-dozen separate teams.

But in larger companies I think this is far better than a single warehouse for a number of reasons:

   * The team is closer to the business: they care more about delivering value and they generally understand the data better.
   * They are focusing on a single subject rather than being ""a center of excellence"" that focuses on getting the cheapest labor possible to use some antiquated ETL tool.",2
1qn1vu8,5,"Hot take üî•

Never, unless there is a regulatory or legal requirement to do so. For example, GDPR requires data to stay within the country so global companies often have this need.  And I am pretty sure this could still be accomplished with one data platform like Databricks or Snowflake. 

When you are not a regulatory or legal mandate you have ways to ensure isolation in most platforms (I want to say all but that is even a bigger hot take).  Adding another data platform sounds like the easy fix but it by far is not. There is so much overhead (DR, maintenance, increased complexity, etc.) having to manage multiple platforms that would be better served by just setting up one platform that has the right controls in place.",1
1qn1vu8,6,"are you 100% sure there could never be value in analyzing them together? that it wouldn't be beneficial to analyze the financial (say, annual revenue) alongside the medical (say, monthly billing) to see for example if there's a relationship between client billing and the company balance sheet?


seems unlikely to be the case go for it",1
1qmu3hh,1,"Airflow, Dagster, dlt... What your desired tool has that those don't?¬†",8
1qmu3hh,2,Why? What will you provide that existing tools do not?,2
1qmu3hh,3,I think [dagu](https://github.com/dagu-org/dagu) has done similar things.,1
1qmseer,1,üëã,2
1qmseer,2,"What is your motivation for this kind of project? Did you face any problems that knowing what has changed would be useful?

I would love to try it in the next couple of days.",2
1qmqfw7,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qmqfw7,2,dear sweet jesus. please build it on fabric and come back in 6m and tell us how it went. make us happy.,107
1qmqfw7,3,"DBT open source vs Microsoft: data is portable across clouds. Microsoft fabric is vendor lock in. Need to change to another cloud, dbt can do that. Need to point to a different database/RDBMS, change the connector and dbt will adjust the sql to the targets sql syntax. With Microsoft Fabrice, each scenario requires code refactoring where dbt does not.",27
1qmqfw7,4,"‚ÄúKinda the new standard‚Äù

Thread solved",20
1qmqfw7,5,"DBT is a declarative framework to Transform data in your ETL, with very good ci/cd and platform integrations.

Fabric is a new platform for reporting combined with ETL. It‚Äôs basically Microsofts attempt to take customers who already use PowerBI away from Databricks.

If you are serious about Data Governance, CI/CD and AI forget Fabric, also a lot of things have bugs or simply don‚Äôt work. If you just want to make your reporting life easier without much IT involvement and have a small team Fabric is fine.

Also afaik Fabric has no declarative pipeline framework similar to databricks DLT or DBT.",12
1qmqfw7,6,I've been wondering the same with Databricks. Why would someone use dbt when you have Lakeflow Declarative Pipelines with SQL?,7
1qmqfw7,7,"First of all, it's Microsoft and second, it's Microsoft. Must I say more?",8
1qmqfw7,8,"Fabric is a platform and dbt is a workflow. I dont see how this is a comparison. Fabric covers ingestion, storage, transforms, and BI but most of its transformation layer is still GUI driven and tightly coupled to Microsoft. dbt matters because it enforces engineering standards around SQL like version control, tests, CI, and portability across Snowflake, BigQuery, Databricks, and yes even Fabric backends. If you want to stay in Microsoft shops only, learning Fabric first is fine. If you want broader data engineering mobility, dbt is the common language. In practice, businesses often separate concerns. Use a managed ingestion tool like Integrate.io or Fivetran to land raw data then keep all business logic in dbt so it is not locked inside dataflows.",8
1qmqfw7,9,"As you are coming from Power BI with no Software Development background, I get why you think like this.

DBT is a great Tool/Framework for Data Engineers to use. It streamlines a lot and makes development of a BI-Solution much more structured. Additionally, it provides a ton of feature to make use of and is compatible with almost every modern Data Platform available on the market.",5
1qmqfw7,10,"The main point when considering alternatives (not only dbt, but almost anything in your stack) is that some tools become de-facto standards, while others remain niche.

Yes, you can probably achieve similar results with Fabric and even with stored procedures. But the real questions are:

1.Will it be as easy and predictable as with dbt?

2.Will your company be able to hire someone who can maintain these transformations without friction?

3.Is the ecosystem as mature: documentation, examples, community?

4.Can tools like GPT or Claude understand and work with this stack?",3
1qmmenb,1,"When I joined a startup that used AWS and I had never used it, I was expected to be able to set up all of my own components. While I was able to use chatGPT for a bunch of stuff, there's so much terminology and jargon that's specific to AWS. 

Studying for this cert helped me accelerate my learning and I had the company paid for the test. So while it isn't necessary, it helped me and it also got me more hits on my resume. This is my own personal experience, can't speak to anyone else's",9
1qmmenb,2,My wife who had never touched AWS before could get that certificate after few months of grinding.,3
1qmmenb,3,AWS certs aren‚Äôt worth the pixels used to display them on a screen. Anyone can grind (or coast) and get those certs. Actual job experience and accomplishments are orders of magnitude more important when applying for jobs.,2
1qmmenb,4,"You should definitely get it as some ATS are tuned to see for particular certs in the resume, and there is no harm in getting something and never using it than not getting it and then regretting someday.",1
1qmmenb,5,"I had 3 months of exp with AWS during a MLOps project. Jumped into AWS SAA and it definitely leveled up my understanding of the major services and how to put them together.

Go for it.",1
1qmmenb,6,"Yes, you can go straight to SAA-C03. Your Azure + Snowflake background will help with core cloud/data concepts. Do a quick AWS ramp first (IAM, VPC basics, S3, networking), then practice exams to learn AWS-specific patterns. A ‚Äúfundamentals‚Äù cert isn‚Äôt required",1
1qmlfg6,1,forged in the fires of production,18
1qmlfg6,2,"\> From my own experience and from some of the discussion threads, it seems like the common denominator and a lot of companies is ship first, model later

I believe this will change soon. Modeling will make you ship faster with AI-assisted coding. It's also important for certain hyped features such as text-to-SQL. Unless the LLM bubbles burst (meaning it would be prohibitively expensive to use), LLMs will be come one of the main data (model) consumers.

As for your question: do proper data models and then code with Cursor or Claude Code. It will be much faster than you hand writing transformation code.",36
1qmlfg6,3,"Work.¬†


Etl was wat we still did in the early 2010s.


Elt is far superior in my mind though. Allows for reuse of data a lot more.",16
1qmlfg6,4,"I was very lucky to learn it while working for a not-for-profit.  low stakes, lots of bright people in a collaborative environment.  Kimball is a great tool for stars.  There‚Äôs some good open source ER modeling software out there.  Get it and play with it.",5
1qmlfg6,5,I got mine by joining a data modeling team. Those teams are getting harder to find these days as companies are disbanding these groups as irrelevant in today's cloud based architectures. An interesting recent development is the push from senior executives for more data modeling work rather than less. I might actually make it a couple more years before being put out to pasture.,7
1qmlfg6,6,"I worked at a startup where their data was a complete disaster. They let me procure dbt cloud with 2 seats ($200/month) and I was able to build a new data warehouse from scratch, which was mostly medallion style.

I currently work for a much larger company, where a lot of it is a combination between medallion and OBT. I studied Kimball and Inmon for my master's program but that was more necessary before cloud computing and storage. Some of the work I do includes optimization to reduce compute and storage.

The data warehouse I work with contains 3k+ models, which over 40+ users contribute to. It's open development but my team has to enforce quality control for the MRs.",3
1qmlfg6,7,In a high volume environment access patterns define the model,3
1qmlfg6,8,"I started in a non DE role, but had to help out with different kinds of analysis on an ad-hoc basis. 

I hate repeating work, so I would try to find ways to automate reports I did. First in Excel, then PBI, then SQL. 

I know engineers that have been working longer than me that don‚Äôt have a clue about data modeling, because they always just try to brute force whatever‚Äôs needed to get to a flat table. 

In terms of getting experience, find an actual project you want to work on. Make a Pok√©dex or work with data from sports or games you like, it doesn‚Äôt matter. 

Important thing is that you try to apply those concepts to an actual problem you want to solve, and more importantly, actually follow the guidance documentation that‚Äôs out there. 

One of the big steps people miss is the conceptual modeling stage, whether that‚Äôs an enterprise bus matrix/business event matrix, etc. If you don‚Äôt define what you‚Äôre trying to do and how you expect everything to tie together conceptually first, then it‚Äôs unlikely you‚Äôll have thought through everything you should be.",3
1qmlfg6,9,"Learn how databases and relationships work, learn some of the theory behind database normalization, play around with some toy examples like the good old example about the library, books and loans, and that's it. Just keep on trying to model real world problems into a nice and efficient way, what's the mystery? Just figure out how some businesses relate their steps in the process to each other and go forth modelling it.",3
1qmlfg6,10,"The data engineer was on mat leave, Kimball‚Äôs book and dbt, BAM!!!",3
1qml84c,1,"Hi, based on your use case, I think you can try RisingWave, a PostgreSQL-compatible streaming database. It can ingest Kafka streams, and you can use standard SQL to build materialized views that incrementally compute your ML features in near real time. Because those features are stored and queryable directly in RisingWave, your application can often read them from RisingWave without needing Redis as a separate serving layer.

So, it can act as both a stream processing engine (like Flink) and a low-latency feature store/serving layer (like Redis), using standard end-to-end.",1
1qml84c,2,"You can try¬†https://github.com/feldera/feldera

It has a delta lake connector¬†https://docs.feldera.com/connectors/sources/delta/ ¬†as well as postgres and redis.
It also supports several advanced streaming constructs¬†https://docs.feldera.com/sql/streaming

The nice thing about the problem
you mention with ""getting the code do to the right thing"" is that you can express your data processing queries as regular SQL tables and views.",1
1qml84c,3,"You can look at lenses.io.  

(disclaimer: I work for them)

SQL Processors is a Kafka Stream based data processing engine that's Kubernetes native.  It's great for relatively simple data processing requirements (stateful & stateless).

Lenses K2K is a Kubernetes native data replicator that's an alternative to MirrorMaker2. 

Both products are integrated in a Developer Experience (UI/API/MCP with IAM & Governance, ...)",1
1qmkh3e,1,"I‚Äôve been using SQL Database Projects extension in VS Code, integrated with a GitHub repo, which works really well.

The project can be deployed as a whole to any server (test/ live). It also checks the ‚Äòbuild‚Äô first - ensuring you don‚Äôt reference columns that don‚Äôt exist etc.

Using with GitHub for version control means team members can use a shared repo, working on individual branches etc. You can set it up so that developers can‚Äôt ‚Äòmerge‚Äô code until it‚Äôs been reviewed by someone else, and so that code can‚Äôt be merged if the project doesn‚Äôt ‚Äòbuild‚Äô.",3
1qmkh3e,2,"The best version control on the market is git. It might take some getting used to it, but once you learn the concept, it works very well. For CI/CD, I highly recommend gitlab.",3
1qmheqn,1,"i didn‚Äôt really look at the repo too closely but your sql looks and smells good, and if you set up all those other monitoring actions you would be pretty competitive for most AE roles out there

just make sure you can explain how and why you made all your decisions",3
1qmgzam,1,"I think some planning/budgeting software still uses it, but is in the process of switching to tabular models, which is the other variant in SSAS, and -as others have mentioned- is the base of PowerBI. 

  
If you are diving into SSAS, just go tabular, and its useful.",4
1qmgzam,2,"PowerBI or other tools do this now. 

Us DEs just do DW, storage and governance. 

PowerBI files are actually cubes.",6
1qmgzam,3,"Multidimensional is currently still supported, but the tabular models are the focus of new investment, and at this point they are much more widely used.

At a high-level, columnar compression and growing server memory made an in-memory columnar approach preferable to the older model that relied on explicit attribute hierarchies to enable materialized aggregations.  It's simpler to design, deploy, and query, and in most scenarios performs better.",2
1qmgzam,4,"For example, when you connect to a PBI model via excel it sends MDX , which PBI converts to DAX.

So yes, MDX is still being used.
However, as a user or developer, would you ever need to learn how to construct MDX ? No. Unless you were a self loathing lunatic.",2
1qmgzam,5,"¬†Check out my proposal to polars for supporting multidimensional array style operations. Since this is just abstracting join and windowing operations it can easily be adapted to work with plain sql too. I know it‚Äôs not exactly relevant to what you‚Äôre asking, but it‚Äôs tangentially related.

https://github.com/pola-rs/polars/issues/23938",1
1qmgzam,6,"It's changing, I'm scrambling to upskill as well. Look into the semantic layer and the new bronze, silver and gold architecture. Warehouses are still important but we control how people get to the right answer consistently differently now. Speeds things up BUT there's huge tradeoffs. I have a hunch we will see companies move to more of a consolidated reporting layer as silo development & tech debt leads to inconsistencies.",-2
1qmgzam,7,"I think they are on the way out personally. I did a lot of essbase in my earlier career. What it was really good at was aggregation and funky financial calcs. Now databases can aggregate just as fast or faster.  I think it still has a place potentially in the finance side of things as managed service saas offerings, but I see it slowly phasing out from now on predominantly",0
1qmgqpn,1,"Can I ask why rust? It's not really supported by AWS but definitely a language I'm interested in.

Also, how much to implement?",1
1qmgqpn,2,"I think conceptually the pipeline makes sense. Regarding implementation, I don't think there's a point in wrapping it in cookie cutter, which is a package more suited for minimal project structure setup.",1
1qmgqpn,3,small nitpick: in the SQL either write all keywords in lower or upper case - don't mix,1
1qmfyr1,1,If you are dealing with large dataset why bother with pandas. Either use polars or duckdb,113
1qmfyr1,2,"So first answer:

From pandas 2.0 onwards a lot of change was made to move from numpy into arrow, so you cant just use np.nans as pandas nans now, its pd.NA. Instead .replace operations you use assignement. Strings and datetime gets some changes as well as categorical types. Some changes with pd.read_excel as well. Slicing and a lot of your operations need to be explicit now instead of implicit. 

Whats going to bite you the most is numpy rather than pandas here.

Second Answer:

Use polars + pandas, especially once you get everything setup in arrow types, its a seamless transfer of the dfs; while working with my team I use polars for the heavy stuff like merges, concats and stuff. ANd pandas for anything that needs to be verbose and redeable, like mathemtical operations or column based functions. Polars sucks at the whole thing because their approach of map_elements is inconsistent and expects something everytime. Polars also breaks their apis and their intended behavior quite a lot.

Just 1.0 to 3.0 from numpy to arrow should be about 4x boost in perf, polars + pandas can be 5-20x and pure polars can be 10-30x.

The main thing I love about polars are sinks and lazyframes. And the streaming engine, I had some pandas code which took 64gb of ram, mixed it with some polars and sink and now its down to 10gb of ram",18
1qmfyr1,3,"If you need speed go polars,
If you hate verbose code go pandas or mb just dont use pandas at all

If you need speed and hate verbose code unfortunately allthoufh we are in 2026, R language's tidytable and data.table are still the only decent ines",6
1qmfyr1,4,"From my understanding polars use less memory and is faster than pandas. Also the syntax is much like spark so when you can transfer to spark easily. However, many production systems run pandas. I dont think theres a Geopolars so you would have to do some bulk work in Geopandas and then compute it in polars (you can swap between polars/pandas easily with polars syntax). Doesnt sound optimal but it could be...",17
1qmfyr1,5,"Try migrating one small existing solution (or self contained unit of something) to pandas 3, and also to polars. 

Then you can compare performance and also what you think subjectively of the developer experience.

I am a huge polars fan. For me, reason alone to use it over pandas 2 is how it handles data types, which works much better with delta lake & parquet typing.

YMMV of course.",5
1qmfyr1,6,"Why did you create a second account to ask the same question you asked in r/Python

Read the changelog and migration guide or ask a ChatGPT",7
1qmfyr1,7,"How big is this code base? I don't think it's really that much of an undertaking to migrate from pandas 1.0 to 2.0 or 3.0. If anything you can just use Claude code to do 99% of this migration, and it will probably be very very good. This is the kind of thing it excels at",2
1qmfyr1,8,"i wouldnt think of it as ‚Äúpandas 3 is magically faster than 1‚Äù. most of the real gains came in 2.x with the pyarrow backed memory model and better string / nullable dtypes. 3.0 is more about cleaning up legacy stuff and making that model the default, not a night and day jump.

for 20m+ rows, pandas can still struggle depending on ops, even with tons of ram. polars is legit faster for a lot of workloads, esp groupbys and scans, but it‚Äôs a diff mental model and ecosystem. the geopandas thing is real too, if you rely on that a lot, pandas is still the safer path. i‚Äôd prob modernize pandas first, then reach for polars where perf actually hurts instead of a full jump all at once.",2
1qmfyr1,9,Go polars,1
1qmfyr1,10,"The difference is minimal, most of the work goes into keeping the project compatible with newer versions of Python and other libraries, small big fixes, and cleaning up the docs.

Pandas 3 introduces pandas.col() to avoid lambdas in filters and assign. Funny enough that change is probably one of the smallest changes in the codebase, while in my opinion it is by far the biggest change in the last 10 years of pandas development.

If you want to go into more details of what changed in pandas 3, I write about the main changes with practical examples: 
https://datapythonista.me/blog/whats-new-in-pandas-3

Good news is that migrating should be very straightforward if you don't do heavy use of internal functions.",1
1qm4vul,1,"when it's one thing the world does not need more of, it's another piece of sw that does something with csv.",4
1qm4vul,2,"Hi, 
I am naive person and your project looks interesting I personally thought so many times to get an sql with csv , so just a quick question what is the maximum size of csv that this project can parse? Also there are a lot pf other file formats out there if your code is modular then adding new file formats would be a good feature addition.
Thank You",2
1qm4vul,3,"Nice idea, this is one of those boring problems that quietly ruins days when it goes wrong.

Stuff I‚Äôd absolutely want as a data engineer:

1. **Type + constraint awareness**  
   - Validate against a provided schema:  
     - max length for strings (so you catch truncation before the DB does)  
     - numeric ranges (e.g., > 0, integer vs decimal)  
     - allowed values / enums  
   - Nullability checks: flag missing values for NOT NULL columns.

2. **Key / relational checks**  
   - Primary key uniqueness (and optionally composite keys).  
   - Foreign key checks if the user can upload a reference list or connect to a DB to validate against existing keys.

3. **Encoding / whitespace / formatting**  
   - Strip or at least flag leading/trailing spaces (especially on keys and IDs).  
   - Detect inconsistent encodings or invalid characters.  
   - Normalize line endings and quote usage; flag malformed CSV rows.

4. **Header and column mapping**  
   - Detect duplicate headers.  
   - Option to map CSV columns to DB columns manually, and validate that all required DB columns are covered.

5. **Date/time and timezone sanity**  
   - Detect impossible dates (you mentioned this) but also:  
     - mixed formats in the same column (YYYY-MM-DD vs DD/MM/YYYY)  
     - timezone awareness or at least flagging multiple timezones / offsets in one column.

6. **Boolean and categorical checks**  
   - Validate booleans against an allowed set per target DB (true/false, 0/1, Y/N).  
   - Case sensitivity for codes where it matters (e.g., product codes).

7. **Performance / UX bits**  
   - Sampling mode for very large CSVs, with an option for full validation.  
   - A clear summary report: error counts per column, first N examples, and a ‚Äúthis row will break your insert‚Äù preview.

Feature ideas that might make this especially useful:

- **Schema import** from an existing database table (introspect table, then validate CSV against it).  
- **Safe default transformations**:  
  - trim spaces, normalize booleans, standardize date formats, remove thousands separators in numbers, etc., with a ‚Äúshow me what you changed‚Äù diff.  
- **Dry run mode**: actually run the generated SQL against a temp table or transaction and show what would fail.

If you really want data engineers to love it, being able to point it at a DB, pick a table, and have it auto-validate a CSV against that table‚Äôs schema and constraints would be killer.",1
1qm3nnr,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qm3nnr,2,"You're asking the wrong question. You're also focusing on the wrong things. Let me reframe it for you:

""Ive been using a hammer for the past 10 years. I've built houses, boxes, fences and doors. I know how to frame, tile roof, and even straighten a car bumper with it. What tool should I learn next? Im eyeing a skilsaw, nail gun, and even power drill.""

Instead, focus on the meta. That is: engineering for solving business problems. The tool you use to build systems doesnt matter as much as successful execution and bringing value to the business. Can you model processes? If someone gave you a process model, can you translate that into technological implementation? Can you translate a process model into a data model?

If you wanna get better at python, do leetcode exercises and learn data structures and algorithms. By accident, you will learn how to program in general‚Äî in any language. The real value is in learn how to think abstractly and creating an algorithm that will solve the business need efficiently and correctly. The brand of language or server is a political opinion or strategic decision (eg: python has the best tooling for DS).

If you want to learn system design, build a python app from zero, no framework. It'll teach you how to organize code, how to build mental models around your code, separation of concerns, IO, dataflow, system orchestration, deployments, etc. Learn what tools exist within your ecosystem of choice. This will give you what you need to think abstractly‚Äî macro decision making, not micro. Focus on big picture like a tech lead (not like a CEO or CTO, thats too big picture).

You know how to use a hammer very well already. Time to become an architect, too.",50
1qm3nnr,3,"My biggest thought is - what is your new company trying to achieve? Whats the goal? This will be one better give u a solution 

Otherwise, very general but ->

AWS is reccomended unless azure is already what ur company‚Äôs stack is on

How‚Äôs your system design?

I‚Äôm assuming you more data modeling since u know ETL? 

 Learn DBT and airflow , it‚Äôll come in handy 

Does your company need real time streaming? Kinesis? Kafka?",5
1qm3nnr,4,"I was a SQL Server DBA for 15 years. A new CIO and CTO came in and said ""in 18 months there will be no Microsoft Technology"".  The last SQL Server was switched off 17 years later. 
My path away from Microsoft started with an EdX free course on learning Linux. I  learned that Shell scripting could do way more than I had realised. Powershell was in its early years so this increased my interest in Powershell too.

Because of the deadly duos rabid hatred of Microsoft I had to learn a lot in a short space of time on an alien OS and with a lot of alien tech. One of the system admits taught me some basic Docker skills. This let me build a small set of images for the tech we were migrating to.  As a Dockerfile is a very small I could reproduce it at home to build the images I wanted.

I'd say basic Docker skills are essential even if all you do is build your own learning labs.  Look for courses by Nigel Poulton.

For Python, make sure you get comfortable with virtual environments. Again, ring fenced learning but you will need this in a Python environment.

Learn about PyTest and Behave for testing your Python code. There are some specific data testing libraries out there. 

Once you build learning labs and are comfortable with testing approaches, life becomes a lot easier.

I built an ETL framework using an early version of Spark. It was effective, what the CTO insisted on, but was overkill for the barely TB data we had at the time. I'm amazed its so widely used given that few companies genuinely have Big Data problems.  Databricks has Spark at its heart.

Learn for the job you want and to keep the job you have.",4
1qm3nnr,5,"Do a python project, load into a non sql server db, use airflow. You‚Äôre good to go in the skills department",3
1qm3nnr,6,High level book data engineering book with no coding subjects might be helpful,1
1qm3nnr,7,"Doing an end to end real world project using AWS, Snowflake, DBT will be very valuable. This will open up a lot of opportunities for you. Also learn Architecture skills, that will help you land a more senior position and get you a overall better compensation.",1
1qlv4el,1,"I've worked a lot with Google Cloud. It sounds like you doing a lot of orchestration across systems. The events may be asynchronous--a potential problem could be that pubsub does not receive or finish processing the request.


You could use Google Cloud Run to deploy framework or web server (FastAPI, NodeJS) that can handle asynch requests OR debug the components of your project to validate the inputs, outputs, and processing times at each step.",1
1qlv3bm,1,Why not just nod then do it the proper way? Do the C-suites regularly look at the pipelines?,5
1qlv3bm,2,"I deal with a lot of different stakeholders from different areas of the business so it's a pretty common occurrence for me.

My usual process is to get them to explain the current process (if one exists) and what they are trying to solve.

If they offer a solution then I'll listen to it to gain domain knowledge I'm missing if nothing else and ask questions to drill down into how they came to that solution as it normally reveals quite a bit.

If I have a solution formulating in mind then I'll explain what it is as simply as I can while getting their opinion, this usually leads to them asking Qs and revealing potential edge cases etc I need to solve for.

Bottom line is... my process is to listen, question and collaborate. It's not about the solution they propose, but why they are proposing it and my job is to design a simpler version of it that ticks all the boxes.

I'm lucky in that my manager will fully support the fact that we won't simply take a list of specs and run with it unless we do discovery first to understand the problem statement.

I should probably also add that I don't just set up the pipelines, I do the full end to end using those pipelines and that's where the 'proposed solutions' usually sit.",3
1qlv3bm,3,"So I think this is a place where you can work on your communication skills and managing up. And understand that these stakeholders are trying to help and to be more efficient. 

So I would challenge you to dig in and try to understand the question behind the question.",2
1qlv3bm,4,"I usually respond by turning their proposal into a quick sketch or lightweight proof-of-concept and using that as the discussion anchor. It shifts the conversation from debating documents to reacting to something concrete. 

Also, explicitly calling out complexity costs (time, risk, maintenance) in business terms tends to reset expectations fast.",1
1qlv3bm,5,"These are common problems in many orgs, especially for SWE projects. Have a read about software business analysis and the techniques and processes used to manage these sorts of situiations. Ideally you could formalise something along this track (with the right weight for the typical project size) to be able to help embed this so you don't fight the same battles each time.

On the AI stuff, I'm also having a problem with managers starting to do this. It is obvious because the content is very vauge, verbose and when I ask for clarification on what specific points actually mean, they can't provide it. It is getting to be a serious issue now. A lot of managers in technical areas are also starting to use it to cover for their lack of technical knowledge (and failing).",1
1qltru0,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qltru0,2,"I left DE and ML for critical care nursing after trying 3 different companies. Don‚Äôt regret it one bit, I feel like my work is meaningful which is what matters to me. Everyone is different but what I‚Äôll say is life is too short to not be fulfilled in your work",7
1qltru0,3,"1) DE: It's an evolving area but three base skills are critical for anyone to have sustainable DE career and they're, sql, data modeling and distributed storage and compute. Tools are temporary as they're always evolving.¬†
Challenging part in DE is that many companies push pipelines too quick into production without proper data model built first without establishing proper processes, documentation, standards and conventions up front.¬†
There's a saying, ""pay me now or pay me later"", later will cost twice to the company.¬†

2) Masters: I'd suggest to do MS in applied stats which will create greater opportunities and sustainable career. Penn State Uni has one of the best program in the states. I heard their GIS program is also top notch.¬†
Doing generic IS/IM degrees won't add any value in the future.¬†",14
1qltru0,4,Yes maintenance is not fun. Maybe you can try the consulting route and build stuff for clients? Maybe having a deadline will light some fire under your butt.,5
1qltru0,5,"Changing industry might help your want for purposeful work. I do data engineering for healthcare tech. Sure, I‚Äôm not directly involved in care, but I can at least tell myself that in some way, the work I do is contributing to better healthcare outcomes for people.",3
1qltru0,6,"Yes, a lot of people feel this way (and in all industries)

No - it won‚Äôt ‚Äúalways be this way‚Äù. Depends on the company, team, manager, vision of the company, size (maybe a start will excite you more), growth potential etc. most employee quit when they don‚Äôt feel like there is progress, not just money. 

Free courses - we‚Äôve talked to over 150k people at data engineer academy. Take my word for it. If you‚Äôre ‚Äúbored‚Äù in your day to day job. USE THIS TIME TO LEARN SOMETHING

Doesn‚Äôt matter if free or not.
Doesn‚Äôt matter if AI or cloud
Doesn‚Äôt matter if YouTube or X

But use this time wisely.

When you get busy again‚Ä¶you‚Äôll wish you had more time on your hands.

I can‚Äôt tell you how many times people are conflicted when they come to us and want to learn something new but they‚Äôve just been laid off and all of a sudden they‚Äôre too stressed to focus.

Use this time wisely my friend. You‚Äôll never regret it",4
1qltru0,7,"Especially in large organizations you either support primary revenue generation, you do r&d, you support people, or you support and maintain the structures everyone else relies on.

Alot of jobs only matter at certain times of year or certain phases or points in a larger process. Some jobs matter every day like custodial/janitorial work.

You get your moment in the sun and you just keep paying the bills the rest of the year. Or you matter everyday and youre job becomes thankless... or your in sales",2
1qltru0,8,"Okay, so probably I have been very lucky, but judging from my limited experience, it doesn't have to be this way. Your problems seem more with the work environment (company, team) than the field. 
For comparison, I have worked for engineering companies, an energy company, a central bank and a grid operator in various data roles. In all of these I had nice colleagues, found challenges in my work, had a feeling I contributed and could develop my skillset.
Now, the hard part is finding the right place for you at this moment. In my experience, it's hard to know beforehand, so the only way is to try different ones (at least apply and interview) until you find one that sticks. A bit like finding a partner.",2
1qltru0,9,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qltru0,10,"Your feelings are totally valid, and also desirable to certain employers - there are many roles and places to work that can satisfy the desire you hold - whether that be a place that values their employees more dearly (and shares the profits more) or an entity whose work is making a true difference to the world. Your post is a great example of a good answer when asked ‚Äúwhy do you want to work here?‚Äù, and can put you above candidates that have better on-paper experience by a company that values the things you seek.",1
1qlt6fk,1,design your own parser. (feels like the 90's again),4
1qlt6fk,2,"I support the intent.  When creating JSON specs, I related to underlying DB tables.  That was the best way to put in DB structure and constraints to JSON definition, even if unenforced.  But then developers checked accordingly.",2
1qlt6fk,3,"As long as its not proprietary information, I'd probably just paste it into gpt with instructions to extract into whatever format you want.",2
1qlt6fk,4,"Honestly, this is the holy grail for a lot of us in data security. Static analysis and schema inference from code is probably the way to go for getting ahead of what's \*actually\* happening, not just playing catch-up. Sounds like you're building a DSPM-like capability, just from the code side.",1
1qlsox2,1,Pretty much every company that works globally adjusts your pay to the costs of living for your country+state of residence.,20
1qlsox2,2,"absolutely not. if you find a job, it's either modern slavery or you got 1 in a million lucky. if companies in G7 hire in Asia for remote stuff it's usually off-shoring and they want to pay as little as possible.",13
1qlsox2,3,"I can do it, if I want to that is. One of my colleagues is currently doing it.


It's super uncommon though, it's my second role where it's been possible.


I won't do it until my kids out of school, even then I'll probably just do it for a few months a year.",2
1qlsox2,4,"It's possible, but difficult. There are a few prerequisites:

\- You should be in Europe first. Finding opportunities remotely from Asia is very difficult. You can leave once the customer network is established.

\- Your consulting entity and banking should be in Europe or US. There's a lot of hassles paying independent consultants offshore. At minimum, you have to work with subcontracting companies that can handle international payment well. However, these companies may squeeze more profit out of you if they know you are working from a third world country.

\- You have to gain enough trust so people would jump through hurdles to allow you to have access to their data, setting up VPN, etc. from abroad. This means 2 years of experience is no where near enough, unless you have somehow exceptionally proven your expertise through public projects.

Without these, you are left with cheap projects on Upwork.",2
1qls0kd,1,"Based on your readme and ingestion file, it‚Äôs LLM-generated. While I‚Äôm not completely opposed to that, as a junior, you should‚Äôve done this entirely by yourself. You need to prove you understand the concepts, not that you can write prompts. 

Beyond that, you‚Äôre missing a ton of code a modern engineer would include. PostgreSQL via SQLAlchemy supports batch uploads, your models aren‚Äôt type-safe for the database, if you really wanted to model an ingestion flow like this you would include database versioning like Alembic, you use incremented IDs instead of something like UUIDs which are more appropriate for a unique ID field, you use date instead of datetime, you don‚Äôt have record tracking like created_at or updated_at, and most of your sub-directories are empty with zero tests.",35
1qls0kd,2,"Although a demo, remove references to usernames and passwords in your repo.


It doesn't really do much right?


Not sure what this proves¬†",10
1qls0kd,3,you mean roast your Jr data eng‚Äôs Claude code skills? Nothing wrong with leveraging but doesn‚Äôt feel cohesive.,4
1qls0kd,4,Okay... this repo doesn't do anything except show you know how to prompt AI.,3
1qls0kd,5,"if i was looking at this ahead of an interview with you, what would you like me to take from this repo?

  
I see smells on various files that they were AI generated. I use claude code at work so I'm not faulting you but i would like to understand what you want someone to take from this?

  
You've got a local setup doing a load and transform of mock data.

This is a ""it works on my computer"" example.",1
1qlqhic,1,"200k isn‚Äôt a ton for ADF. What is the problem with the current approach? As a Data Architect, if there isn‚Äôt a near real time SLA, I don‚Äôt see why your pattern isn‚Äôt acceptable. Given you do it monthly, if the overall job takes 20-30 minutes, that‚Äôs still not a problem worth over engineering.",8
1qlqhic,2,"200.000 records is so little, unless it‚Äôs hundreds of columns you wouldn‚Äôt even need to batch it in an Azure Function.",5
1qlqhic,3,Async requests to MS Graph api and bulk inserts to db. Or just do sync calls with pagination,1
1qlqhic,4,So as I will be passing an array of guids how can I update records in dataverse - is that possible? As I think the copy activity requires a target data source which is an array of guids which need to be updated in dataverse..,1
1qlqhic,5,"So my issue is the files have copied.
I then do some checks and validation which creates a list of Guids that can be updated in dataverse.

I then need to update (only) the records of the guids I have.

Thanks in advance",1
1qlqhic,6,Where is SQL sitting? On-premises or in the Cloud?,0
1qlosqx,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qlosqx,2,There's one on datacamp. But that's more of a basics of etl pipelines using python.,1
1qlosqx,3,"well, first time I needed this I just went to YouTube for python etl test videos because courses were way too long for what I wanted. later I found DataFlint, it has stuff for schema and null checks and is simple to follow. you can also try Coursera if you want more structure, but if you want to see real testing fast, try something that lets you do it step by step.",1
1qllhix,1,Did you know that FM has SQL commands? They are fast and don't require relationships between tables to implement them.,2
1qllhix,2,I read it as filmmaker to Postgres¬†,0
1qlkijq,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qlkijq,2,"just speaking about the az104 and 305: they are quite similar, and actually I found 305 easier. But I see this data streak in your list. This is somewhat different from the two previous exams, and not 100 sure if you would need the architecture exam if you want to go the data route.

Good luck with your studies",3
1qli8ca,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qli8ca,2,"Why are there so many downvotes for comments? Python for data engineering is pretty straight forward, just learn to write a for loop.",20
1qli8ca,3,"Idk if sqoop and Hadoop are all that useful at this point. Could just be my lack of use in that area but I don‚Äôt remember seeing a lot of these in the modern tech stacks when applying for jobs over the years and researching what skills are best to have. 

IMO whenever you‚Äôre job searching you really need to have your resume(s) pointed towards what you want to work with. Most companies have only a few tools for data engineering, orchestration layer and logic layer. Airflow and databricks for example. Pick a cloud provider, orchestration tool, data lakehouse/warehouse platform and start doing little projects. Like airflow orchestrates databricks notebook that pulls a dataset from azure datalake storage and then run a databricks notebook to convert the file to a delta table. Or durable function pulls API data and writes to bronze layer of databricks. 

You can pick whatever tech you decide I just mentioned those because it‚Äôs the route I decided to go down but I also incorporated snowflake just for a more overarching reach. 

Python can be learned along the way but it seems a little aimless to just sit down and ‚Äúlearn Python‚Äù for something that is as specific as data.engineering",4
1qli8ca,4,"Find practical projects that cover the end-to-end data engineering lifecycle: [data] ingestion, review, cleaning, validation, transformation, loading, storage, data lakes/warehouses/lakehouses, etc.",8
1qli8ca,5,Check out Arjan Codes YouTube videos. I‚Äôve found the topics he covers very useful in my work,1
1qli8ca,6,Just learn to write a real lambda project a you should be all good,1
1qli8ca,7,"I also want , what are the necessary python topics are needed for data engineering and some of the project ideas based on that",-14
1qli8ca,8,RemindMe! In 3 days,-22
1qli8ca,9,Remind me in a week,-22
1qli8ca,10,RemindMe! in 2 days,-24
1qlf7l8,1,"It‚Äôs not a very entry level friendly career path honestly. You need to have deep SQL and Python knowledge to be a good data engineer, and that usually only comes with experience.",1
1qlf7l8,2,Remind Me! in 2 days,1
1qleiv1,1,"The fastest way to network is honestly by doing stuff in public. Stick around here, comment on posts, and share small writeups of what you‚Äôre building. Outside Reddit try linkedIn DE posts, Slack and Discord groups Most DE networking happens around projects and conversations, not formal events people notice when you consistently show up you know.",3
1qld1id,1,"I‚Äôll say it. IMHO, your job/company is not worth burning yourself out over. Every role I‚Äôve been in over the past 15 years has had at least 1 systemic issue that I was never going to be able to fix no matter how hard I tried/lobbied/played the game. Try to focus on things that are within your realm of control if possible.¬†",18
1qld1id,2,"This is the kind of thing you'd talk to your boss about.¬†


Say it's an area of interest and see if you can have some allocation to prototype some ideas.


Emphasize your interest in enhancements not that you think current stuff stinks.


No might be the answer. And it might be a sensible answer.


I obviously don't know enough about the problem to say. But just philosophically, uniform quality throughout a system sounds like a better idea than it is. Things are not uniformly important. So if you have uniform quality it means finite resources got diverted from things that matter more to things that matter less.",6
1qld1id,3,"I don't think you need me to convince you. Based on a) b) c), you obviously should not do anything about it. Don't even complain to anyone including your supervisor.

However, if you want to progress fast in your career, you need to be comfortable with taking ownership, upset people, and sometimes be completely wrong.

Be careful with the thought - ""I know it could make a pretty big difference"", it could be the opposite because fresh out of school your intuition has not been refined yet. I recommend actively learning from the analysts first, like a short rotation, before telling them how they could do their job better.",2
1qld1id,4,Do it. Be the change you want to see in the world,3
1qld1id,5,I would definitely say take that leap and communicate what problem you identified and the solution you think of  but i would definitely advice you to communicate this to your manager during 1:1  first instead of communicating to entire team and go on from there,1
1qld1id,6,"What has the business defined as the curated/gold standard for the data?  
The DAs might only need to report on specific outcomes not what you're finding.  
If the business doesn't care, why work extra? It becomes a passion project for no business defined reason.  
  
You want more scope and stimulus move to a role that has data engineering along with curating the raw data for the analysts so DE + AE > DA  
  
Be mindful though\~ DAs are the bridge between the business and interpreting the results, so they won't necessarily be as mentally focused on curating data as opposed to reporting on it.  
I Understand the energy, it's best used as mentioned for personal development or moving into a higher role elsewhere if not possible at your current.",1
1qld1id,7,"Float the idea with someone more senior you know who knows the organisation better. You'll need to have a pretty well refined idea of what benefits these improvements will make, ideally aligned to the current business objectives.

You often also don't have to go all. Support for a small pilot exercise to investigate more and see if this may be worth pursuing would be a good outcome at this point.",1
1ql5y9s,1,My favorite one is the PASS Summit in the Fall in Seattle.,2
1ql5y9s,2,"There is a ""small data"" one every year that might cater more to on prem.¬† Just know about it, never been so can't give any details.",1
1ql5s1b,1,"Start using functions that you import. Keep it simple at first. I‚Äôd start with a config.py, constants.py functions.py. If it‚Äôs a small company I‚Äôd chat with any other developers about a shared library in your vcs",35
1ql5s1b,2,"Have you tried marimo?
Its a jupyter alternative and its like the best of both worlds, you can run it as notebook while the code gets stored in pure Python (better for git as looking at html from jupyter notebooks isn't very good)
https://marimo.io/features/vs-jupyter-alternative",9
1ql5s1b,3,I'd say there's nothing particularly wrong with what you're doing since the purpose is to analyse and manipulate data. If you're trying to write software then yes it's not great because you can't really write comprehensively testable software without functions. You don't generally have things to plot and tabulate with software so you just need a debugger to inspect variables. This tool chain doesn't work that well for data science.,22
1ql5s1b,4,"Modularize your code as you go, at the end the notebook SHOULD JUST be orchestrating functions that do all the work. Then offload that to airflow or dagster, write outputs to s3 and maybe a db even

Keep the notebook in line with the dag for debugging and testing. Log, log alot, and export backups, start using mlflow for experiment tracking and have it live in there? If you're training anything that is.

Dozens of other options but this is such an immediate and necessary win for anything else to really happen.

ETA: learn about hot reloading functions in jupyter, so when you change a print statement or add a func and save the file, you don't have to reload jupyter hours rerun the import line, it's like a 2 line command at the top of the notebook to make this happen.",10
1ql5s1b,5,"Prototype in Jupyter; deploy with Python files. That was my rule at my previous role anyway. 

Now I‚Äôm using fabric and don‚Äôt have a choice.",4
1ql5s1b,6,"This is working for you because your code doesn't need to scale right now. You can get around your concerns around ""visibility within functions"" by having logging and debug logic that's parametrized. You could do simple things like save intermediate dataframes to file when `debug==True` and inspect those in a notebook.",4
1ql5s1b,7,"If you are not writing your code using functions, how are you unit testing it? If you are not unit testing it then you are not writing production quality code. 

If I heard the phrase ""I don't like functions"" at interview it would make me think data analyst not engineer. Not to be derogatory as both are valuable but different skills. 

You can absolutely write modular code without compromising the positives about the jupyter notebook experience. If you write your code  in a modular way it will be incredibly easy to migrate it anywhere.

Some things are understood to be industry norms for a reason.",2
1ql5s1b,8,"> I personally dislike functions as well unless neccesary and there's reuse required. 

I have bad news for you if you need production-level code....

On a more serious note, if you use an IDE like VScode, you can incrementally build code logic with interactive code execution like in a notebook and also become familiar with how to use the debugger for your programming language. Once you have an initial chunk of code that does what you want, parameterise it and stick it in a function. Then write some tests for it, ideally. A parameterised notebook is not all that different from a very lengthy function. You just want to separate the code in your notebook into functions so that each function is doing a single thing and returning/producing a single thing.

It will be slower at first because you're rewiring your brain to use a new process and interface, but you'll get faster and the benefits can be great if you're doing anything that's a deliverable and you want some quality assurance. And this is just the tip of the iceberg. Once you get to this level there is a whole world of code design best practices you can venture into to make your job easier while giving you more confidence in the correctness of your code and its artifacts.",2
1ql5s1b,9,"I don‚Äôt get how nobody has suggested this but learn how to use .py files with the REPL. In vscode for example, you can work in a .py and then highlight text and send to the repl so it basically mimics exactly a notebook but you have the benefit of being in a .py file.",2
1ql5s1b,10,Nothing wrong using Jupyter notebooks as a source of debugging before turning it into a script. One thing I would say though is that you have to start LOVING functions if you want to automate processes as your project grows. You can work with functions in your notebooks and convert the notebooks into .py files.,2
1ql54ba,1,"Versioning
Git - strive for everything as code and version it

Extract+Load
Investigate DLT whether it can help you in data ingestion.


Transform
Dbt is actually super useful once your project grows larger. Apart of many other things the most useful thing is that it builds lineage out of the box.

Orchestration
We use Dagster instead of Airflow, it is better fit for data world and  has very good synergy with dbt ( each dbt model is a separatate dagster asset). 1 big orchestration tree instead of many separate as in Airflow.

CICD
Github actions

Python
Can be used in Extract Load and even Transform phase. 

Reporting
Prefer those with good API and ""report as a code"" We use Metabase.

Data modelling
Not a tool but very difficult but useful skill to grasp. With  advent of AI it is very necessary again.",37
1ql54ba,2,Why don't you start with a problem you are facing instead of a tool you want to implement?,25
1ql54ba,3,Something to build internal tools and apps easily. Like Retool etc.,3
1ql54ba,4,Look into automating a data dictionary or data catalog.  Documentation isn‚Äôt sexy but it‚Äôs worth the investment in the long run.,3
1ql54ba,5,"This depends on your platform and team size, 

if you have 

- a lot of tables in your warehouse, 
- a lot of data people creates garbage tables
- DE team lost control in dwh 

You must have dbt and enforce take permissions the service account from unrelevant data peoples, meanwhile you neee to have ci-cd pipelines and table dependency management for data linage, data governance it will give back dwh control to data engineering team. 

It just about one example for dbt.",2
1ql54ba,6,"Seems your manager is idiot. You should increase architectural complexity by adding new tools only if it's really required. Simplicity is the key to success.  
  
But if you are forced to, just pick something that will make your resume more valuable.",2
1ql54ba,7,"Why do you feel dbt won't add value?¬† I have seen small and large orgs use it successfully  
My recommendation is:  
dlt for data ingestion  
dbt for transformation, data quality, and docs  
airflow for orchestration (this can be hard to manage, so consider a managed service like MWAA, Datacoves, Astronomer, etc)  
The key is also to think about how all the parts connect using git, ci/cd etc.",2
1ql54ba,8,"It's lower case dbt. If you're using airflow and sql then it's probably useful. The biggest thing I like about it is that it generates the documentation and lineage very easily. Yes airflow makes a dag but I've never liked the styling as much. Anyways, dbt is a great tool to know for best practices but i suppose it depends what you're doing with the sql and only you can answer that part",3
1ql54ba,9,"Data quality checking tools like great expectations, soda; Metadata/lineage like atlan; monitoring (ex. Grafana)",1
1ql54ba,10,What are you using (or plan to use) for BI?,1
1ql3n7t,1,"The job market is bad for junior DE, it‚Äôs impossible for cs grads who can‚Äôt problem solve at a basic level, for example using the subreddit search or reading the subreddit rules.",11
1ql3n7t,2,"Only the largest companies tend to hire DEs at the junior level, so there just aren't many positions to begin with. You can still get into them, but you're going to be competing against a lot of individuals for less jobs. If you can get an SWE role then trying to land one with a focus more on infrastructure and data will probably allow you to easily pivot to a more data centric role after a year if you want to",5
1ql3n7t,3,"Data eng isn‚Äôt really a ‚Äújunior‚Äù level role. Regardless, stop overthinking and focus on learning to program.",2
1ql3n7t,4,Imo you need some exposure to regular software development to be a data engineer. Any data engineer out of college role would be highly suspect as I'd just assume you're working with low or no code solutions / in a large company that has highly specialized tools for DE. These roles will be replaced by LLMs. I've interviewed several career DEs from FAANG and they either can't code their way out of a paper bag or had to learn software engineering principles anyways to make something hyper performant. It always seemed to be one or the other.,1
1ql3n7t,5,"The market is terrible. 

I highly suggest starting as a data scientist if you can manage the switch.",-4
1ql0r1w,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1ql0r1w,2,Business and soft skills,1
1qkxxls,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qkxr5q,1,"Its kind of frustrating as a senior data engineer that people are getting to technical interviews and I can't even get passed the AI resume filters.

Recruiting needs to change in light of AI but I don't have any idea how.",207
1qkxr5q,2,In person interviews are making a come back,24
1qkxr5q,3,"Stop looking for the perfect candidate on paper, everyone that uses AI on screening will tailor a perfect resume based on the job description. You will find more ""real"" people when you look for almost perfect CV's",104
1qkxr5q,4,"We‚Äôve experienced the same during interviews‚Ä¶ Candidates giving unbelievably amazing responses on really niche (business specific) subjects.

We‚Äôre now doing on site interviews.",25
1qkxr5q,5,"I haven't had to hire since widespread AI use, but my main question is what do these people have on their resume for working experience? Fake jobs, unrelated stuff, or nothing?",9
1qkxr5q,6,Then stop using AI to screen candidates. You‚Äôre filtering out good people before they even get to the technical interview. And if you really don‚Äôt want people using AI do the interview in person,6
1qkxr5q,7,"We switched to white board style interviews. Walk me through how you would solve this problem, what similar projects have you worked on in the past. It was a better experience than prior interviews we did which allowed too much AI usage over a teams call.

I have also heard of people mandating cameras be on telling a candidate to close their eyes before asking a question, lol. But that‚Äôs not very professional",5
1qkxr5q,8,"We had the same problem, we switched all technical interviews to be on prem. We get HR to tell them that from the off, lots of people drop out at the point they are told",17
1qkxr5q,9,"Had very similar experiences. Before AI was being used widely it was ghost interviewers and people whispering answers or at least typing answers in a screen the interviewee could read. I've had catfishers interview then a different person onboards. It happens, we just need to be better at detecting and stopping it.  There will always be people who lie their way into jobs though. 

Just last week I interviewed a guy who seemed to be using AI to craft his answers. Although some answers were obviously not scripted, and he seemed to know what he was talking about, he still seemed like he was using an AI interview tool. After talking to some of the recruiters familiar with his region of the world it was apparent that this was VERY common and candidates felt they had to do it to stay competitive. 

We had a follow-up interview where we told him that we all use AI tools in out jobs and that we can't fault anyone for using them, but we wanted to hear his answers unassisted. We made it clear that it was OK to say ""I don't know"" or ""I don't understand the question"". The interview went much better. There was some language barrier and he didn't understand all the questions as asked, but when clarified he was fine and gave good answers. When he didn't know something he said so. It turns out his resume was real, he did know what he was talking about. 

I think going forward we will address the use of AI, let them know it's ok for some aspects of their job and even for helping them interview. We also need to get good at asking the right questions. Not to ""trick"" the AI, or to somehow catch them in a lie, but to help us understand if they have the basic knowledge we're looking for.",8
1qkxr5q,10,"Ask them something that's technically possible that no sr would entertain and pre-fetch what an AI would propose for it. 

""Currently we are using X stack to handle this process, but we are deadset on migrating Y part of it to pure Binary, don't explain why we wouldn't do it, just explain how we should go about doing it.""

Any good dev will immediately pushback or say they won't want to do that as part of their work and those using AI blindly will give you a canned answer. 

I'm doing a Fabric integration task that's a very-non-Fabric work around and almost all LLMs start with strong disagreement and if you negate that they give you almost a verbatim response to source material they learned it from, regardless of service.",3
1qkxlp5,1,I am gonna be that guy - If you format your post I'll read it and try to come up with a suggestion.  Break that bad boy up!,3
1qkx5md,1,"Autoloader with schema evolution?

You can first parse the file, store the schema info and use it to guide autoloader.

But tbh, this is hellish.",2
1qkx5md,2,"If your ETL uses a defined list of columns, then you don't have to worry about the order or new columns.

1. load data into `raw_table` with whatever cols/data is in the csv
2. INSERT operation from `raw_table` to `my_table` with your list of cols.

If the user tries to rename cols then strongly, politely tell them not to do that!",1
1qkw05o,1,"Asking those who have years in DE is perhaps pointless. The landscape was much different then, say 5-10 years back when DE was considered a hyped position.",16
1qkw05o,2,"I feel, you usually start as data analyst or something like that, and then transition to data engineering. It is not that hard field, j ust knowledge of a lot of different technologies simultaneously is needed. Though i guess, trainee/intern may get hired even without all that shit. 

Idk, why are you getting so condescending answers.",10
1qkw05o,3,How many years of experience in tech do you have?,5
1qkw05o,4,"I‚Äôve been a DE for 4 years now. I started as full stack SWE for 1.5 years ‚Äî> DE. I have 2 other DEs on my team - one came from full stack SWE background, the other had a bi analyst background. So, don‚Äôt feel down on yourself for getting a DE job in particular - they‚Äôre very few and far between.     
    
At my company there are 30+ SWE, 20+ analyst/data scientists, and 5 DEs (self included). Simply put, most companies need way less DEs vs analysts and SWEs. And speaking frankly, I doubt we‚Äôd ever hire a junior DE at my company. Since I‚Äôve been at this company, we‚Äôve hired many junior analysts and SWEs - never any junior DEs.    
  
My advice, get your foot in the door at any dev or analyst job, work there 1-2 years and work on as many ETL tasks as possible. If you find you still want to be a DE, start interviewing with experience under your belt.",3
1qkw05o,5,"You need to ask anyone who broke into it since 2023. Different job market and landscape since 2023. Advice from years prior are useless because it was an easier entry before 2023. From a trend I noticed back when I applied for roles, I‚Äôd say you need to be one level above where you‚Äôre looking to get any job. I had skills of a Data Analyst with some DE and DS skills but couldn‚Äôt land any job. Eventually through networking and showcasing my projects I now work as Tech Finance Analyst where I am overqualified. But the over qualification matters because I do more work than I should be doing and the over qualification is what‚Äôs needed to balance out heavier workload",3
1qkw05o,6,"I didn‚Äôt need to convince my current company - they reached out to me for this current job, which is my first DE job. I have a lot of data analysis, DBA, metadata and cloud experience so moving into a DE position is a natural progression in my career.",2
1qkw05o,7,"first did 2 internships in data analytics where i dabbled in data processing as well. then landed an analytics engineer internship where i did more typical de work, and put de on my resume LOL",2
1qkw05o,8,Which country,1
1qkw05o,9,How do you convince? Convince yourself first then confidently interview and they‚Äôll be asking you to join. Either you know it or you don‚Äôt!,1
1qkw05o,10,master SQL,1
1qkujmz,1,"the consultant is half right. airflow is complex and will eat time if ur team isnt already familiar with it. but lambda for orchestration is a weird rec, its fine for simple triggers but once u have dependencies between jobs, retries, backfills, monitoring, etc. ur basically rebuilding an orchestrator from scratch.

if airflow feels too heavy, look at dagster or prefect, similar concepts but less operational overhead. or if ur dbt transformations are the main thing being orchestrated, dbt cloud has built in scheduling that might be enough.

lambda makes sense for event-driven stuff, not for managing a dag of dbt models running on a schedule.",33
1qkujmz,2,"Lambda is an odd one here.

Airflow... It has an upfront cost, but I can say that it pays back in dividends, especially if you take a few weeks out to figure out dynamic dags with jinja templates and can make use of it.

To me, if you have 10..20...even 30 pipelines to manage, ok lambda could do, windows scheduler would also do. Hell, Simon clicking ""run"" would do for small services.

If you are like me and managing about 600 pipelines, its worth the effort.",14
1qkujmz,3,Lambda can only be up for max 15 mins right? Or am I missing something? I guess he was trying to say step function maybe.....,10
1qkujmz,4,"Depends on the complexity of your pipelines tbh.

If you have a reasonable number of dbt models you can just slap Astronomer Cosmos on top of dbt for Airflow and you get a generated DAG with a task for each model, giving you visibility, easy retry workflows, etc.
It's also quite customizable and runs without major problems even on a single node.

Ofc, there are other ways to do this, I'm not affiliated or anything.


What's the specific ""complexity"" this consultant was referring to?",7
1qkujmz,5,Probably meant step functions orchestrating lambdas that execute redshift stored procedures,8
1qkujmz,6,"I'd say you will need proper orchestration engine.

I'd recommend dagster over airflow, as it caters to data workflow more.

Using Lambda will be painful in the long run. I'd recommend finding a new consultant.",6
1qkujmz,7,"I've encountered this ""Airflow is too complex"" opinion a lot in NA.

But that is not my experience. And from what I've seen these opinions often lack substance. When you dig a little deeper, it's usually just misinformation and parroting stuff they've heard online.

Airflow works great IMO. It's effective, flexible and easy to use.",8
1qkujmz,8,"With that stack consider MWAA, start with a small instance and start writing DAGs.",4
1qkujmz,9,Silly,3
1qkujmz,10,This consultant is an absolute idiot and a huge red flag,3
1qkr43c,1,"What exactly was the on-prem? Some single node sql databases or maybe complex, high concurrent distributed stuff?

Fundamentals are the same. Medallion architecture is nothing more than traditional staging to dim/facts tables. Networking remains more or less the same in the cloud as in on-prem. If consultants claim they have some super new architecture this usually is BS - I haven‚Äôt seen anything entirely new in data world in recent years. 

If you understand SQL and database engines internals you will easily pick up Spark.

Question is - do you have experience with Python and knowledge of distributed computing? If so, then in few weeks you‚Äôll understand how it all works in cloud.",6
1qkqu2z,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qkqu2z,2,Most Data Engineering problems within the business are not that simple lol,75
1qkqu2z,3,"Great! You have learned a tool which is at the forefront of data engineering tools. 

Now try to convert a legacy system with no documentation and limited comments over to it. Oh and by the way you can't use AI on the legacy system because it's client confidential and your company doesn't have an enterprise level license for any good AI tools. 

Also the stakeholders involved don't even understand why you would want to transition it over so now you're in an hour long meeting with a presentation attempting to explain to all involved why this is a good idea in the first place.",163
1qkqu2z,4,"LLMs are not going to replace data engineers.

Learning pandas and numpy was never the point. It's good that LLMs significantly reduced the time spent on learning libraries.

LLMs now give you time to think about how to structure your solution. It's not going to be able to solve complex problems, at least not yet.

In the current working environment, you'll see extreme gaps in productivity. People who are strong at fundamentals and make LLMs their slaves would see huge burst in output and quality, while people whose main competitive advantage was knowing libraries are becoming redundant.",50
1qkqu2z,5,Man. Only if business could tell you what the columns mean and why are the values null. Lol,31
1qkqu2z,6,"Working in production is totally different to these sort of mini projects / tasks where everything I clean, the requirements are clear and it‚Äôs made to teach you / run easily",25
1qkqu2z,7,"Well, the answer is in your question. You had: 

* The development environment set up perfectly
* Complete requirements with concrete acceptance criteria
* Easy and straight forward tasks
* An AI setup integrated with your production system
* No stakeholders to report to

So yes if you have all that, then the job is easy.",23
1qkqu2z,8,"Half tempted to lock this because we get a speculation post at least once per month.  Well, feels like once per month anyway.

>All you have to do is ask AI to do it. Scary.

My favourite opinion on this is with AI, you have a lot of people saying they can do anything now.  It's like the equivalent of guns not being available to a general population becoming available and suddenly everybody starts saying they're a soldier, hunter, marksman etc.",10
1qkqu2z,9,"You definitely know nothing about DE if you are thinking about things like ‚Äûhow soon we will reach the point‚Ä¶‚Äù. 

Requirements are never that simple, AI code completion or even whole script writing isn‚Äôt as good as you think. You cannot put into AI output from client API so you need to even know what you want to achieve, you need to take this i.e. json to anonymize it, you need to know what you want to get from this LLM. It‚Äôs like endless list of things you need to think about in that field that don‚Äôt include heavy coding. 

Also data governance, security‚Ä¶",6
1qkqu2z,10,"I think the field is going to converge more and more on machine learning engineering. I think building pipelines is largely going to be automated away, and not by AI. The major warehouses are shipping with CDC tools to replicate data from your Postgres/MySQL/etc so that you don't have to build that anymore. And more and more SaaS vendors will export data directly to your warehouse, so that you don't really have to build those either. AI will be able to do a lot in terms of glueing that together. 

Where I think data engineers will spend much more of their time in the future is on something much more valuable, actually building data products (internal and external) that derive value from the data. Every org I've worked at wants to be data driven, but the people in the business domains have really weak ""data reasoning skills"". I don't think AI fixes that because it won't help you if you don't know the right questions to ask. So my bet is that you'll have data engineers/scientists/analysts converging more and more into a role where they need to bridge that gap to make all this data we've collected valuable.",3
1qkpue9,1,Did you try to talk about it?,0
1qkpue9,2,"I think leaders think of these two roles as interchangeable unfortunately. Depending on what your company does; I would document how may hours the Eng stuff takes, and how much the analysis stuff takes. Then talk with your leaders to make a roadmap to either get you fully into one side, or to come up with a process to make a handoff step from one to the other.

You have to come solution minded to your leaders with a way to get you more into what you want to do but also solve their problems at the same time, otherwise they won‚Äôt care",1
1qkpue9,3,"For now make an advantage of being one man army. Try to prioritize work you prefer - e.g. you got some analysts task, focus on the primarily. Having DE exp will help for sure in future as analyst also. I was in a similar situation - being an only guy with data knowledge as a junior. For me it was great experience, since I was not blocked by anything and could experiment and learn hands-on. If you mess up sth, that‚Äôs their fault they didn‚Äôt hire someone more experienced to help you. Read a lot, use AI to check ideas and don‚Äôt be afraid of mistakes. You‚Äôll make progress in a year which guys in huge companies do in few years.",2
1qkpue9,4,"What kind of DE tasks are you doing generally? You make it seem like your python is a little on the weak side, so are you doing most of your work in SQL?",1
1qkovgd,1,In my experience they require a Facebook account. And you can‚Äôt create one just for work if you already have a personal one. They will lock the account. It sucks. Your org can add your work email to their business account but when you accept the invite you still log in with the personal account. You have to use the developer portal to create an Oauth app to get an access token. Also that token will expire every 60 days or so. The paid tools that allow SSO will not do this.,2
1qkony4,1,Check the query plan,3
1qkony4,2,"Dude, while I know this might be marketing, there are LLMs and opensource ones avaialble that do this for you, not sure this needs to be a Reddit post. 

Ha, is that the new version of ""This meeting could have been an email"", ""This post could have been an LLM prompt""?",0
1qkony4,3,"At first glance, is the subselect worth it? Indices will work for category but the subquery seems a bit unreasonable and could destroy the index advantage. Can you limit the data beint pulled?",0
1qklwjw,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qklwjw,2,"Driver, executors
Lazy evaluation 
Transformations, Actions
Stages
Narrow and wide transformations
Shuffles
DAG, data skew, partitioning

These are the topics that matter for Spark in interviews.",97
1qklwjw,3,"For me, it doesn‚Äôt really matter. I was on the interviewing side a couple of times. While personally, I usually prefer sparksql for structured data, if the candidate is capable of solving issues either way that‚Äôs what matters most. Probably it depends on the company‚Äôs standards later on, but not at the interviewing stage",16
1qklwjw,4,"More dataframe but both are valid, if you can't do one but can another then doesn't matter",7
1qklwjw,5,"Most interviewers care less about which one you type and more about whether you understand what Spark is doing under the hood. Being comfortable with the DataFrame API is usually expected since it is more flexible and composable, but you should also be able to read and reason about SparkSQL because a lot of real pipelines mix both. A good answer in interviews is often explaining how the two map to the same execution engine and when you would prefer one for readability or maintainability. If you can show that you understand query planning, shuffles, and performance tradeoffs, the syntax choice becomes secondary.",2
1qklwjw,6,"I guess it is important to highlight when to use what. 

I mean between dataframes and SQL are architectural differences and both shine in different usages.

While dataframes shine in their programmatic way, with chaining, validating, etc.

SQL shines in their parsing nature, with i.e. window functions, complex joins, CTEs...

It is important to stick mainly with one API and not mix too much!",3
1qklwjw,7,"Add catalyst optimizer and tungsten execution engine to it. After writing transformation logic and before calling actions like show or count, use df.explain(true). Practice reading logical and physical plans for your transform logic. It helps in interviews.",3
1qklwjw,8,"It doesn't matter, but you should be able to use both.",2
1qklwjw,9,"depends on the company and the interviewer. I got dinged negatively during an interview with a larger tech company that had people who ONLY worked with dataframe transforms even though sparksql evaluates pretty much the same in an interview context because they rarely worked with sql. you have to read the room/interviewer unfortunately.

personally I'd be cool with either but try to understand what the interviewers preference is, if any",1
1qklwjw,10,Sql is better to learn because it translates more to writing sql for actual dbs from a interview prep efficiency standpoint. The interviewer probably won‚Äôt care,1
1qkkug6,1,Paywall..,6
1qkikn4,1,"This one feels like marketing fishing, waiting for someone to bite and/or use it as a plant to then suggest their product as an alternative. All alternatives are use-case determined.",1
1qkerci,1,I'm not replicating upstream data models into a separate warehouse or lake house.  Life is too short to live through that pain.,24
1qkerci,2,"Do you have, or have you come across, bona fide evidence of how using database cdc -> messaging -> message ingest/db changes on the replica‚Äôs side is better than just a good old fashioned DB copy? I do first question the business requirement (I really can‚Äôt imagine a business case where there is a need of such low latency between the source and consumer where messaging is the only option). It all sounds fine in theory, but as you mention in the post, you end up doing a full db copy anyways lmao. Messaging Seems like overkill and a solution looking for a problem but maybe you work in a high risk industry like defense or gambling or something idk",9
1qkerci,3,"If you‚Äôre using clickhouse, they have clickpipes which is a good realtime Postgres cdc option.",6
1qkerci,4,Debezium plugin on Kafka connect to cluster to databricks Delta Live Tables,3
1qkerci,5,"I used stitch a couple of years ago for a ton of SQL Server databases to BigQuery and it was affordable and bulletproof. At the time it was $10k/yr - now $1250/m. That particular integration used CDC+Kafka+Debezium under the hood, which I had also had my team build out at a prior company for a production migration project and it was also bulletproof at 100 million rows a day. Not sure if it solves all of your problems, but worth a look if you have the budget.",5
1qkerci,6,"I have used quite a few of them... Real-time I feel like your have two enterprise grade options Kafka Connect and spark structured streaming.  And your choice ussually boils down to are we already running a spark cluster or a kafka cluster.

I have much more experience with Kafka Connect and sure it has it's pain points but it is the best in class solution for real-time data at scale.  Although i will add Red Panda is an increasing an option that I would keep on the table.

The problem with the managed solution is they become expensive and slow if you are working with any volume of data or any high update frequency.  

If you have small data or don't have real-time requirements the managed solution are all great. Currently we run Kafka connect and Open Source Airbyte. We are slowly moving away from Airbyte, but it works great for all of our small tables that need to be updated ever 15 minutes or less.",2
1qkerci,7,"Hard to recommend solutions without more context. A few questions first:

* How big are we talking? (GB? TB?)
* Update frequency?
* Do you actually need all tables from all 10-15 DBs?
* Any overlap/duplication across them?

If you genuinely need everything, you might want to look into **Iceberg Topics** (Confluent just released this). Basically streams your CDC directly into Iceberg tables that you can attach straight to your lakehouse landing zone. Gets you ACID, schema evolution, time travel, and hidden partitioning with essentially zero ETL. Could be worth exploring depending on your answers above.",2
1qkerci,8,"https://docs.cloud.google.com/dataflow/docs/guides/templates/provided/cloud-spanner-change-streams-to-bigquery

Only works with spanner and bigquery, but has been great.",1
1qkerci,9,DMS replication for PG -> redshift,1
1qkerci,10,"Depends on what you‚Äôre replicating to. ClickHouse, snowflake, and Databricks all have native options (some better than others‚Ä¶). 

If you‚Äôve had enough of managing Kafka yourself but you like the latency, my company (Streamkap) is a good option as are companies like Estuary, Artie.",1
1qkc7gg,1,I‚Äôm also in the middle of learning/teaching myself different data integration and ELT tools to position myself for a career change. Would be happy to connect and share what I‚Äôm doing.,5
1qkc7gg,2,"This was years ago, but I learned a bit about databases and coding while trying to automate shit for my job.¬† Also had access to our data warehouse as part of my work so kinda saw how it worked.¬† Then got lucky I guess and found a job running a ""reporting"" org that needed a data mart.¬† Hired a guy that built it and suddenly I was a data expert in my circles.¬†¬†


It's much more complex these days and you have to commit yourself to constant learning to be successful.¬† I miss that in finance and accounting stuff didn't change that much year to year, so when you left work you didn't have a dozen blogs to read and a few videos to watch to keep up.¬†¬†


To start out though just try to lean the coding part and what all the pieces are.¬† I recommend reading fundamentals of data engineering to get the concepts without all the online hype.¬† One huge advantage you have is that the business context should come easy to you with your background and a lot of pure CS people don't get that.¬† Then just try to build stuff to get experience.¬† There are lots of open source tools where you can learn to build pipelines and the infrastructure around it.",2
1qkc7gg,3,"Yes. Accounting -> Analyst-> BI analyst/engineer > Data Engineer 

Started learning Python (already knew SQL). Originally wanted to do data science. Went from multiple accounting roles at Small Businesses to an analyst at a large company. Moved into a Bi role at the same company but different department. Started a masters program during that time. Started learning Tableau. Went to a new company and started building out Tableau dashboards from scratch. Started using Python (and pandas) for actual work. Switched companies again for my first actual DE role.",2
1qkc7gg,4,"I worked as an accountant and started by trying to automate all my month end close tasks. I used Python to build some web automation scripts to download monthly reports from various platforms we used. I learned sql and used it to query our data warehouse to get immediate answers to business questions I had instead of having to wait for our BI team to get back to me. I used Pandas and excel-related Python libraries to automate most of the Excel work I was doing. 

I learned a ton through that process and then went on to take the Data Engineering specialization through Deeplearning.ai to learn some more about what data engineering entails and some of the tools I might expect to use in a data engineering role. 

After completing the course, I worked on a personal project that involved creating an ETL pipeline with some public financial data I was interested in and a dashboard. I used some of the tools I learned about in the course to complete the project. 

It took me a while to find a job, but eventually managed to find an entry-level role that‚Äôs a mix of data engineering and analytics engineering.",1
1qkc7gg,5,"been wanting to get out of accounting since the beginning. did lots of self learning. did a bunch of accounting projects that‚Äôs adjacent data engineering at every job i had. got bored during covid and got a degree in analytics. hardwork finally paid off and i did an internal transfer

i wish id gone to college for cs but went with my parents suggestion for accounting. i stopped thinking about career change since my pay and wlb has been pretty good. then this transfer opportunity came up",1
1qkc7gg,6,I feel like it would be very difficult in today's market. I have 10+ years experience but the job market is Soo tight right now you need to have x years experience in several different tools just to qualify,1
1qkc7gg,7,"Got a bachelor in accounting, made the transition in 2021, self study for about 1 yearish.. got my first job in 2022. Now 4 years later i'm still here and it was the best decision i made. Decided to make a master in software engineering to ""formalize"" the change of career. If i wouldnt have made the sacrifice my life would be really different. 

You can do it, im the proof that if you really want to it will happen!!",1
1qkc7gg,8,"lol, I have always thought of data engineers as the accountants of tech so I can back this move",1
1qkc7gg,9,"I did, but I had a BSc in Computer Science that I got before I started with accounting/finance. Spent 13 years in finance related positions before I became a DE, but during all those years I was always playing with the accounting systems / creating excel spreadsheets with VBA / designing systems for the team etc. I also was responsible for the ERP systems in the country, doing implementation/testing/upgrading stuff. 

I got the DE job because I could convince them that even though I didn't know the specifics, I could easily learn what I needed, and my background/knowledge would be a benefit for the company. Never learned anything specific to get the job.",1
1qkaaf3,1,Okay.,28
1qkaaf3,2,"Anybody who would trust AI generated analytics on top of structured data, not to even mention unstructured data, is a fool, and probably holds a senior position at my company",15
1qkaaf3,3,"Mmmhmm. And can he guarantee 100% accuracy and repeatability? Don't be me wrong, I'm not an AI hater, but these systems are not deterministic. Right now, I can stand in front of my CEO and confidently say that our pipelines will apply the same transformations and return the same data every time. If there is a fault, I can trace that back from a report all the way to the source, and I do this quite regularly. Can I do that with his model? Or is it going to be a black box and I can't see what's happening in there?

I'm not going to my CEO and telling him, ""Well, there's a problem with 4th quarter expenses. They're 15% higher than we expected, but I can only trace the issue back to our ingestion model. I can't tell if someone is scraping money off the side, or if HR made a salary mistake, or the warehouse ordered too many widgets.""

And no, I'm not using the model to get those answers either. ""Well, the model is telling us that the answer is x. I don't know how it got that answer. We just need to trust the model"". If I ever get to that point, I will quit and get a job wrangling geese, because there will be no need for my expertise. You just go and make multi-billion dollar decisions based on a stochastic model. Good luck to you.

If I don't have full visibility and reproducibility across the entire pipeline from source to report, then their product is useless.",7
1qkaaf3,4,"AI can't ""reason"" about anything",2
1qkaaf3,5,But what put it into the source? Hopefully AI can keep track of millions of relationships.,2
1qk7xr1,1,"Not hype. Learn ... vectors, embeddings, RAG, serving endpoints, langchain, how tokens work, etc ... In 5 years data engineers will be expected to support AI systems from a data perspective¬†",63
1qk7xr1,2,"If by AI you mean LLMs, it is basically web development",12
1qk7xr1,3,"AI is branding and there's little value there.¬†Neural networks, network analysis, SEO, Natural Language processing are real methods and they have value.¬†",35
1qk7xr1,4,I‚Äôve been apparently doing AI Engineering for years and I‚Äôm still unsure what it is. I venture a guess that it‚Äôs software engineering,9
1qk7xr1,5,Both. It is hype but it is also width studying.,3
1qk7xr1,6,"I think LLMs introduce a variety of technologies which will be necessary for Software Engineers and Data Engineers to understand to varying degrees to develop modern applications. Things like MCP server, RAG, various Agent SDKs, security issues around deployment of an LLM, etc.

That sort of stuff is probably worth learning. No idea what an AI engineer is.",3
1qk7xr1,7,"I do not think it is pure hype, but I also think the title gets ahead of the reality. What people call AI engineering today often looks like solid software and data engineering with models added on top. The fundamentals still matter a lot.

If you already work in data engineering, learning how models are trained, deployed, monitored, and fail in production is a real advantage. That part feels durable. Chasing every new framework probably is not.

Worth studying in depth if you enjoy building systems end to end. Not worth it if the goal is just to ride a buzzword. The market usually filters that out pretty fast.",2
1qk7xr1,8,"AI Engineering? Learn CUDA, Loss Functions, to build transformer models, to design training pipelines, to train models. Check out Temporal.io to build durable / retry pipelines.

Look for expected skills in job openings.

Copy this thread

https://news.ycombinator.com/item?id=46466074

and ask Gemini ""any AI engineering positions listed in this thread?""",1
1qk7xr1,9,"Anyone I know with a serious job tells me they use AI every single day. I‚Äôm still a student so I can‚Äôt tell you for sure. But it‚Äôs a tool like anything else, and it will be ubiquitous with not just any role in DE but any role related to tech, software, or programming.",1
1qk7xr1,10,"I think AI Engineering is going to be the next ""become a coder"" - Big, but too saturated. I would learn AI, but maybe in a different lane. For me, im in the lane of AI Clarity. Which is basically manipulating the system to get the results I seek. There are many different avenues within the AI realm. You just get to pick which fits you best!",1
1qk66m7,1,BS,1
1qk0djh,1,Seeing the amazing progress of ClickHouse in the market is so encouraging. It is clear they are getting ready to compete with the other well-known cloud data platforms and I wish them all the success they deserve. Respect!,5
1qk0djh,2,"When will all these different SAS products start biting the industry in the ass? Sure it's great because you can develop fast but what the hell are the environments going to look like in 5-10 years? These companies will realize they got their customers by the balls and start increasing prices like we saw with the cloud rollout. Also comes into play when looking for jobs, the requirements are a unicorn list of services and better hope the one you worked with sticks around.",1
1qjx3t1,1,"If you fancy a migration. I used Aecorsoft which was very reliable (and cheap). The vendor is pretty weird though, pretty much only one Chinese guy who never sleeps. Support 24/7.",3
1qjx3t1,2,"congratulations, you discovered how shitty SAP is. only solution I ever found working is pulling daily snapshots. yes, full loads, daily. yes, fact tables too. yes, they are big and yes it's expensive, moronic and everything else.
you can pay a sap consultant to implement the transaction logs on sap hana and that could work as well but i have not seen it done so far.",3
1qjx3t1,3,"might check snp glue.  Had good experiences with it for an S4 HANA to Snowflake replication process, assuming they do databricks as well.  Cost is probably fivetran level though",2
1qjx3t1,4,Why are you using HVR? Theres a HANA connector in the managed service which is much simpler to use.,1
1qjx3t1,5,There have been recently a few bugs in certain situations - to my understanding these should be fixed. Upgrading your agent/hub to the latest should help.,1
1qjx3t1,6,Curious: why do you not want to use Fivetran's managed service solution for SAP?,0
1qjwtpf,1,"Short answer: yes, but not with a built-in ‚Äúcopy workflow‚Äù button.

AWS Glue workflows can be migrated without recreating everything in the console, but it takes either an API based export or infrastructure as code.

One option is to pull the workflow definition from the source account using the Glue API. `GetWorkflow` with `IncludeGraph=true` gives you the workflow structure and dependencies. From there you also need to export each referenced job, crawler, and trigger using `GetJob`, `GetCrawler`, and `GetTrigger`. In the target account, you recreate the workflow with `CreateWorkflow`, then recreate jobs, crawlers, and triggers so the DAG lines back up. This works well for a one-time move, but you still have to fix account specific things like IAM role ARNs, KMS keys, S3 bucket names, connections, and any hard coded account IDs in job arguments.

The cleaner long-term approach is to turn the workflow into infrastructure as code. Glue workflows and triggers are supported in both CloudFormation and Terraform. Once modeled, you can deploy the same definition to another account with only variable changes for account specific resources. This avoids ever doing a manual rebuild again and is what most teams settle on after the first migration.

In practice, if this is a one-off accident recovery, scripting against the Glue APIs is usually fastest. If you expect multiple environments or future moves, converting the workflow to CloudFormation or Terraform is the better investment.",3
1qjwlaz,1,"Also - a further thought on the modeling aspect before I run away and get roasted... is there modeling considerations to cut down on the memory usage with CouchBase? I've never use it and this is more exploratory for me... so, long and flat may be... bad for the streaming size? Was thinking MQTT... Kafka is a fkin memory hog.",1
1qjvrps,1,"It's difficult to estimate without looking at your workloads. However clickhouse is likely to be cheaper.

However I would not use Clickhouse for a traditional DWH if I had the choice of remaining on BQ.",9
1qjvrps,2,"(Upfront: I work for ClickHouse)

I would anticipate that it would be cheaper, but I suppose it depends on how you use it and your ability to optimise. ClickHouse is a much more efficient engine than BQ query-for-query, but how you optimise your capacity on k8s is veeery different to how you'll have optimised cost on BQ. 

E.g. if you're using BQ with on-demand pricing and query infrequently, the cost-per-query would be higher on BQ than CH, but that might still work out cheaper overall than running your own infra 24/7. But if you're running lots of queries consistently throughout the day, that might flip the other way, where the infra overhead is worth it. You need to look at your usage pattern and work that out.",9
1qjvrps,3,"We‚Äôve moved several years ago from BQ to self-hosted ClickHouse, and I, as mostly analyst, miss those times a lot. Didn‚Äôt have to think about all ClickHouse ‚Äúfeatures‚Äù and just wrote sql. Life was easier",6
1qjvrps,4,What's your current BigQuery cost? How many users are using it? How big is your data?,2
1qjvrps,5,"As always, it depends on your use cases and workload patterns. IMHO you would only save $$ with a self-managed CH cluster yourself if you had an always-on, steady workload that required high performance queries.  Otherwise you need to set up auto-scaling and scale-to-zero, which both BQ and ClickHouse Cloud offer.

The CH team had a decent article comparing cost and performance here:

[https://clickhouse.com/blog/cloud-data-warehouses-cost-performance-comparison](https://clickhouse.com/blog/cloud-data-warehouses-cost-performance-comparison)",3
1qjvrps,6,"Clickhouse is blazing fast but it‚Äôs also has very high maintenance (not cost, more like skill + effort). I would steer clear from self hosted clickhouse unless you have someone that has both the capacity and skill to do it.

Clickhouse cloud pricing is actually not so bad when you compare to databricks or snowflake, although there are some drawbacks to it.",1
1qjvrps,7,"One thing I found with BQ vs CH is that in CH you can do natively a lot more ETL type stuff on ingest than BQ. It is also more flexible about the input and output data formats. 

YMMV but I found ending up having to hook up additional external stream processing stages to my pipeline with BQ, adding cost and complexity whereas with CH was handle everything by itself.",1
1qjvrps,8,Bigquery is the way to go. It's worth every penny you'll pay for it. Life is easier when you don't have to maintain the infrastructure for your database. Then it's also easy to let others have query access and self serve their own data. And if you are truly worried about cost you can set daily query limits to avoid blowing up the bill.,1
1qjvrps,9,"(upfront: I work for Firebolt)

As others have said, it depends, and mostly depends on your workloads.   
  
With extensive expreience in OpenSource, make sure you build in the TCO of self-hosting, for any technology, as it's rarely as cheap as it originally see.",0
1qjv1ja,1,DuckDB or Spark is all I can think of,56
1qjv1ja,2,"Not strictly speaking a one to one alternative, as it is something you need to host and operate yourself, but you can build something kinda similar with open source tools like Trino, Airflow, Spark etc.

  
As for the commercial portion of it, Stackable wraps those tools into a plattform that makes it ""easy"" to deploy (sadly, it remains complex software!) - and provides support and other enterprise features around it.

full disclosure: I work at Stackable :)",8
1qjv1ja,3,"Yes, SAP Analytics Cloud",78
1qjv1ja,4,Ovh cloud dataplatform,5
1qjv1ja,5,Polars has their heardquarters in Amsterdam.,18
1qjv1ja,6,"While not a complete out of the box solution and might defer in feature parity.  
Scaleaway offers a clickhouse datawarehouse. That will lack sql workflows though.  
I am not familiar with Snowflake, but most likly it has dbt similar worfkflows with data validation   
and testing.  
You can define constraints on your producition tables though.  
But for testing you buisiness logic, you would need to move that into app layer. 

There is also data lab spark cluster in case sql hits limits. But again here, testing would be part of app layer.  
There is not such an standardized way to test data processing workflows in spark I so far i came only across few projects which have set it up at all.

[https://www.scaleway.com/en/data-warehouse-for-clickhouser/](https://www.scaleway.com/en/data-warehouse-for-clickhouser/)  
[https://www.scaleway.com/en/docs/data-lab/quickstart/](https://www.scaleway.com/en/docs/data-lab/quickstart/)",5
1qjv1ja,7,Exasol obviously,5
1qjv1ja,8,Exasol is what you are looking for.,21
1qjv1ja,9,"Apache Spark is open source and free. 

Roll your own compute with Spark on managed kubernetes/docker.",35
1qjv1ja,10,Self-host or cloud Clickhouse,6
1qjmtcv,1,"this viral post coins it  
[https://foundationcapital.com/context-graphs-ais-trillion-dollar-opportunity/](https://foundationcapital.com/context-graphs-ais-trillion-dollar-opportunity/)

by definition of context graph, vendors (system of record) do not have it.

you also don't need to wait for them to implement something, you can find solutions in OSS space like cognee or trustgraph",2
1qjmtcv,2,Semantics?,1
1qjluof,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qjluof,2,"Yep, that ADF control table + foreach + params pattern maps over fine, just not as a drag and drop. Databricks Jobs handles chaining and parameters out of the box. For actual looping use either (1) do the loop in a driver notebook or (2) keep ADF as the orchestrator and just trigger Databricks jobs. So no, you don‚Äôt have to write everything in notebooks, but the foreach part is usually code or external orchestration.

If you redesign, the main win is fewer tiny jobs and fewer cluster spin-ups. One job run that reads the control table and processes a batch is usually cheaper and faster than 200 little runs. Also, if you are ingesting lots of files, people often go auto loader or DLT to cut orchestration overhead.

Also lol at being told six hours ago, classic.",2
1qjluof,3,I would avoid orchestrating via ADF and instead move everything to Databricks. Look into pydabs to deploy the jobs defined in your database rather than looping over them.,1
1qjido1,1,"""This is where stream processing frameworks come in. Developers often use [Flink](https://iceberg.apache.org/docs/1.4.3/flink-connector/) or [Kafka-Connect](https://iceberg.apache.org/docs/nightly/kafka-connect/), both of which have a relatively steep learning curve (compared to writing your own application in the language of your choice) that feels somewhat unnecessary when writing simple data pipelines with no data transformations. Is that really all so necessary when you really just want to keep track of the last processed kafka offsets for a current iceberg snapshot?""  
you seem to miss the point by a huge mark.",7
1qjgfq6,1,"You‚Äôre already aligned. Best path: Data Analyst   Analytics Engineer  Data Engineer.

Focus on strong SQL, Python, data modeling, and one cloud (Azure is fine). Build real projects.

Six figures in your mid-20s is very realistic if you move into DE/AE within 2 4 years.",1
1qjgfq6,2,">My goal is to get a six figure job when im in my mid to late 20s

It's all too common with posts like this information is lacking.  Context is really important for helping people help you here.

I'd edit your post and add a lot more detail.  Where you are now, where you want to be, and if you can legally work in said places (basically your visa situation if you have one).

In the context of USD, six figures in rural Wisconsin is going to be very different from six figures in California.  Similarly, getting a six figure job in the US vs other countries also massively varies.  Location is really important.",1
1qjg1mp,1,Don‚Äôt do OBT and make an actual dimensional model with facts and dimensions that respect granularity instead?,2
1qjbawr,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qjbawr,2,"Many businesses who are stuck on Fivetran don't have another option, whether it's because there is a technical skill gap, staffing, etc. There are alternatives, but high switching costs. 

Fivetran is trying to make more money, to properly expand their product offering, and make good on their merger with dbtLabs, sqlmesh, etc. They need staffing hours dedicated to people making good on the synergies between their products. Essentially flying the plane while building it. 

So yeah. They now own probably 50% of the teams that manage the software products many DE's rely on. Probably more. Are we surprised prices went up? No. Will they keep going up? Almost definitely.",44
1qjbawr,3,"Isn't this expected? I am not sure why FT customers are surprised by the increase in their FT invoices. Any VC-backed company needs to show 50% YoY growth, or it faces a down round. FT raises prices for its customers at every renewal to keep its VC satisfied. There are plenty of alternatives available that are much cheaper. It's time to stop paying per-row for data.",25
1qjbawr,4,"We pay around $10k a month for fivetran and are paying somebody to migrate to an in-house built solution. I think the ROI is positive in like 3 months. In 2021-2023, I paid $10k/yr to stitch for unlimited rows. Fivetran is out of its damn mind.",13
1qjbawr,5,"I know nothing about you or your customers situation but for us, there was a small increase but not significant.  Most of our connectors attract more than 1M MAR per month with some around 10million so perhaps that's why we didn't see a large increase.  Our small MAR connectors are generally less than 500,000 paid MAR each with a few < 100,000MAR (we're talking $40USD for the month).

What plan are you using?",8
1qjbawr,6,"When I was head of data of my previous company, Fivetran was a one man replacement.

1 month of salary = 1 year of data sync.

The prices kept increasing till 1/2 month of salary = 1 month of data sync.

At this time we had to replace. And I was an early adopter of Fivetran.

I'm now working as a contractor, with 10Yrs of experience, let me know if you need help migrating from Fivetran.",8
1qjbawr,7,sweet summer child,16
1qjbawr,8,"Price model changes rarely benefit the customer, so what are they doing? Making more money. It‚Äôs that simple. 

We jumped from them last year to portable because of the price and (lack) of service",9
1qjbawr,9,"Same here. After the pricing updates, despite 41% reduction in MARs, our costs spiked 193%. It‚Äôs insane cause we initially got into this to save time and money",5
1qjbawr,10,"Without actual numbers or sources to provide a real opinion on, if you're spending thousands of dollars a month it might be worth looking into Estuary for your workloads, it does pricing by GB and task runtime vs MAR. 

We've seen price reductions of 80% for some SaaS sources like Shopify and 90%+ for databases with high transaction volume.",5
1qj9gsv,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qj9gsv,2,"If your table was already partitioned by id, then you're right it wouldn't do anything and it wouldn't help. 

But why do you have to repartition by id? Only certain types of operations require specific partitioning -- joins where you cannot broadcast either table, or sequencing operations like row number or percentiles.

If you are doing anything else, like sum/count/average/etc, then you don't need the data to be partitioned by any particular value and you can do a random shuffle to get a uniform distribution across partitions.",3
1qj9gsv,3,"yes, you can use techniques like salting or change your partition logic to prevent this",2
1qj7daf,1,"One thing that's helped me is just going back and forth with LLMs on system design and have it expect an answer from a Staff-level engineer. Unless you've done the thing in real life and it's problems and solutions are seared into your brain, you need to do as many problems as you can. 

You did well to take the problem home and rework it though.",8
1qj7daf,2,">The marketing team are interested in tracking the number of times an item is bought for the first time each day

This irks me. Assuming you've replicated the question as it was presented to you, I would have interpreted it exactly the same way. Unless they had a comma before 'each' in which case that would change the meaning (see kids? Grammar is required!).

And I can understand it threw you. Interviews are stressful at the best of time. Hopefully you'll be able to find something just as good soon.",2
1qj7daf,3,Thanks for sharing your experience! We move on and do better OP. There will be plenty more down the line.,1
1qj7daf,4,"Reading through this sometimes I feel like people get so concerned over the tech and using modern tools that data warehousing/data modeling principles just are treated like an afterthought. 

If I was asking a question like that in the interview, for an answer I‚Äôd really just be expecting extremely high level rough approach with more and seeing if you understood what the resulting fact table shape should look like along with the general logic of how you‚Äôd go about doing it.

There‚Äôs a borderline overwhelming degree of emphasis here on specific technologies and file types etc. when it really just sounds like they wanted to understand if you knew what kind of logic belongs in each layer and roughly what the process would look like. 

I don‚Äôt mean any offense here, but from how you described it and the tone of your post, if I was the interviewer I would be concerned you‚Äôre missing the forest for the trees, being more concerned about the technology than about just fundamentals of working with data. 

Wish you better luck next time, just my two cents.",1
1qj5y75,1,"What's wrong having the data in Parquet format sitting somewhere online in object storage (S3, Azure Blob, etc) and then selling permission to access these files?",4
1qj5y75,2,Are you doing block point lookups or range scans for analytics over multiple blocks.,2
1qj5y75,3,"Choosing a technology would depend on a lot of things.. such as your current capabilities, infrastructure, type of data, architecture as well as budget (also company politics). How we would usually approach this is to create a POC for these different technologies, trying to compare these as ""apples to apples"" as possible, with clear, defined criteria. 

with a proper POC, you should be able to have objective data, i.e. the numbers to support which technology would have the fastest search given your constraints - budget, data, architecture and context... as well as know the trade-offs and idiosyncrasies of such technologies. 

...by the way, clickhouse was the best solution for us",2
1qj5y75,4,"ClickHouse, best bet. They also just launched a Managed Postgres service with seamless integration between the two. It‚Äôs the stack of the future",2
1qj5y75,5,"GoldSky does something similar I believe, and they use ClickHouse under the hood.

[https://clickhouse.com/blog/announcing-cryptohouse-free-blockchain-analytics](https://clickhouse.com/blog/announcing-cryptohouse-free-blockchain-analytics)",1
1qj1a6l,1,"Just finish all the modules under any learning pathway - e.g **LEARNING PATHWAY 1: ASSOCIATE DATA ENGINEERING - finish 4 modules under this pathway**

Once done, they will issue 50% COUPON after *06 February 2026.*  
Remember to do only one pathway, doing multiple won't issue multiple coupons.",6
1qj1a6l,2,Certifications are only useful when your organisation sponsors them for free. Paying self for these certificates are not worth anymore as most of the people are having it by just studying through dumps.,3
1qj1a6l,3,It is even worth ?,2
1qj1a6l,4,"I want to learn, but I cannot create an account to be able to access these learning resources. I used my personal email and it was rejected",1
1qj1a6l,5,"I got coupon for free, 100 percentage off and registration led for data bricks data engineer professional certification exam",1
1qj0hi3,1,"It's difficult to understand your question:  moving data from OLAP to OLAP **what**?  database, server, system?  What's ""ADBC""?

So, first off, definitions:

   * Data Warehousing: process, not a place.  It's the process of curating data to enable reuse by transforming, versioning and integrating it.  The place could be a product optimized for data warehousing like Snowflake, or it could be Postgres, DuckDB, or just a set of flat files or a spreadsheet (if say you're warehousing your bowling league's scores).
   * OLAP: On-Line Analytical Processing: typically a service or system that supports analytical queries.  It might use DuckDB, Snowflake, Postgress or a set of flat files.  And it might use a dimensional model, One-Big-Table (OBT), or a transactional model.  And the queries may be handled by a middleware or API layer without giving clients the ability to use SQL.

So, there's an overlap between the two concepts, but it's not 100%: a warehouse doesn't have to involve OLAP, and an OLAP system doesn't have to involve warehousing.

With that in mind, there's lots of valid reasons why people might move data between OLAP systems, here's a few:

   * Warehouse/Mart pattern - a warehouse may publish its data to smaller data marts that use different technology or that have different retention/security/data elements, or that support different parts of the business.
   * Master Data Management - one system may publish dimensional data for reuse by another data mart, warehouse or OLAP system.
   * Scaling - one system may push a copy of its data to a failover or peer system.",13
1qj0hi3,2,"I would say it‚Äôs an anti-pattern in the sense you don‚Äôt have a single OLAP platform. If you are moving a fact or dim from one DWH to another you are opening the gate to potential data inconsistencies, straying from a single source of truth.

However, as anyone who has worked in an enterprise will tell you, large companies have many tools. They don‚Äôt choose Snowflake vs Databricks, they have both. In those scenarios, it makes sense that there will be OLAP to OLAP pipelines. Additionally, tech debt is a big thing. I know of customers that instead of deprecating Terradata, they just replicate it to Snowflake because the effort to rebuild it is not worth it. They would rather prioritize the investment in new initiatives.",4
1qj0hi3,3,"Do have a link to this post? I would say its too aggressive to call it an anti pattern. I think there are cases where you can leverage zero copy. For example if you keep your data stored as Iceberg or Delta Lake, you can use different engines on the same data without moving it. But as you said, there are cases where moving it is just going to make sense, such as:

* Using DuckDB on the edge
* Compliance (ex send redacted or de-anonymized data between regions)
* Data Sharing with 3rd party vendors or customers (ex. send data from your BigQuery to your customers' Redshift)
* Archiving (ex use a cheaper/slower system for historical data)",1
1qj0hi3,4,"Yes, but sometimes it's just the easiest thing to do.  For instance, if you build a great customer master dimension table and only make it available to other teams in your DW or OLAP cube, that's where other teams are going to get it from.

But the fault is really with the producer, not the consumer.",1
1qj0hi3,5,"Enterprise have many anti-patterns because their goal is revenue.

Engineering is always about managing the technical debt, not creating perfection.",1
1qj0hi3,6,What is 'ADBC' and why are you asking about moving data between two OLAP-like databases?,0
1qizifw,1,Any idea guys üí≠,1
1qizifw,2,Chat any help ?,0
1qizifw,3,"If you do a retry the collumn will be added and everything should work,",2
1qizd0c,1,A first do is to check if the legal basis gives you the right to do it. If there's personal or confidential data involved you might have more legal blockers. That's something to ask to your company's legal department and or data protection officer if you have one.,1
1qizd0c,2,"well, secure tunnel solves most headaches, look at site-to-site vpn or private link stuff for real isolation, zero trust vibes help too, orca security and palo alto both got strong cloud watch tools but never just trust the first layer, always audit who touches what, me and a pal went through this last year, made life easier, just stay sharp on compliance because clients panic if you skip basics",1
1qiylvo,1,Thank you! I've started reading your links in November and have gone back to older posts to have more :),5
1qiyh0w,1,"We already have a [quarterly salary discussion](https://www.reddit.com/r/dataengineering/comments/1pbi5i7/quarterly_salary_discussion_dec_2025/)

Perhaps try r/dataengineersindia for India specifc DE stuff.  Locking this up.",1
1qiyh0w,2,"Your post looks like it's related to Data Engineering in India.  You might find posting in [r/dataengineersindia](https://www.reddit.com/r/dataengineersindia/) more helpful to your situation.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qixwtz,1,Welcome to the first day of the rest of your miserable life managing Informatica.,3
1qixl62,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qixl62,2,"We log it into an Azure SQL database.

If you're using ADF, write procs and log the pipeline activities using Stored Proc activities.

When using spark/some sort of code, we have a class to handle the connection and a class for handling logging.",3
1qixl62,3,"For ADF, I usually push logs to Log Analytics and set alerts on failures. In Databricks, I check job/cluster logs and use Azure Monitor for alerts. Works pretty well together.",3
1qixl62,4,"we are now building loguru sink into unity catalog table which we'll be querying. that's for databricks.
what's pretty unusual is we're running adf pipelines from databricks thus we're getting logs from these this way as well",2
1qivk0x,1,"Management wouldn't be wearing the hard hat and swinging the hammer. They'd be staring at their watch, tapping their foot, with a whip in their hand, grumbling to a consultant while a contingent worker on a 3 month contract is sweating bullets trying to make this happen.",39
1qivk0x,2,It‚Äôs actually me with the hammer usually,33
1qivk0x,3,Ain't no way management touches code,10
1qisvk1,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qisvk1,2,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qisvk1,3,"None of these are really data engineering certs. Those are more cyber security. I‚Äôd just wait until something interesting that you actually need comes along, like databricks, aws or azure",1
1qisbbq,1,"ppl are way too hung up on certifications.

no one cares

i have interviewed way too many ppl with an AWS cert who cant have a conversation about AWS resources.",100
1qisbbq,2,"I heard that usually DE chooses you, not the other way around. I started as a backend dev but my job focus slowly shifted to data engineering. While formally I'm still a backend SWE, in reality my job is 75% DE.",39
1qisbbq,3,"There are hybrid roles where you do both DA and DE work, for example, consulting. I went for a consulting role where I was hired for my DA skill but got put on many DE projects. Afterwards, I simply change my title to DE.¬†


This is probably easier in a mid-size city. In large cities, companies have dedicated analytics team so jobs are more specialised. In smaller cities (I'm in Brisbane in Australia for example), most data jobs are hybrid because companies have 1 data team who do everything. I was put on consulting projects where I do end to end whereas my friends who are in the same consulting firm but in Sydney, still do DA projects for very large firms and banks.¬†",24
1qisbbq,4,Started as a data analyst until an engineering position was open 3 years later. Was internal so the DE manager knew me and it worked out.,23
1qisbbq,5,many people (myself included) view roles like DE as a specialisation. ie you have a good foundation in engineering and now you‚Äôre specialising in data. Universities try to short cut that engineering requirement by having a dedicated ‚Äûdomain XYZ‚Äù degree. Which is great but I don‚Äôt need a data engineer who doesn‚Äôt at least have a foundational knowledge and foundational experience in software engineering.,18
1qisbbq,6,"Got an internship, got lucky. Rare but it‚Äôs out there so don‚Äôt give up",11
1qisbbq,7,Worked as an analyst and did engineering stuff. Put that on my resume,7
1qisbbq,8,it was 20 years ago. if you could read the oracle documentation and write pl/sql procedures and packages you where hired. no one cares for certifications. they are so consultants can make their hourly rates more expansive without knowing much more than before the did take the exam,4
1qisbbq,9,"Started as an intern analytics engineer at a startup with 0 experience before. Continued full time in the team, moved on to more data platforms and architecture stuff.",5
1qisbbq,10,"I've started as data analyst, naturally moved into engineering like a year later cuz i put in the work. BI has this benefit",3
1qirtu3,1,"What are you struggling with?

Have you gone through the User Guide?

- https://docs.pola.rs/user-guide/getting-started/",4
1qirtu3,2,The O'Reilly book: [https://polarsguide.com/](https://polarsguide.com/),4
1qirtu3,3,"The docs are incredibly good if you know what you want to do with the data already. If you don't, you need a general book on data xyz first, Polaris second",3
1qirtu3,4,"Literally just https://docs.pola.rs/api/python/stable/reference/index.html

Just started using it for a project and searched the docs as I go.",2
1qirtu3,5,"You can read this book for free for 10 days if you want to check it out:

[https://learning.oreilly.com/library/view/python-polars-the/9781098156077/](https://learning.oreilly.com/library/view/python-polars-the/9781098156077/)",1
1qilwgb,1,"It depends to me. Is the code you‚Äôre running some super lightweight script or something? If so, directly in a PythonOperator is probably fine. If it‚Äôs something heavier, then your idea is better. Airflow is an orchestrator, using it to actually PERFORM ETL or other major transformations or whatever is an anti pattern.",42
1qilwgb,2,"This is the norm at my company.

I'm a senior DE & Airflow expert there.

Though for most jobs we don't need the KubernetesPodOperator we just use normal Operators with the KubernetesExecutor.

So you still use the regular old PythonOperator, but under the hood you're running everything in Kubernetes.

Any questions?",16
1qilwgb,3,"Airflow can be quite powerful given its support of wide range of operators. But we should be very careful of what we pick as it is always a step away from becoming a clusterf*ck.

Personally we use it as a pure orchestration platform  only and other things are managed out of it.",6
1qilwgb,4,"I want to understand who is running compute in airflow and why ? 


 What the OP mentioned is fine as long as your compute cluster like Spark, bigquery, redshift and other operators are decoupled from the airflow orchestrators layer. 

As in compute happens on actually big data processing tech like snowflake , Databricks etc. airflows should just be telling run this at 3am in morning and mark success else make the DE life a mess with failure emails.",3
1qilwgb,5,"I work at a super small company and everything I do runs in containers, so every DAG is its own container. It‚Äôs just a lot easier to maintain and debug, and I don‚Äôt see it as much of an overhead.",2
1qilwgb,6,We do leverage PythonOperator for light/orchestration/formatting scripts. Any heavier Python work is done outside of Airflow.,2
1qilwgb,7,"i am newbie in airflow but in my company they are running airflow through EKS, is there any learning material to understand these types of deployments ?",2
1qilwgb,8,"you are 100% correct. doing the actual work in a python operator doesn't scale because it runs on airflow's compute. It can work for small things, but its bad practice and will backfire as soon as real load is placed on it.",2
1qilwgb,9,"My company uses venv operators, but I don't think we've ventured into remote execution with Kubernetes",1
1qilwgb,10,"Love this idea - splitting orchestration away from the pipelines jsut makes everything cleaner. Sounds like you building a pipeline engine that stays pretty independent from each pipeline‚Äôs logic, which is the right direction.

I just might have to nick this one, mate Will give credits to the BeardedYowie 8-)",1
1qig8ea,1,I took a similar path as you. I was an on prem person for most of my career. I now support Azure and Databricks as well. The skills transfer really easily by and large. I would have more issue with you not knowing python.,39
1qig8ea,2,"Not gonna lie, I‚Äôm trying to hire someone like you right now. I have to hire in Pune, India, but, yeah, I value you.",17
1qig8ea,3,Knowing the so-called cloud stuff or Python-based solutions is highly overrated. You can search and find plenty of people posting here who are unable to find job with these skills.,18
1qig8ea,4,"On prem and SQL only can be a good base, but I'd probably try to learn a cloud platform of some kind in your free time. They are basically infinitely scalable and have a lot of tools integration that is a lot less likely to be used for on prem data warehouses. Specific cloud implementation you use matters less, but having experience with a cloud platform at all is important in the sense that I would guess recruiters would choose someone with slightly less YoE who knows a cloud platform well over someone who has more YoE but none with any cloud warehouses.",3
1qig8ea,5,"I am also kind of in the same boat as you. We use either on-prem or open source tech in our data infra and no cloud is involved. I also have knowledge of cloud personally, but not on production level. 

From the pov of switching, man cloud is a must. I am trying since the past few months and all I get in return is you don't have enough hands on with cloud so we can't take you. 

I am now thinking of getting a cert in AWS cloud for the same.",2
1qig8ea,6,"Knowing the concepts which probably you already know is way more important than knowing tools or cloud providers. Nowadays, with LLMs, given your knowledge on fundamental principles of data engineering you can get away with imementing and debugging most use cases in the cloud. 

So, it might be worthwhile getting a certification and/or updating yoir portofolio with cloud project just for your resume. However, I wouldn't ne worried of being ""left behind"". Lean on your strong points during interviews and how you approached problems using fundamentals and treat cloud for what it is, a tool.",3
1qig8ea,7,I don‚Äôt think all your experience will go vain you can use that to learn cloud platforms quickly but yes learning cloud will gives an edge,1
1qig8ea,8,"you have heavy sql experience, just deepen your skill in cloud as other said, then pretty sure you will be fine. Companies, especially big tech, they value experience more and more recently",1
1qig8ea,9,"At least makes sure you know enough python to get through Leetcode interviews and have a couple of personal projects.  I don't use python much, largely because I find it an ugly language and it's too slow for a lot of what I need, but it has its place and works fine there (largely getting C / Scala to do things).

The problem with knowing only SQL is that you don't have coding experience and that matters because it's a different mindset to set based transformations.  Similarly you see issues with people who can code but don't understand SQL.

TL;DR not good",1
1qig8ea,10,Same boat and it's tough but I have 3-5 years of aws. I'm cranking out the AWS DE cert and then databricks if I'm still looking,1
1qifo47,1,"The solution is probably a combination of processes/guidelines and tooling. The root cause is that you have implicit dependencies on unstable assets. More detail is needed but I'll throw out some suggestions.

Tools like Atlas & Flyway can solve database versioning issues, and give you defined states of your db for every env.

Avoid `SELECT *` in your ETLs. Define the columns so jobs fail loudly and not silently.

""our environments point directly at the upstream teams‚Äô dev/test/prod assets"" sounds suspicious to me, like updates to db's are being done automatically and without warning. Perhaps roll those changes up into a daily release branch with slack messages and announcements to avoid surprising people.

Data Observability tools exist to help (metaplane, bigeye, anomalyarmor) but you got a process problem. You want to create processes to encode expectations into automated checks so that violations are caught mechanically.",4
1qifo47,2,"> We rarely see ‚Äúreality‚Äù until we deploy to prod.

It's completely normal to not see ""reality"" before reaching production. Unless it's highly critical and you are allowed to maintain a live copy of production in test environment, the test data is inevitably less complete. The smart and efficient solution is to have a proper analysis of the different cases existing in production, and use them to build your test data, ideally by the upstream team that is responsible for this data. Consider that it cannot be complete, try to make it good enough. Don't think you can predict all future use cases, the solution for those is proper error logging, monitoring and alerting, so you can react fast when they will happen.

> In practice, this means our dev and test environments are very unstable because upstream dev/test is constantly changing.

Teams should be free to have their team-only development environment were they can break everything, but there should be another test environment that is relatively stable to allow downstream usage of data like you need. I think you need to talk about having such a stable test environment with the engineering management, in my experience it was mostly called ""staging"". It's not supposed to be a copy of ""production"", and it's better if it is not for information security, but it should be a good simulation of production cases to test the code edge cases (as I described above).

In our case, our dev environment is our local laptops. We have hand crafted test files crafted to represent the different cases, and we are able to run full flows locally with either our tools running locally or mocking. Local allows to iterate much faster than if you have to deploy and rely on non-local tools. The unit tests and full flow tests are automated to be able to run in CICD or on request on PRs if they are too slow for systematic run. 

Then there is the company's staging environment that is fed by upstream teams with simulated data representative of production, and the same code and tools as production. There we can run our code in the same tool environment as production. 

> Upstream tables aren‚Äôt versioned

What does this mean? If the code that creates/alter these tables is not versioned, that's pretty bad. The process needs to go through a proper CICD. Or do you mean there's no number you can track to know if the table schema changed?

> schema changes aren‚Äôt always communicated. When that happens, our pipelines either silently miss new fields or break outright. Is this common? What‚Äôs considered best practice for handling schema evolution and communication between upstream and downstream data teams?

Upstream table changes need to be communicated. This is not negotiable for platform stability and needs to be taken to engineering management. They need to establish a proper channel, for example: ticket (with PR links), documentation page, announcement in dedicated channel, regular meetings to plan changes and exceptional meetings for emergencies. A team should only be allowed to deploy such change once the communication process has been respected and the reception by downstream teams has been confirmed.  
This also means the upstream team is responsible for updating staging so it matches the change and you can properly test it in staging.",3
1qifo47,3,"> Related question: schema changes. Upstream tables aren‚Äôt versioned, and schema changes aren‚Äôt always communicated. When that happens, our pipelines either silently miss new fields or break outright. 

> Is this common? 

Absolutely.  There are some rare scenarios where it doesn't happen all the time - like the upstream system *never* changes, or is run by the exact same team doing the ETL work.  But in almost all other cases it's a complete mess.

Plus there's other bad cases like where the upstream system adds a new column, you're using a solution that provides some schema evolution capabilities like Fivetran, so your ETL process just silently ingests the new column - but that new column isn't actually used by anything - and it now has 50% of your costs allocated to it from a different column everyone is using.

> What‚Äôs considered best practice for handling schema evolution and communication between upstream and downstream data teams?

Data contracts & publishing domain objects:

   * Upstream system publishes a ""domain object"" any time any field within the domain changes.  A domain is a higher-level abstraction like user, customer, invoice, supplier, etc.  And the domain object is typically a denormalized collection of related info.
   * The domain object could be published to a messaging system, which could get ingested directly, or simply redirect messages into a file on say s3.  Or maybe the upstream system just writes all pending domain objects to s3 every 5-15 minutes.
   * And the domain object is locked down with a data contract.  This contract is kept in version control, and both the source & target teams can use it to validate all data being sent & received.  I prefer jsonschema for this, and use it to identify columns, their types, as well as info like min/max values, min/max string length, null rules, enumerated values, regex formats, etc, etc, etc.",2
1qifo47,4,"We let upstream dev look at acpt, and acpt to prod.


Any dev environment is top unstable to work against.",1
1qie1ih,1,"In the Airflow slack there is a channel dedicated to K8S. I think you will get more answers there.


In general the webserver and the scheduler should be running all the time.


What are you trying to achieve with spot instances? Is just cost savings? Is your workload running only in the morning? How many DAGs do you have?",1
1qidvaz,1,"3 yeo is still pretty junior so Im not surprised that you don't get much traction on medior/senior vacancies. Especially given your experience is mostly SAS. Not the best time to be looking unfortunately given AI.


Sas is legacy. But your experience is still meaningful.¬† Im not sure how you sell that SAS experience but id make the modeling/¬† extracting logic the main focus and mention SAS as little as possible. Focus on the DE principles you learned, not the tool.

In my mind, a modern DE knows git and source control. Last I worked with SAS (base.. in 2012) it didn't support git. If you know it great, otherwise learn it. Im migrating people from oracle to Databricks and git / branching is a big big change for them.


I am a DE for about 10 years now.¬† I only know Azure. Whatever tool you learn, chances are that a company you apply to uses something else. Don't worry too much, Id say either AWS or Azure is best, but GCP is not terrible.


Where are you located?",3
1qicj6k,1,"You have a job so you can be patient and look. 

Your best bet is to find something else and yes, it's worth sticking with Data Engineering, This job market right now if very stagnant. There's no lay offs, but companies aren't really hiring much. 

Companies are wanting people in a more hybrid role, so I'm not sure what your area is like, but think of moving. At some point, if your job market is lacking in jobs then you're going to be stuck with what's available.",33
1qicj6k,2,"If you are US based underpaid, if you are from outside US sadly it is standard salary.",23
1qicj6k,3,"Highly underpaid. $100-120k would be more of my expectation for a good mid-level eng


Apply to everything and try to get hits. It‚Äôs a universally bad experience for all candidates right now for a multitude of reasons. You can try SWE but your experience is DE so personally I would spend more time on those postings",16
1qicj6k,4,"Senior in Canada. Converted to USD my salary about 65k. 

Plus RTO next month. Just to look at screen and colleagues in different buildings and cities.",3
1qicj6k,5,What is the local region?,3
1qicj6k,6,"You are ‚Äúunderpaid‚Äù the difference from new offer and current pay. 

You don‚Äôt have a new offer and therefore you‚Äôre not underpaid yet.",7
1qicj6k,7,"You‚Äôre being paid terribly. I‚Äôm in a very similar boat, I started out as a very basic analyst but now I‚Äôm an analytics engineer with a tech stack of dbt, Snowflake, Power BI, and Python, plus I do quite a bit of platform engineering. I only make 52k at almost 3 years of experience (also US remote).

I can‚Äôt really answer your questions, I‚Äôm obviously still trying to get it figured out. But one thing I know for sure is we‚Äôre both incredibly underpaid and our only solution is to find new jobs. It would be kind of a waste if you switched career paths instead of leveraging the skills you‚Äôve developed to increase your pay.",9
1qicj6k,8,"It sounds like you're drastically underpaid, but frankly nobody here knows whether your experience has actually developed you into a stronger DE. Converting SAS to Databricks can be really complex or it can be rote work assigned to any warm body. 

With 2 years of experience, you should be able to pass DE interviews that would double your income. If you can't pass interviews, then either you need to improve your general interview skills or work on skill gaps.",2
1qicj6k,9,"63k is not alot. My first job in the tech industry paid that much 10 years ago. 

I think you need to spend some time on training and learning while you have a job. Honestly wouldnt just do the bare minimum at work, but take the opportunity to take certs or whatever to broaden or strengthen your resume.

Conceptually it sounds like migrating SAS code to Databricks is niche, but if you generalize it enough, there are likely opportunities out there that are asking for similar things. Id honestly aim for job postings with anything to do with Databricks.

But yeah good luck. I havent looked too hard but I assume most postings have been for senior or staff roles.",1
1qicj6k,10,What's the metro area you live in/your employer is based in out of curiosity?,1
1qichn4,1,"CS with Math or Stat minor, don't waste time in grad school unless you get free tuition for a good school.

My undergrad is stat&ml from a CS focus school.",8
1qichn4,2,"CS degree is not dead... The career path for graduates is hard at the moment, but if you want a career in software or data, do CS.

Don't plan your life around short term mood swings in the industry.

Just my opinion. I hope I don't get roasted for it!  CS is a great field to study, and if I'm wrong and the computers do enslave us... You can join the resistance üòÅ",6
1qichn4,3,"Aren't people saying every career path that's not HVAC dead?


Maybe that happens.¬†


But in the absence of proof you have to assume strong problem solving¬†skills will continue to be valuable for humans like they've always been. CS or the like develops that.¬†


There's a universe where actual nuts and bolts level knowledge becomes more valuable than ever over the next 5 years. Because trillions of¬†lines are getting vibe coded by people that don't have it. Maybe they can be vibe maintained. Maybe not.",5
1qichn4,4,Information Systems,3
1qichn4,5,"MIS or CS are the only options, in my mind. Personally, I value the MIS over CS but we do more SQL than Python. So, traditionally, probably CS.",3
1qichn4,6,"if you're 4 years + away from caring about a degree i wouldn't pay much mind to what folks job searching today think.

you also dont need to make decisions about grad school.

CS or math are probably fine; you probably have 2 years before you start taking much core classes so you have plenty of time to adjust.",2
1qichn4,7,Why is your ambition to become a data engineer? What do you think data engineering is? It‚Äôs not really engineering.,1
1qichn4,8,Study linguistics.,1
1qib5fr,1,"Haha, brilliant, the original youtuber John Teel loved this spin!",1
1qi7quj,1,Yeah that was my exact experience in a similar job.,98
1qi7quj,2,Is that normal? I am not sure. All I know from experience is that the bigger the org the more red tape and the more pointless meetings you get stuck with,63
1qi7quj,3,"I've found that no matter what you're doing, working in big corporations means less responsibility and more redtape. That's why I prefer consultancy rather than working inhouse. Because you have a higher probability of working on big projects and are less likely to do day to day stuff. (logging every hour of your day is a bitch, though)",46
1qi7quj,4,"Yeah, this is pretty normal at large, regulated companies. Once you hit a certain scale, a lot of the ‚Äúwork‚Äù becomes coordination, approvals, and moving changes safely through environments rather than writing code. The irony is that the more senior and mature the org, the less time you actually spend coding. devops + strict governance + multiple environments usually means pipelines are easy to build but slow to *land*. Insurance is especially heavy on this. Some teams do manage to streamline it with better self-service, platform teams, or looser controls in non-prod, but 60 to 70% overhead isn‚Äôt unusual. Whether that‚Äôs acceptable or soul draining is kind of the real question",21
1qi7quj,5,"Most senior DE in major manufacturing company in Europe here. It's normal, the further I go into high impact projects the more calls and business engineering I do. Currently I do mailing, meetings and monitoring the control dashboards and it's already 4 hours in work and I'm about to finally open Databricks to throw in some code. Don't fear of going Data Engineer -> Solution engineer, there is good $$ there",9
1qi7quj,6,"Depends on the project. Last one got so crunchy my boss was doing all the meetings and admin while I done all the work. Other projects it's been near 50/50.¬†


Part of me prefers the pressure of a very hard deadline and someone else dealing with the rest of the chaos.¬†",7
1qi7quj,7,"Ah, ‚Äúagile.‚Äù Somewhere there are happy people that never heard of that miasma of dystopian micromanagement. I really need to be more grateful for what I have.",5
1qi7quj,8,Sounds like you're in a large company. My company was bought a year ago by a large company and I'm experiencing this too. I'm actively looking.,4
1qi7quj,9,That's normal.,4
1qi7quj,10,"Unfortunately yes.   The bigger the company, the bigger the work getting done tax",4
1qi76hy,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qi76hy,2,"Remember, age is just a column!",23
1qi76hy,3,"You are a junior DE. Its expected that you dont know.


Ive been in DE for 10 years now,¬† I dont know anything about a lot of tools that I havent worked with.


I do azure, I barely know anything about AWS or GCP. I have no exposure to snowflake.


I still earn a good living in what I do know.",13
1qi76hy,4,"I'm 46 and am in school to change careers to DE. I'm looking for a new adventure in a career that I think I'd really enjoy. That's too valuable to me not to pursue. I know I might find challenges that younger DEs won't, but, at this age, I know how to put in the effort and work hard to make it work out. I'm both scared and excited. If this is what you want, go for it!",3
1qi76hy,5,"if you‚Äôre thinking about switching to data engineering, one thing i‚Äôve learned is that it‚Äôs not just about knowing one tool. there are tons of service providers now for storage, compute, orchestration, etc, and honestly there‚Äôs kind of a war going on between them. you‚Äôll hear names like apache iceberg, snowflake, dbt, kafka, airflow, databricks, aws services, azure services and many more. knowing the names is easy, everyone knows them these days.

what really sets you apart is how well you can design or tailor a solution based on actual business needs. two companies can use the same tech stack but design it very differently depending on cost, scale, data volume, and reporting needs. a big part of the job is deciding what to use, what not to use, and how to connect things in a clean way. delegation and leveraging other people‚Äôs skills also matters once you‚Äôre working on real projects.

most data engineering jobs today are still around reporting and analytics pipelines, so don‚Äôt ignore that part. if your goal is to earn more, then real-time streaming systems are worth learning. they‚Äôre harder, higher risk, and higher value, so fewer people do them well.

at this point, tools like snowflake, dbt, databricks, unity catalog, etc are almost basics. just knowing them isn‚Äôt enough anymore. the real skill is making smart choices across these tools, estimating costs, and building something that actually works for the client without overengineering. that‚Äôs where good data engineers stand out.",2
1qi76hy,6,"I've noticed that if you get your fundamentals right (code/data eng. practices/principles, writing efficient queries, data structures and the solutions within your cloud platform of choice (even just surface level)) you're already in a great position as a junior. Once on the job you get specialized anyways.

I started 3.5 years ago with barely any knowledge that was DE specific, you learn a lot on the job, through your own experiences and by studying things you notice you're lacking in. The first 6 months I couldn't really do much because of how much I had to learn, it gets better afterwards!",2
1qi76hy,7,"Age is just a number man! Appreciate your thought process! I think to learn DE you should definitely give yourself sometime and for me personally mentoring worked. I came across my mentor, he helped me through.",2
1qi76hy,8,"So.¬† I'm a 38 year old senior data analyst.¬† I just made the switch to DE.¬† You can do this.¬† You probably already are. It's more technical from a project oriented perspective, but being stagnant is worse than never trying.",2
1qi76hy,9,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qi76hy,10,"38 definitely isn‚Äôt too old, I‚Äôve seen plenty of people pivot into DE in their late 30s or 40s and do just fine. What you‚Äôre feeling is pretty normal because DE has a huge surface area, and learning it feels endless at first. Every topic opens three more doors. The trick is not trying to learn ‚Äúeverything.‚Äù You don‚Äôt need mastery of every tool you need a solid mental model of how data flows end to end. Once that clicks, new tech becomes variations on the same ideas, not brand new worlds. Your PM/BA background is actually a plus. Understanding requirements, tradeoffs, and why pipelines exist is something a lot of younger DEs struggle with. Focus on building a couple of boring, real pipelines and ignore the noise. Feeling overwhelmed doesn‚Äôt mean you‚Äôre behind it usually means you‚Äôre learning the right things",1
1qi6yc8,1,"It‚Äôs not garbage at all, but it does feel a bit over engineered for ingestion. If everything is ultimately staying in Azure, I‚Äôm not sure the Athena layer is pulling its weight unless you really need to query AWS data in place. In most setups I‚Äôve seen, ingestion is kept dumb, pull raw data from AWS, Oracle, SharePoint, etc. straight into ADLS and do all the SQL/transform logic downstream in Azure. Athena adds another engine, catalog, and permission model to maintain. So yes landing the data first and querying it later is usually the cleaner pattern. I‚Äôd focus on making ingestion reliable and replayable, then worry about schemas and cost logic after",2
1qi0z60,1,"For infinite loop Dataset workloads, structured streaming micro batches are the recommended approach. They isolate DAGs per batch, manage lineage, and prevent silent exits due to metadata growth. If you stick with batch loops, you need a mechanism to restart the Spark context periodically and checkpoint and clear lineage aggressively, but that is more fragile. Tools like dataflint can also help flag where your plan or metadata is growing across iterations so you can catch it before it silently exits. Structured streaming gives predictable long running behavior and scales better on YARN for production workloads.",10
1qi0z60,2,"Is this ran locally or on the cloud? Because if you are running infinite loops on the cloud, holy fuck do you like to live dangerously.",4
1qi0z60,3,Because you‚Äôre not supposed to use it like that. Either schedule the job every couple of minutes with a cron/script/orchestrator or use structured streaming.,1
1qi0z60,4,"I had this problem recently, add this inside your very naughty for/while loop after your sleep.  
spark.range(1).count()",1
1qhzxaw,1,"I‚Äôve seen a few teams use these, and they solve slightly different problems. Hex is great if your team is more notebook driven and likes mixing SQL with Python in one place. Sigma shines when you want business users querying the warehouse directly without you building tons of models. Omni sits closer to a traditional semantic layer, metrics-first approach. Evidence is more developer focused and works well if you want version controlled, code-based reporting. What works best really depends on who your primary users are (analysts vs business vs engineers) and how much governance you need. None of them are perfect most teams end up picking the one that matches their workflow rather than raw feature depth",7
1qhzxaw,2,"Done a few evaluations of most of these along with Tableau. Some thoughts, but don‚Äôt take it as comprehensive reviews:

Hex
- Been a while since I looked at this, but I would say visualization flexibility was not as robust compared to Tableau or Sigma.
- Definitely felt more like a notebook and exploration tool useful for scientists and analytics but maybe not for business-oriented user.
- Great for ‚Äúdata apps‚Äù beyond just a visualization tool.

Omni
- Great if you are moving from Looker. It will feel very familiar.
- Very structured data mart/semantics layer.
- Close to mature in terms of features but I would say Sigma is more mature.

Sigma
- I would say this is the closest to Tableau in terms of maturity of features.
- Usage logs is more convenient to use than Tableau.
- Actions allow great flexibility in both changing visuals or filtering or more.
- Input tables are cool and it could be really cool as a way to create ‚Äúdata apps‚Äù and use Sigma dashboards as data collection tools in the same UX).

Evidence
- Haven‚Äôt heard of this one yet.",6
1qhzxaw,3,"Depends what you're looking for? a BI and reporting tool or an exploration tool - or all in one ?   
I'm currently looking for a good AI data agent tool so I tried Hex and Omni.  
Omni definitely feels more like setting up a BI tool - and the AI agent is great because it gets a lot of existing context (table metadata + dbt)  
Hex is more notebook like, so I think it's only good if you have a technical user population. The agent is a bit harder to setup and for now I haven't had good results with it",2
1qhzxaw,4,Curious to learn this as well. Trying to move out from Tableau.,1
1qhzxaw,5,"We are using Hex. If all you‚Äôre looking for is a BI tool for creating dashboards, I imagine there are better options. As an actual analysis tool it is exceptional. The Threads feature (agentic AI over your data warehouse) works incredibly well.",1
1qhzxaw,6,"I‚Äôve used Sigma, happy to answer any questions.",1
1qhzxaw,7,"I've tried all of these lately, as I switched from a data job in tech to fractional-head-of-data, now advising clients on BI/analytics tool selection. Spoke with a ton of current users as well. Common thread - Looker/Tableau are ""dying kings"" and this new wave of platforms will win the game :)

* **Hex**: best for analyst velocity. Weak if you need a governed explore experience for lots of business users.
* **Omni**: best ‚Äúmodern Looker-lite‚Äù vibe. Strong semantic model + self-serve exploration. You still need someone to own modeling/definitions. Founders' mission was basically ""if we built Looker in 2025, what it would look like"". 
* **Sigma**: best for business adoption. Spreadsheet UX on warehouse data. Governance can sprawl fast unless you enforce metric patterns.
* **Evidence**: best for narrative, versioned reporting. Feels like ‚Äúanalytics as code/docs‚Äù. Not an ad-hoc slice-and-dice tool.

There are also very interesting newcomers in the space like Supersimple, which combine the best of all worlds in an intuitive UI + includes enterprise search (gives you complete answers to business questions from DWH + your tribal knowledge in Notion/Slack/Confluence etc). Worth including in your assessment.",1
1qhyv0a,1,install the RDS for PostgreSQL aws_s3 extension,4
1qhycmc,1,"Are you triggering the jobs using sparkOperator /PythonOperator or via a bash script using BashOperator?
And can you share if anything is getting printed in the logs for those 10 mins?",2
1qhycmc,2,"u/Great-Tart-5750

I was able to resolve the issue.

Downgrading to Python 3.10 exposed the real cause in the scheduler logs. The default setting  
`[execution_api] jwt_expiration_time = 600` (10 minutes) in `airflow.cfg` was expiring the token, which explains why every job failed after \~10 minutes.

I fixed it by increasing `jwt_expiration_time` to `86400` and also updating `jwt_leeway` under `[api_auth]` from `10` to `60` seconds.

Error I was seeing:

    airflow.sdk.api.client.ServerResponseError: Invalid auth token: Signature has expired",2
1qhu7di,1,"1. Less than in software engineering, but steady. 
2. I believe this depends on the market, like any other.
3. I believe it will take longer to be replaced by AI.
4. Learn SQL. For good.",1
1qhu7di,2,"I believe salaries and job opportunities depends also on the region/country, because the previous comment said less than software engineers but where I am at (as much as I know), Data Management is more.",1
1qhu7di,3,"Coming from embedded/automotive, you actually have a good base. System level thinking, debugging, and dealing with constraints translate well to DE. Job wise the market is tighter than a few years ago, especially for juniors, but experienced engineers switching domains still have a shot if they can show real projects. It‚Äôs not an instant jump though expect some ramp up time. Salary progression is good but not magic. Early on it‚Äôs usually comparable to other software roles, then it flattens unless you move into senior/staff or platform heavy roles. The YouTube numbers are outliers, not the norm. Long-term demand is still there, but the bar is higher. ‚ÄúGlue code + SQL‚Äù DE roles are getting squeezed; people who understand data modeling, reliability, and production systems are still in demand. With your background, you‚Äôre closer to that second group. If you‚Äôre serious, focus on building one or two end to end pipelines and learning how data behaves in production, not just tools. That‚Äôll matter way more than the title change",1
1qhu7di,4,">1. How are job opportunities right now for data engineers, especially for someone switching domains?

  
There is no better way to answer this than by simply applying to jobs.",1
1qhpdfc,1,"What you‚Äôre feeling is very common in production support roles, especially in large service companies. Ticket based work tends to plateau quickly, which isn‚Äôt a reflection of your ability. Your PL/SQL experience is actually relevant for data engineering procedures, debugging, and understanding data flows all transfer. The main gap is modern tooling and owning pipelines end to end. Given the market, I wouldn‚Äôt rush a hard switch. A safer move is to upskill alongside your current role learn Python properly, build a small DE style project, and try for an internal move if possible. If it‚Äôs impacting your health, that‚Äôs a real signal just make the transition planned, not impulsive",1
1qhp36r,1,"Lakeflow is sort of the branding/bucketing for all of the traditional etl, ingestion, pipelines, orchestration, etc type functionality on the databricks platform. 

[https://www.databricks.com/product/data-engineering#related-products](https://www.databricks.com/product/data-engineering#related-products)",1
1qhp36r,2,"If I am not wrong ( databricks experts can tell better)  Lakeflow is nothing but a low code /no code pipeline builder. Imagine drag and drop features. Imagine you want a CRM data like Salesforce or you want Google Analytics data. Just use the connector and your job is done without manually writing APIs, retries etc and you just focus on a transformative logic and the lakeflow takes cares of the rest from Extracting ( pre-built or your own connector) -> transformation ( you helping out here) -> load ( your final BI layer). 

  
So in short if a stakeholders wants a Salesforce CRM or Google Analytics data you can setup your pipelines within few clicks and finish it off. Just imagine a lot of abstraction where you can just manually enter refresh schedule etc etc.  

On most occasions these no low code / no code solutions don't work for lot of enterprises based on complexity. For simple data dumps they work really good.",1
1qhp36r,3,"I think it is a new umbrella term of sorts to cover their pipeline functionality. 

Lakeflow jobs are just normal Databricks jobs from what I can see. Lakeflow Spark Declarative Pipelines are the new Delta Live Tables which use Spark Declarative Pipelines. Lakeflow Connect are their connectors.",1
1qhp36r,4,"You‚Äôre mostly on the right track, the confusion is normal because Databricks‚Äô naming doesn‚Äôt help. Think of Lakehouse as the overall architecture, data lake + warehouse behavior on top of Delta tables (not a traditional warehouse, but it fills that role). Lakebase is newer and more OLTP oriented. It‚Äôs meant for serving low-latency app workloads while still integrating with the lakehouse for analytics. You dont need it for most DE use cases unless you‚Äôre mixing transactional apps and analytics tightly. Lakeflow is basically Databricks opinionated pipeline layer. It wraps ingestion, transformations, orchestration, and governance together (Delta Live Tables, auto ingest, quality checks, lineage). It‚Äôs not a new storage layer, it‚Äôs about how data moves and is managed. Rough mental model would go like this, Lakehouse = where data lives Lakeflow = how data gets there (and stays clean) Lakebase = optional transactional sidecar Once you see it that way, the pieces line up a bit better",1
1qho5x2,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qho5x2,2,"Hey, Start with some public domain data sets and try to build a dashboard on that. I would personally love to do analysis on imports and exports of the country. You can plot different graphs based on total value, trade deficit, state wise data, for say last 10 years or what ever is available. This can help understand what country is producing and bringing value, and what all things it depends on imports.",2
1qho5x2,3,"Just do the vanilla etl end to end: ingest api into a database, model it into a data model, build a dashboard on top",1
1qho5x2,4,"Find a free API and use Python to write an ingestion script that pulls the data into a database or data warehouse. Use dbt to transform the raw data into something meaningful, then visualise it on a dashboard. Containerise everything with Docker for easy setup and use Airflow to orchestrate each step the pipeline (ingestion -> cleaning -> aggregation)

You can build the entire stack for free using open source tools like ClickHouse and Metabase.",1
1qho5x2,5,"Great to see you diving into data engineering early on. For a capstone, consider something hands-on that shows your grasp of the end-to-end data workflow. A few ideas:

1. ETL Pipeline: Ingest data from public APIs (maybe weather, finance, or social media), clean and transform it, and store it in a cloud database. Visualize results or build a simple dashboard to display insights.
2. Data Warehouse Project: Set up a mini data warehouse using open source tools (like PostgreSQL, DuckDB, or even cloud warehouses if allowed by your school). Populate it with multiple sources and practice schema design, partitioning, and basic analytics queries.
3. Batch vs Stream: Compare batch ETL (e.g., with Airflow or Prefect) and simple streaming (Kafka or just a basic pub/sub) on a dataset. Even if you use sample scripts, explaining the trade-offs is valuable.
4. End-to-End Analytics: Choose a topic (like campus resource usage or student performance if you can get anonymized data), gather, clean, and enrich the data, and display it in a lightweight front-end. This gives you a taste of the full stack.

Don‚Äôt worry about building something massive; depth of understanding matters more than shiny complexity. If your team isn‚Äôt super technical, you might look into tools that let you build simple frontends for your data without writing a whole web app from scratch; it‚Äôs a nice touch to showcase your processed data to non-technical folks.

Best of luck on your capstone!",1
1qho5x2,6,"For a beginner DE capstone, I‚Äôd keep it practical and end to end rather than trying to use every tool. Pick a real data source (API or public dataset), ingest it, clean/transform it, store it properly, and make it usable for analysis. For example try build a small pipeline that pulls data daily, handles schema changes or bad records, and loads into a warehouse. Add some basic data quality checks and maybe a simple dashboard or query layer on top so people can actually use it. What matters more than fancy tech is showing you understand core DE ideas ingestion, transformations, data modeling, reliability, and documentation. If you can clearly explain why you designed it the way you did, that‚Äôs already strong signal for junior DE roles",1
1qho5x2,7,Where do you get the energy,1
1qhk6rc,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qhk6rc,2,"I can‚Äôt say if it‚Äôs great for you specifically, but in general, I think data engineering is a great career choice. The job security, pay and opportunity is solid, especially if you work hard to learn the fundamentals and some tools. 

I don‚Äôt see the need for data engineers decreasing any time soon, even in the face of AI. To the contrary, there is more data engineering work to be done to facilitate AI, and there will always been a need for data management. 

One caveat though: breaking into DE might be hard right now. AI is hurting the entry level job availability. But once you get some experience, if you like the work, it‚Äôs a solid career path IMO.",45
1qhk6rc,3,"If you were in marketing before, Data Engineering is by far a better option. Though it can be hard to enter the field. 

The best position to be in as far as I'm concerned is a data engineer who works closely with the business and management. AI is actually pretty good for data engineering since it makes data more valuable. However, the bigger risk in DE is outsourcing. Being connected to the business and the data strategy is a much safer (and better compensated) place to be. If you don't have the personality for it, and many in this field don't, focus instead on relationships with data stewards. Be an expert in what all the data is and all the nuances in it and you become pretty safe. Not as well compensated, and not as mobile between companies, but highly secure.",16
1qhk6rc,4,"I entered  my data engineering role with just unpaid data analyst internships, I graduated in May, AND I majored in MIS, not CS. Their is a very high technical barrier you (usually) don‚Äôt have to be a master of Python(Spark) and SQL and the entry level but be prepared to hit the ground running you need to be able to keep up with the senior developers and prepare to problem solve unfamiliar territory",7
1qhk6rc,5,"The market‚Äôs rough right now, especially for junior roles, so your confusion is pre normal. Data engineering isn‚Äôt a bad move, but the cert alone won‚Äôt carry you most teams still want to see some hands on work. If you liked Python/SQL more than marketing, DE makes sense, just know it‚Äôs harder to break into than analyst roles right now. The GCP cert helps with structure and interviews, but it‚Äôs not a silver bullet. If you go for it, build something small on GCP alongside studying so you‚Äôve got something real to talk about",4
1qhk6rc,6,DE is a good career but requires multitude of skills and three of them are the core and non-negotiable skills.¬†,3
1qhk6rc,7,"who is encouraging all these new/recent grads to grab certificates left and right.

i have no idea who cares about a ""GCP Professional Data Engineering""

you want to be an engineer? build something.

good engineers build things. 

they build a thing, and another thing and another thing... and they see why an idea is bad because they watched it crash.",1
1qhk6rc,8,"Data engineering is still a solid path, but it‚Äôs not an easy pivot in this market. Entry-level DE roles are pretty competitive. The GCP PDE cert can help signal direction, but it won‚Äôt be enough by itself. If you go this route, pair it with hands-on projects (pipelines, ETL, BigQuery, orchestration) and be ready to explain design choices clearly in interviews.",1
1qhk6rc,9,"In general, based on the GanAI trend, DE is a solid and better path compared with most of the tech job.
But in the end it still depends on your goal, what kind of life do you plan to live out?",1
1qhk6rc,10,"If you‚Äôre looking to break into data, consider doing any role that requires solid fundamentals of SQL i.e prioritise SQL then Python. For the cloud, again, learn fundamentals, pass a certification but have a go using the free trials. In azure for example, but this will translate to other cloud providers, just under different names; learn about storage (blob storage), setting up of linux based VMs, serverless compute like Azure Function App, deployment using Docker (you can have Python, or JS only Function Apps) or generic where you drive the dependencies using docker. Then either data factory or airflow or other orchestration tools, data warehousing ie oltp vs olap concepts in the cloud and ultimately some basic networking so that you can for example make data factory actually be able to read from a VM hosted SQL Server. so tl;dr - SQL -> Python -> Cloud, but obviously for Python/Cloud, learn only the concepts that are relevant to data analytics/engineering. I‚Äôm saying this as an analytics consultant with 3 yrs of experience in analytics specifically, and research experience prior to that.",1
1qhjnv4,1,"I‚Äôll be there. Again! My 4th round I think‚Ä¶

Treat the entire weekend as a meetup. It‚Äôs one contiguous conversation with other people entering and leaving your immediate conversation. Topics and speakers change on a whim. Lynn is a great host who understands the flexibility of that vibe. 

Visit a few vendors in the hall, but they are not ‚Äúpitching‚Äù you in the sense of a hostage. They grok that we are the practitioners who yes/no product adoption at our own home org. They do keep some free drink (alcohol) tickets and dispense freely. Once closing town hall begins, drinks are typically sponsored by some vendor. 

Sunday breakouts are simply formal structure surrounding impromptu topics. If some topic is relevant to you, attend that! If you favor a particular speaker, attend that! I chose Mark Freeman‚Äôs data quality breakout last year; only 4 of us showed so we just walked to lunch. 

Some groups organize around Sat PM dinner. Try a BBQ joint within a 3 mile radius if offered. You will get a social offer if you have been social all weekend. üòâ

I‚Äôm hiding in my cave until weekend when it pays to be social.",2
1qhjb83,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qhjb83,2,"Your background is honestly more relevant than you think. A lot of DE work overlaps with SDET skills: automation, pipelines, reliability, and systems thinking. Snowflake plus Airflow is real signal, the main gap is owning data models and pipelines end to end. I wouldn‚Äôt over focus on certs. One or two solid projects where you ingest, transform, and warehouse data will help more. When applying, frame your experience as pipeline automation and orchestration rather than ‚Äútesting.‚Äù If you enjoyed that work, the transition makes sense",2
1qhi4lr,1,"The first edition is great, but you're so close to the new edition that, I honestly would hold off for another 2 months I guess, and fill in with other resources for learning in the meantime. 

Assuming the purchase of a $30-50 book is not trivial to you. If it's trivial, then hell buy both and support the author twice.",16
1qhi4lr,2,"It‚Äôs a great book, I still revisit sections from time to time.

If your goal is to understand data systems from a software engineering and architectural perspective, it‚Äôs one of the best reads out there.

It‚Äôs less about tools and more about how to think about data systems, so it‚Äôs valuable whether you read it now or later in your career.

Highly recommend.",36
1qhi4lr,3,"I‚Äôm reading it right now, the book discusses different aspects of making applications reliable, scalable, sustainable etc. A bit of history here and there and how things work, the problem they solve etc. 

It is worth reading. If you have worked building apps, you will find that many concepts are familiar to you.",6
1qhi4lr,4,"I think it‚Äôs fair if you want to wait for the new addition.  I think a ton is applicable in the first version regardless.  I read it really slowly, 8 months , but it really did open my eyes to a lot of patterns I work with or around. 
Read for most of 2025 for reference",3
1qhi4lr,5,"It‚Äôs very dense subject matter and needs the accompanying visual representation to get a handle, especially for topics like LSTM, unless you are already familiar with the concepts being discussed.  I‚Äôd skip the audiobook and get a hard copy or digital version.",3
1qhi4lr,6,"It was recommended to me during grad school by my professor, and it‚Äôs easily one of the finest books available. I‚Äôm not a DE (I‚Äôm an MLE), but I work with DEs daily and can see how practically useful it is once the concepts sink in, far more than just hammering theory. Hands down!",1
1qhi4lr,7,"This is a good time to read it. It will give a whole new perspective and way to look at things. It will help you understand how systems work, so that you can make better design choices and build better systems from the get go. Not a easy read but then multiple revisions will help deepen your understanding on Data intensive applications",1
1qhi4lr,8,"I started reading the 1st edition(I have a hard copy version), now I'm thinking if should just switch to 2nd edition as I also have access to the ebook of 2nd version. I prefer reading from hard copy.",1
1qhcekr,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qhcekr,2,Meetings and constantly changing requirements,134
1qhcekr,3,"Context switching and the absolute insane overuse of agile. Agile isn't supposed to mean ""throw everything at the engineer and then prioritize"" 

I'm at a company that wants to think and act like fin tech but because we don't have solid processes it creates so much tech debt. Bringing it up is like you dropped an f bomb in church",77
1qhcekr,4,"I struggle with long meetings 
I lose my focus half way through when they repeat same unnecessary things over and over that I miss important parts discussed then I have to figure them by my own",55
1qhcekr,5,"I think for us all it's context switching , which includes meeting transitioning. I have two tools I absolutely relish:
1) Todo manager: I use Super Productivity. 100% local with ADHD and Focus/Flow management in mind
2) Rituals: I plan my week and I plan my day. Every time as the first thing I'll do. When forced to switch context, I look into the day plan that I set up.",24
1qhcekr,6,"Yup. Lead engineer with primarily inattentive type adhd and a sprinkle of autism. Context switching and changing requirements are the worst. And meetings. Oh my god.. the meetings. 

I decided that I didn‚Äôt want to deal with it alone and I felt comfortable enough to disclose my adhd to my manger, my PM (that I have worked closely with for 5+ years) and a few more coworkers. Full acceptance and support all around. The PM was even like ‚Äúuhm.. yeah, I could have told you that‚Äù

I still forget shit. And sometimes trail off. Or hyperfocus on the wrong task. Or deepdive into some weird deviation on the 8th decimal in an algorithm while listening to Finnish melodic death metal at a dangerously high volume. The difference is that now they know why.",15
1qhcekr,7,"Yep, sole data engineer here and standing up a brand new CDW.  Diagnosed and medicated when i was a kid.  Re-diagnosed and medicated since about 5 years ago.

Biggest help for me is cognitive therapy with psychologist who specializes in ADHD and stuff.

The validation thing, learning to trust myself, learning to stop self-shaming myself, etc has been a lifesaver.

And to literally sit down and talk with someone about all the stupid junk that goes through my head.  Just being able to express frustrations with myself and occasionally with coworkers (data analysts).

On top of designing the ingestion and bringing in new data sources, I'm also the only account administrator for our CDW.

Literally nobody else on my team, even IT, understands the work I do.  They wouldn't even know I am doing it unless I straight up explain it to them.

It does force me to communicate and document like a mad dog.  Force my team to make decisions together with me so they stay involved and in the knowledge loop.

But going from access control, onboarding & training new users, coming up with a strategy for ongoing development, and also switching back to discussing whether to bring in <this other critical system's data>, peppering in these smaller data sources, it's a fucking lot.  I

Sometimes I think the ADHD helps lol  But my therapist is my fucking hero.

Based on the text vomit I just dumped here, I would recommend you find a therapist you click with if you are struggling.  And ADHD people usually feel like they're struggling, don't we.",16
1qhcekr,8,the 7 seconds it may take spark to start evaluating a simple query,14
1qhcekr,9,"Reddit when someone says something that is wrong. 

Or that I am wrong. 

Argh! Distracted again FML",8
1qhcekr,10,"Meetings.

Every 2 weeks in person scrum retro + scrum planning for 4 hours with my team (2 people), web analysis team (4 people) + both team leads + head of department.

My tickets were like 5 minutes of the 4 hours and everything else so fucking boring and had nothing to do with my work at all so ... felt asleep 2 times while sitting next to the head of ...",7
1qha2l9,1,"To help everyone

There are (at least) two types of problems in the world of business.

One is measurement / analysis. This is things like ""how much did we sell last month?"" These are solved by data. databases, warehouses, SQL, BI, semantic layers. 

The other one is process / logic / workflow. This is things like ""what sales actions should I take with this customer over the next 3 months to improve my chances of a win?"" Or ""what data should I use to answer this question?""  These are solved by different kinds of data, usually qualitative in nature - metadata, documents, runbooks, applications, code, vector searches, RAG, and graphs. 

""Context graph"" is a buzzword - it's just a graph database, the same ones we've had for 40+ years. The ""context"" just describes its purpose - a tool for providing context to AI models and agents.

Offloading context, just like offloading data catalogs and other tools, help AI agents by preventing them from hallucinating, drifting, or getting context overload.

Just like we don't expect an airline pilot to handle all measurement, observation, and emergency procedures themselves, we give them accurate dashboards, runbooks, and a support crew to help them achieve their tasks.

Data engineering obviously has a role in both problems in terms of investing, transforming, and managing data into data warehouses and graph databases.",43
1qha2l9,2,"I've heard people muttering about context graphs, but nobody's been able to define it for me. I know about knowledge graphs, and am wondering if a context graph is where you apply a knowledge graph in order to feed more contextually appropriate content to an LLM than an equivalent vector-database equipped RAG search would normally do - but it can't just be that.",27
1qha2l9,3,"The meta for llms changes every week, so don't get two freaked out about it.  

Business context is usually captured with either RAG or graphs.  RAG uses a vector similarity search algorithm to look up related information and graphs rely on nodes and edges to connect relationships in your data.  Basically look at how Neo4j works.  They have been talking about the graph approach for a while, but barriers are how your data is organized/tagged as it's less effective the fewer relationships you have.  That's not easy to establish depending on what your metadata looks like.  Also massive graph dbs tend to not be that performant.  

You can still get pretty good results with them, especially for document search use cases or something like that.  But there are also people talking about how tensor based LLMs are limited and not going to get us to the promised land and the next step might be difusion models or something else.  AWS also was talking about how they are maxing out what they can do with generic LLMs in training and how they could be more effective if they had access to actual business data to train on (not going to happen).  Agentic systems are the current hot thing where you can create ""tools"" for the LLM to use based on the context.  Like if a user asks an accounting question, the LLM can call an accounting agent that might have textual, RAG or graph resources to answer the call.  

It's interesting but also infuriating that there are so many approaches and the ""best"" approach changes almost weekly.  It's still very early in AI development so I guess I would say don't freak out about learning one thing as it will likely change, but definitely do get into working with AI is future work will depending on it.  It's not going away.",5
1qha2l9,4,"Sounds like GraphRAG rebranded, tbh.",1
1qha2l9,5,"Old tech rebranded with new application. Tradeoffs, just like any other.",1
1qha2l9,6,"Context graphs are the apex predator of the modern data stack. By leveraging a hyper-scalable, multidimensional architecture, they unlock the latent potential of your unstructured data silos, transforming static touchpoints into a living, breathing ecosystem of actionable intelligence.
The Strategic Pillars of Context Graphing
 * Semantic Interoperability: We‚Äôre moving beyond flat-file legacy systems into a graph-native paradigm that facilitates seamless cross-functional transparency.
 * Predictive Synthesis: By mapping the relational DNA of every node, your enterprise can achieve a 360-degree holistic view that is both proactive and boundaryless.
 * Cognitive Agility: This isn't just about connectivity; it's about context-aware liquidity that empowers stakeholders to pivot at the speed of thought.
> ""To win in the post-digital era, you need to stop managing data and start curating relationships via a robust, cloud-agnostic context fabric.""
> 
Essentially, it's about disrupting the status quo to drive exponential value-add through a unified, high-fidelity neural map of your entire business gravity.",1
1qh9fwt,1,"Huh? ... ok ... hmm ... cool actually good, geez thanks for the link!",2
1qh9d0d,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qh6wo1,1,They don't want you working in their systems by the sounds of it,36
1qh6wo1,2,"What I learned from freelancing in enterprise: when the business model already works, the incentive is to prevent further changes, not to enable change.

Looks like you won't be changing sh\*t on that project.",10
1qh6wo1,3,Take the W and just work as well as you can. You're getting paid a nice day rate.,23
1qh6wo1,4,"Some of the tools on the GDE have modal interfaces that are too big for the GDE screen resolution with critical controls off screen.
The Tab key doesn't help here either",8
1qh6wo1,5,"I had that happen to me once, the interface they gave me such low res that you could barely read the fonts, so I went into the MONITOR settings and adjusted the resolution and make it 150x better. When my manager came over she nearly had a heart attack, she was furious. I thought I was fired. But she warned me .. touch nothing, make nothing better. Working in a corporate environment is quite different.",4
1qh5bo7,1,adlog,3
1qh5bo7,2,"Standardizing decentralized data pipelines at scale is always a challenge‚Äîso many moving parts, especially when teams or regions are semi-independent. Curious to see if Vinted leans into open standards, a strong internal platform, or something more bespoke. Anyone know if they use a single orchestration layer like Airflow or if they stitch together different tools per team?",1
1qh4sp0,1,"I'm in a team that built everything on AWS services, with similar amounts of incoming data.

It was fine at first. As long as everything was simple with a single region and incoming product, and a few people had been working on it and had direct experience with how everything was done, then the 'quirks' were kept to a minimum and everyone knew them.

Then as new team members got onboarded things got harder. People had to be taught all the quirks of which role to use when creating a glue job vs an interactive notebook, they had to be shown the magic command boilerplate to get glue catalog and iceberg tables working, they needed to know the bucket that was set up for output for Athena queries. With more people working not everyone could be across everyone else's work, so people weren't familiar with how various custom jobs and scripts had been made, and because each job was its own mini vertical stack there was a lot of repeated components in infrastructure, policies, ci/cd scripts.

As new use cases came on that didn't fit the mould new ways of doing things had to be added. Kinesis and firehose come in, airflow orchestration gets tasked for some small transforms while others go to glue jobs. Someone wants a warehouse/database to query, so redshift is added. Exports to third party processors are needed as are imports, so more buckets, more permissions. API ingestions are needed so in come lambda functions, with each one coded and deployed differently because nobody can see what everyone else is doing.

Then finally users need access to data, and the team just isn't set up for it. There is no central catalog with everything, it is spread out across half a dozen services, and the only way to know where anything is or goes is to dig through the code. That 'worked' for the DE team, since they were the ones doing the digging, but there was no effective way to give access to everything. Every request for data took days or weeks to finalise, and often required more pipelines to move it to where it could be accessed.

We're moving to Databricks soon. It gives a unified UI for DE and other teams to access the data, you get sql endpoints, you can run basic compute on single-node 'clusters', it has orchestration built in, it gives you a somewhat easier way to manage permissions, and it works for both running your own compute and giving data access. Instead of a mishmash of technologies that don't make a unified platform, you get a consistent experience.

You'll just have to pay extra since it is doing a good portion of that unification work for you.

If you had a hundred DE type roles it might be more cost effective to stick with base aws services, and have a dedicated team focused on dx, standards, and productivity, to cut out the managed compute cost. But if you're just 3 people, you're probably not there.",42
1qh4sp0,2,"Full disclosure: I work at Databricks.

One of the benefits of Databricks is that your engineers don't have to do the work of stitching all of those AWS services together. This is especially relevant for you since you're such a small team. Databricks will allow your team to spend less time on infra work and more time building your data  lakehouse.",21
1qh4sp0,3,"Everyone suggesting Databricks here. I‚Äôm part of a company where we chose to build our Lakehouse 5 years ago on AWS.
We have an amazing solution with around 10 platform engineers, 50 engineers working on the platform and 20k end users.

We recently did a full analysis of total cost (cloud costs and payroll) of using Databricks vs our Platform and we are definitely much much more cost effective.

That said, it required great leadership and product vision and it works because we‚Äôre a big company with specific needs that were not answered by Databricks at the time - for example when Iceberg was first out we went all in meanwhile Databricks kept saying it wasn‚Äôt their priority and pushed delta lake.

Now I would say Databricks is so easy to get into and has improved so much over the years‚Ä¶ if we had to start now I think Databricks would be the go to",7
1qh4sp0,4,"I am in the Data Science/MLOps, but tend to work with Data Engineers. Take my opinion with some level of salt. I work on the Databricks platform full time. I have experience with spark, orchestration, A.I. Agents, etc. 

Databricks can do everything you listed. It is not the best in each category. Airflow is a superior orchestration tool than Databricks Workflows, for example. However Databricks provides tools in all categories that are more than good enough.

I find myself surprised how easy most things are. I usually  do a POC with a G.U.I. first.  Then reimplement with code, checking against the G.U.I. POC as I go. There is a G.U I. for Databricks Workflows and A.I. Agents. In general, everything seems to be as easy as possible, with good default settings. 

The advantage of having good enough tools in broad categories that all integrate well with each other makes life easy.

Measured against AWS, Databricks is more expensive. Your company can either pay more for compute ( go with Databricks) or pay more to expand the team with specialized people (AWS). Expensive compute is cheaper than expensive specialists.

Plus Databricks is moving to everything runs on serverless. In practice, I would say 80% to 90% of prod code runs on serverless. I see this improving with time.

Closing remark. Databricks is a thought leader. They have made many open source software that everything else is compared to. (Spark, MLflow, DeltaLake,...). Databricks competitors run Databricks open source software. Agent bricks hosts 20+ LLMs out of the box. I don't know what the future holds, but I bet Databricks drives it.",4
1qh4sp0,5,I‚Äôm at about this point too. Signs point that dbx is the better solution and will enable developer velocity more.,5
1qh4sp0,6,Does databricks really do all of those micro services in one? I'm close to AWS de cert but my local area is all azure,3
1qh4sp0,7,"I've been working for the past 5 months with Databricks, and I think it's better for this scenario.

\- Lakehouse Architecture (no need to have S3 AND Redshift)

\- Delta Lake with ACID transactions, Time Travel, Schema Enforcment / Evolution, Z-Ordering, etc

\- Spark Declarative Pipelines for ETL

\- Databricks Jobs for orchestration

\- Unity Catalog for governance

\- Dashboards for reporting

\- Lakeflow Connect for connecting to multiple data sources, with built-in connectors

\- Delta Sharing for sharing data externally

\- ML and GenAI Features (I haven't worked with this yet)",3
1qh4sp0,8,"I built up a data lakehouse architecture starting 5 years ago and we had to stich AWS services together. Which worked well and performance was inline what we needed. We used all of the technologies you listed except data tone and only a bit of lake formation. Then we wanted to have a business facing data catalogue with lineage and there wasn‚Äôt much available in AWS. As of 2026 I would argue that is still the case. For data catalogue we used OpenMetadata. For hosting we used EKS btw.

Long story short it takes a lot of development effort to put all of these technologies together and to maintain it. It works well once done and it is scalable. 

But if I would do it again I would use Databricks in 2026. Gets you started quickly. Downside will be cost as you have to pay for DBUs on top off AWS cost.  Arguably you would need less data developers which could pay for the additional cost.  I know this is a bit off a sticking point nobody wants to think about. But if you plan to use your devs to build the AWS solution then you already covered the dev cost. If you go with Databricks you may need less data devs, assuming that some part of their time is currently used for infrastructure work. If the new AWS architecture would need more devs than the argument would be that Databricks would allow you to operate with the current number of devs.

If you decide to go with databricks use an open source table format like iceberg to reduce vendor locking. Also bear in mind, databricks can support all of the requirements you listed, but is not best in class for all of them like BI, orchestration, gen ai etc. the core is around spark and related services.",2
1qh4sp0,9,"If you want to move faster, go with Databricks by leveraging Native lake house(delta, streaming, governance) that's all integrated. It comes with much less Ops burden, and Unity Catalog is far simpler than Stitching together Lake Formation, Glue, IAM. However, it comes with a price: vendor lock-in, and it can get expensive if you don't control clusters well.
If you choose to DIY on AWS, you'll need to spend a lot of time maintaining instead of delivering because of high operational overhead. AWS self made stack makes sense only if the platform engineering is strong. But for small team with high volume, Databricks is good choice from my POV",2
1qh4sp0,10,"I feel with the advent of agentic programming, infrastructure is a solved problem. If you architect it correctly and follow devops best practices with modular iac, good documentation, guard rails, you should be fine with AWS. Keep it simple.",2
1qh4esu,1,You created Entire UI to interact with the Events ?,1
1qh2653,1,Just don't. For quite some time it's not a software company but a Bitcoin ETF,4
1qh2653,2,"Yeah tried it a while back when it was still MicroStrategy. the mosaic semantic layer is powerful but definitely not plug and play took some time to model cleanly. solid once it is set up though, especially for complex bi setups.",1
1qh2653,3,Used it for years and actually liked the semantic model and object design (rest was trash).  There should be an option in the report menu (I think under edit) when you run it that shows the SQL it came up with to get the results.  Maybe you can debug from there?  We set it to outer join everything in case some pipeline broke and we got referential integrity errors.,1
1qgzdju,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qgzdju,2,Your SSIS skills apply directly to Azure Data Factory. Do Microsoft Learn's DP-203 modules first. Skip random tutorials.,3
1qgyrpt,1,"Yeah, this is a really normal point of confusion when you move from DA work into ingestion side DE. The big mindset shift is that schemas tell you how tables relate, but they don‚Äôt tell you how data changes. Streams only show what Snowflake sees as new or updated rows since the last run, so if the source system quietly edits an old record, you‚Äôll never see that unless you deliberately go back and reprocess it. That‚Äôs why a lot of pipelines use a lookback window (reloading the last N days every run) or just full refresh smaller tables. If the source doesn‚Äôt give you a reliable ‚Äúlast updated‚Äù field or CDC feed, you kind of have to assume old data can change and design around that. For tables without dates, truncating by time doesn‚Äôt really make sense. Those usually get merged by a stable business key, or fully rebuilt if they‚Äôre small enough. The ‚Äúforked‚Äù tables usually follow the lifecycle of the parent record, not the ingestion date. In general, when you‚Äôre trying to understand ingestion, I‚Äôd ask ‚Äúhow does this data change in the source system?‚Äù rather than ‚Äúhow do these tables join.‚Äù Once you answer that, the truncate vs merge decisions start to feel a lot more obvious",3
1qgy9rx,1,"We've been working on a huge migration lately at my company and we very soon realized that row by row validation ia impossible. What we settled on was the following:
- ensure schema is the same
- ensure partitions are the same
- ensure every number column max and min coincide
- ensure every date columns max and mins coincide
- ensure the sum of relevant metrics coincide (works ONLY for non-negative and non-nullable number columns of course). You can think about performing this sum for every partition separately for a more fine grained validation

I hope this helps!",24
1qgy9rx,2,Could you possibly output the source catalogue data to parquet and compute the hash signature and do the same for the target?,5
1qgy9rx,3,If its just catalogue then it shouldnt impact the table. Im confused why this is even a worry. If you doubt the table there is underlying sources you need to check,2
1qgy9rx,4,Ensure row counts match. Perform spot checks for accuracy and completeness. Not sure how you would go about fully validating 30 billion records honestly.,2
1qgy9rx,5,"Are you just trying to be confident they're the same or do you need 100% proof?

I'll throw this idea out there.

1. Create the new table by running a `DEEP CLONE` on the original table.
2. Run `DESCRIBE HISTORY` on both tables. 
3. Check that the tables each have the exact same version history. 

If two tables have the exact same changes throughout the life of the table is that good enough for your purposes? As /u/Firm-Albatros said, I'm confused why this is even a worry.",2
1qgy9rx,6,"A little late to the game here but I've used SUM(ID\_column) between some suitable dates on a few occasions to ensure that the correct ID's are in place. If they are, then chances of the rest of the column data being messed up are slim. Running row checksums on billions of rows will probably be...time consuming.",2
1qgy9rx,7,Please provide more details what your validation notebook contains.,1
1qgy9rx,8,"Create traceability. An additional table that would show for every record that successfully migrated the values in both source and destination tables, you can also add a column for transformation logic if ¬†that is applicable. This table should have the exact number of rows as the source and destination.¬†",1
1qgy9rx,9,"I have this idea:
pick a metric field x with high cardinality values
then for every dimension run a sql query to sum/avg that field x aggregated by that dimension and compare results with the same query results from the original table.

basically you will be comparing this table
dimension Y, sum(x), avg(x), count(*), any other agg function.
for every dimension that you have one by one.

this will eliminate the statistical possibility of migration issues.",1
1qgv0fv,1,"As an FYI, MinIO entered maintenance mode.  https://github.com/minio/minio?tab=readme-ov-file#maintenance-mode",5
1qgv0fv,2,"Ducklake?


Its limited concurrency, but easy to setup catalog which manages filestores.",2
1qgv0fv,3,Maybe not. But for a personal project.. maybe its good enough.,1
1qgv0fv,4,ETL is going to be your difficult one to find. Unless you code it with python or something but tools make it so much easier,1
1qgv0fv,5,sounds like a great stack to me.  check out Stackable who have k8s operators for all (or most) of what you are wanting. they have some demos showing how to couple multiple operators together to get the set up you want.,1
1qguef9,1,"Most teams either pin to a specific installer version or bake this into an AMI so bootstrap doesn‚Äôt depend on external links at all.
Storing installers in S3 works, but it adds maintenance. A simpler option is like downloading directly from the vendor and version pinning so it fails fast if something changes. If this is production critical, prebuilding an AMI is usually the cleanest approach",3
1qgl0av,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qgl0av,2,Sure. I‚Äôm 44 and I‚Äôm a first time data engineer!,71
1qgl0av,3,"57 my new title as of November is Senior data engineer. Mostly python, dbt on bigquery.  Obviously working adjacent in the past as a lead engineer and experience with data and programming over the last 30+ years.",30
1qgl0av,4,"I worked in QA for over a decade before transitioning into Data Engineering in 2021. It‚Äôs a competitive field and entry-level roles can be tough to break into, but with genuine interest and consistency, the switch is definitely possible. One good approach is to first move into Data Analytics within your current company and then transition into Data Engineering from there. Good luck!",15
1qgl0av,5,"I switched much later, near 46. You‚Äôve got a lot of time still. It‚Äôs just a job title. You‚Äôre likely doing the same stuff now",10
1qgl0av,6,Im 44 when I got my GCP Data Engineer role,6
1qgl0av,7,"As a data leader with an extensive technical background myself I have interviewed many data engineers and scientists. Here is my advice:

If you want to break into this field you need to take your studies very seriously. In the age of AI your coding skills alone are no longer enough. You need to be able to help the business architect solutions to data problems. Furthermore if you are not knowledgeable on architecture design you probably won‚Äôt land a senior role at all. Those who stand out the most to me are knowledgeable on modern tools but more specifically the value they provide. Those who standout the least cannot explain how to design a data solution. I think it‚Äôs a highly rewarding field with opportunity for growth but you have to be ready to work hard, especially when it comes to continuous learning.",16
1qgl0av,8,"I see a lot of comments on how easy it is to switch. You can pivot yes but it would still require a lot of efforts. Also depends on which country you are targeting for, I‚Äôve seen companies in Europe are much more flexible for these things than US/India. Not to forget going back to entry level roles would require you to take a pay cut.

Regardless, it is possible yes, but you can start finding similar roles in your current organisation and pivot to make it easier.",4
1qgl0av,9,I moved at the age of 40 ü§≠,3
1qgl0av,10,"Q: Is it a good decision to make this career move at the age of 38?  
A: Age is irrelevant.

Q: What kind of roles should I target?  
A: Junior/Mid/Senior is completely different at different companies. At some orgs you might qualify for mid-level, at others you might not get a look-in at beginner. You're very unlikely to be Senior anywhere. Just go for roles that meet your requirements.",3
1qgdliv,1,"\>,,< seems eerily similar to CS431 assignment‚Ä¶ good try! (edit:typo)",1
1qgdliv,2,"For a first streaming project, that‚Äôs actually a good choice. At a high level, think of the pipeline as a flow rather than a bunch of tools. Your Python service pulls data from the carpark API and publishes events as they arrive. From there, you usually want some kind of streaming or messaging layer to decouple ingestion from processing, so bursts or failures don‚Äôt break everything. Next comes processing, where you clean the data, handle duplicates or late events, and compute whatever metrics you care about (occupancy, availability trends, etc.). This can be done in near-real-time or small micro-batches depending on complexity. The processed output then lands in a store that‚Äôs optimized for querying or visualization, which your frontend reads from. The key ideas to focus on early are idempotency, schema consistency, and what ‚Äúreal time‚Äù actually means for your use case (seconds vs minutes). Start simple, get something end to end working, then layer in streaming frameworks, windowing, and fault tolerance once you understand the flow",1
1qgcog8,1,"C, A, and then B only if your role deals with architecture or if you want your career to go in that direction.",12
1qgcog8,2,"If you‚Äôre like new to DE, I wouldn‚Äôt read them cover to cover in one go. They serve different purposes. I‚Äôd start with fundamentals of data engineering to get a broad mental model of the field and the vocabulary. Then read The Data Warehouse Toolkit selectively focus on dimensional modeling and skip deep dives that won‚Äôt make sense yet without real projects. Designing Data Intensive Applications is excellent, but it‚Äôs more of a ‚Äúwhy things work this way‚Äù book, so it lands better once you‚Äôve built or broken a few pipelines. Zoomcamp will honestly teach you more faster than passive reading, so I‚Äôd prioritize that and use the books as references alongside it. Build something, hit a wall, then read the relevant chapters that tends to stick much better than linear reading",6
1qgcog8,3,"FYI B & C are available on Spotify as audiobooks (included with paid subscriptions)¬†

I found it useful to listen to them to get an overview, but then also have the physical book as a reference",3
1qgcog8,4,"I'd recommend starting with c then a. It gives you such a good overview about the field. b is a little different than other books since it doesn't focus on the data engineer's work especially when you are starting your career. It's more about how to think about systems, design decisions, tradeoffs, it primarily focuses on *architecture* of data systems and the ways they are integrated into data-intensive applications",2
1qgcog8,5,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qgcog8,6,"I bought all these books several years ago and have skimmed them. I‚Äôve read C the most. I work for medium sized manufacturing firm and I am by far the most ‚Äúdata engineer‚Äù type of guy but it‚Äôs just not  that deep per the content in these books (it can be if I had the resources to)! We do have plant systems that sit on time series historian. I can pay a few k for a class on that. I‚Äôm also big on erp, costing and financial reporting (P&L / B&S) a business technology generalist. I constantly struggle with delving into the path of those books versus just keeping one foot in the cfo and IT circle.",1
1qgbu67,1,"Vscode. Also for testing, but i dont want notebooks and use .py files.",39
1qgbu67,2,"VS Code for most development and occasionally Marimo when notebooks are needed.

Jupyter required too much overhead to continue with it.",17
1qgbu67,3,"I use VS Code exclusively for notebooks. I work on a bunch of non-Jupyter projects (Python services, scripts, Terraform, SQL/dbt, etc) and doing everything in VS Code is a lot easier. I get the same linting, autocomplete, keybindings, etc. And now with LLMs, nice to have the assistant panel in VS Code and Cursor.",7
1qgbu67,4,"I‚Äôm using DataSpell IDE, it‚Äôs paid, but good for notebooks and to work with data in general",4
1qgbu67,5,i have started using marimo.,7
1qgbu67,6,"Neither. nbclassic. I have never tried with VS code, but much prefer the simpler jupyter notebook server over the full Jupyter lab bundle. Fyi you can use iPython.display to change the width of the cells from within the notebook so you aren‚Äôt stuck with just adjusting browser zoom.",3
1qgbu67,7,"ipynb files in vscode/cursor. easier llm integration for me to ask.

and better venv management with UV (i have a script to spin up jupyterlab session in 1 command)",2
1qgbu67,8,Jetbrains. Usually PyCharm or DataGrip.,2
1qgbu67,9,VS Code‚Äôs Jupyter setup is bloated. Edit settings.json to cut padding-most waste comes from defaults. JupyterLab still wins for pure notebooks.,4
1qgbu67,10,"IMO if plot zooming and such is what you‚Äôre most interested in, just run Positron (VSCode fork). They took some of the helpful UI from RStudio around plots and charts and added in interactive code execution (think: highlight a block and run it) which gives you 95% of the Jupyter functionality people actually use with a fraction of the pain. Plus it‚Äôs backed by a strong foundation.

Feels to me that‚Äôs pretty close to what you‚Äôre looking for.",1
1qg4ve4,1,"Architecture, how different pieces of software interact, what are the coponents of production software that should be present in a robust application, etc.",7
1qg4ve4,2,"I recommend focusing on three things beyond just coding: understanding architecture & systems, how apps interact and stay strong; keeping up with DSA & problem-solving; and building practical projects with version control, testing, and CI/CD. Even in the era of vibe coding, being able to design, build, and ship a solid app sets you apart.",1
1qg2t2x,1,"Hey there, imo, why not try generating your own synthetic streaming data? You‚Äôll get much more hands-on exposure, and it‚Äôs actually fun to work with.

You can also try implementing dbt, it fits really well with the Medallion architecture and tools like Dagster (for orchestration). With dbt and Dagster, you can easily write test cases and handle data quality checks.

For monitoring and alerts, I‚Äôve found Grafana to be free, simple, and effective to set up.

I can‚Äôt comment much on your architecture since it depends on your data and use case. If Medallion fits your needs, that‚Äôs fine, but you can also experiment with Kimball or a hybrid approach to understand trade-offs better.

Do let me know, how things went at the endüôå",3
1qg2t2x,2,"That‚Äôs a very solid homelab setup already. For datasets, NYC Taxi (Parquet versions) is still great for medallion style practice, especially if you downsample by date or vendor. TPC-DS generators are useful too, but they can feel a bit artificial compared to semi-messy realworld data. You can also look at public datasets from places like AWS Open Data or Google BigQuery public datasets and just pull subsets under 100GB.

Medallion isn‚Äôt the *only* layering pattern, it‚Äôs just the most common mental model. Some teams add things like a ‚Äúraw but validated‚Äù layer, or domainoriented marts instead of a single gold layer. The naming matters less than being clear about guarantees (schema stability, data quality, latency).

For monitoring, beyond infra metrics, it‚Äôs worth looking into data-quality checks at the Spark/Iceberg level (row counts, freshness, null checks). Tools like Great Expectations or simple custom checks in Airflow go a long way in a POC. Scaling pain usually shows up first around small-file problems, metadata growth, and orchestration complexity rather than raw storage cost.

Honestly, you‚Äôre already doing the right thing by focusing on practice and observability early that‚Äôs where most realworld lakehouses struggle.",3
1qg2t2x,3,"Nice lab! For data, just pull something from Kaggle or gov data. Medallion's fine to start, but often simpler is better, or more granular zones if your data needs it. Beyond Netdata, Grafana/Prometheus works for ops, and Great Expectations is legit for data quality. Good luck!",2
1qg2t2x,4,[https://divvy-tripdata.s3.amazonaws.com/index.html](https://divvy-tripdata.s3.amazonaws.com/index.html) or [https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data](https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data),2
1qg2t2x,5,Look no further for free datasets [https://opensource.googleblog.com/2026/01/explore-public-datasets-with-apache-iceberg-and-biglake.html](https://opensource.googleblog.com/2026/01/explore-public-datasets-with-apache-iceberg-and-biglake.html),2
1qg2t2x,6,DuckDB can generate TPC-H example dataset of various sizes locally: https://duckdb.org/docs/stable/core_extensions/tpch.html,1
1qg2t2x,7,"I would avoid using the minio, the project died, Here we are using Ceph, but you find lighter options. I would also avoid the hive data catalog, You can find better catalog solutions, for iceberg there is Polaris and unity data catalog oss.",1
1qg2qf4,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qg2qf4,2,"Not directly, the APIs are too different. If you are lucky the actual magic is abstracted away from the specific processing framework and therefore portable.",2
1qg2qf4,3,"Well, if it's Hadoop than you'll be able to run MR jobs. And rewriting them into Spark RDD shouldn't be a problem",1
1qg2qf4,4,"My Short answer to this would be not really, at least not without changes. Spark and classic MapReduce look similar conceptually, but they‚Äôre different execution models and APIs. Spark doesn‚Äôt natively run old MapReduce jobs the way YARN used to, so there isn‚Äôt a true ‚Äúdrop in‚Äù compatibility layer. In practice, teams usually rewrite or wrap the jobs. For simple map ‚Üí aggregate ‚Üí write patterns like you described, the migration is often straightforward in Spark (even more so if it‚Äôs just daily aggregations feeding Hive tables). The bigger friction points tend to be custom input/output formats (like Protobuf), assumptions about reducers, or job-level configs that don‚Äôt translate cleanly. If zero code changes is a hard requirement, that‚Äôs a red flag you‚Äôd need classic MapReduce support somewhere. Otherwise, the realistic path is incremental rewrites: validate the Spark output against the existing Hive tables, then retire the MapReduce jobs one by one. Most teams find the effort manageable once they accept it‚Äôs a migration, not a switch flip",1
1qg2hus,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qg2hus,2,"If you enjoy building data pipelines, designing systems, or architecting solutions, Data Engineering is probably a good fit for you. DE usually doesn‚Äôt get boring, there are a lot of tools to work with, depending on the company, data volume/variety/velocity.
Don‚Äôt make a decision based on comments or content alone. Build something, spend some time on it, and see if it works for you. All the best üôå",9
1qg2fz2,1,"This is a really clean idea. Walking a dataclass to drive an interactive input flow feels like a nice middle ground between raw input() calls and going all-in on a CLI framework. The validation and normalization steps make a lot of sense, especially rejecting schemas that would create confusing UX rather than trying to ‚Äúsupport everything.‚Äù The session graph + undo model is also a thoughtful touch, that‚Äôs usually where ad-hoc input tools fall apart. One thing I‚Äôm curious about is how you see this evolving in terms of non-interactive use (e.g. prefilled defaults, config files, or replaying a session). Overall though, this feels very well scoped for internal tools, which is probably why the design reads so clean",2
1qfznzw,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qfznzw,2,"I am at the same stage, from BI to DE. I had my time deploying models to manufacturing production, I think DE has a good lens in becoming an architect.

Since u have great YOE, I assume u had fair experience with SQL jobs. What I am doing now is catch up on fundamentals, u will realized ppl here are quite tool-centric. But as starters, just pick one ecosystem. And move to the bronze layer side of things, think of design engineering decisions. Cost-performance trade-offs, synergy with your current companies infrastructure, don‚Äôt overengineer over a futile problem. Think what your fellow analysts complain about data readiness, availability, latency, etc. 

Project-wise, I am starting a new project myself that have analytics served as use case for parliamentary discussions. How can typical citizens provide insights where government can evaluate more effectively than just a simple survey form. 

And as you move up to the tech chain, there‚Äôre security, compliance, more financial constraints, software engineering discussions.

Many possibilities! But gonna be an exciting ride",3
1qfznzw,3,"based on what you just wrote, data engineering sounds like a fairly natural next step for you. A lot of people who get bored in senior analyst roles feel that same shift, less enjoyment from ‚Äútelling the story‚Äù and more from building the systems and models that make everything work. If you enjoy data modeling, Python, and building tools that reduce friction for others, that‚Äôs already a big piece of modern DE work. Many DE roles today are less about pure infra and more about enabling analytics at scale, which fits your background well. You‚Äôre not starting from zero, you‚Äôre just moving closer to the plumbing. On the ITIL / process side yes, it‚Äôs more structured, and that can be frustrating at first. The upside is fewer fire drills and clearer ownership the downside is slower change and more ceremony. People coming from ‚Äúfast and loose‚Äù teams usually struggle with pace more than the work itself, but most adapt once they see the stability benefits. As for frustrations, you‚Äôve already named most of them unclear requirements, data quality issues, tribal knowledge, and legacy decisions you inherit. The flip side is that fixing those things is often exactly what makes the job satisfying. If building and improving systems is what energises you right now, the secondment sounds like a low-risk way to test whether DE clicks long term",1
1qfxoqp,1,"A manager / boss that knows his stuff would. 

Not any HR because they play word bingo. 

So either white lie, add text like, this is the equivalent to X years on AWS and GCP.

Or go through an agency.",3
1qfxoqp,2,I was heavily experienced in GCP. Azure and AWS shops would not consider me. Not much else to the story,1
1qfxoqp,3,"Yes, they usually will, especially if your experience is solid. Most companies care more about cloud concepts than the logo on the service. If you understand things like networking, IAM, storage, data services, and how to design pipelines, that knowledge transfers pretty well between Azure, AWS, and GCP. That said, you‚Äôll have a better shot if you can map your Azure experience to the equivalent services (e.g. how what you did in Azure would translate to AWS/GCP). Some teams may still prefer direct experience, but plenty are happy to hire someone strong in one cloud and let them ramp up on the others",1
1qfv0iu,1,"i think basic SQL like joins, window functions should work maybe some libraries of python, some of ADF stuff how to view logs and some situation based stuff should work and also they may ask some stuff related to KPIs.",1
1qfv0iu,2,"You‚Äôre understanding the role correctly. ‚ÄúData Solutions Analyst‚Äù is usually a hybrid between a data analyst, light data engineering, and some business analysis. From that job description, they‚Äôll likely focus a lot on SQL (especially T-SQL), basic data warehouse concepts, and how you turn messy or vague business questions into usable insights or dashboards in Power BI.

If there‚Äôs an assessment, it‚Äôs often a SQL task or a small data interpretation exercise. On the interview side, expect questions about how you handle unclear requirements, explain numbers to non-technical people, or deal with data that doesn‚Äôt match expectations.

Also, feeling underqualified is pretty normal for this kind of role. They usually care more about solid fundamentals and how you think through problems than knowing everything upfront",1
1qfumps,1,"You can find our open-source project showcase here: https://dataengineering.wiki/Community/Projects

If you would like your project to be featured, submit it here: https://airtable.com/appDgaRSGl09yvjFj/pagmImKixEISPcGQz/form

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qfumps,2,"Check out the free tier. You can probably build a couple decent portfolio projects without spending a dime that way, just be careful. You could also start locally using the Azurite storage emulator (blobs, tables, queues) and the Azure Functions runtime. With this setup you can build simple event driven systems triggered by storage events, queue messages, etc. I will say it can be a bit tricky to get set up. The docs are decent, but there are some ‚Äúexercises for the reader‚Äù that go unmentioned.",5
1qfu4xb,1,Pro tip: Map their deployment stages to your data dependencies. Silos = missed SLAs. And get their release timelines documented-nothing worse than DE blocking a security patch rollout.,1
1qfrqq5,1,"Yeah, that‚Äôs fine and actually pretty common. Once you‚Äôre at a reporting / ‚Äúplatinum‚Äù layer, the goal is usually stability and clarity rather than abstraction purity. If those models are meant to serve BI tools or end users, having static table references is often intentional. The key thing is consistency and contracts. As long as the upstream silver/gold models are well-defined and versioned, referencing them directly doesn‚Äôt really break dbt‚Äôs model it just means you‚Äôre treating that layer as an interface rather than something that‚Äôs constantly reshaped. Many teams do this to avoid unnecessary rebuilds or to keep reporting logic simple. The medallion naming matters less than being explicit about what guarantees that layer provides (schema stability, freshness, ownership). If your ‚Äúplatinum‚Äù layer is basically a semantic/reporting layer, static references are usually a feature, not a bug",2
1qfpglp,1,Landing is part of bronze. Bronze as a layer can consist of multiple stages. If you write a stream to delta with append then its a single stage. If you have parquet files over some transfer protocol and then load to delta jt has two stages. Bronze just denotes its raw data.,45
1qfpglp,2,"Yes we have a raw layer, autoloader reads from it and appends to bronze , then cleaning and deduplication happens in silver",15
1qfpglp,3,My general rule is if the data is being ‚Äúpush‚Äù it goes into landing first and then I ‚Äúpull‚Äù it into bronze. This is true of even other parts of the solution where we need to use third party tools to get data from source systems. Even though they are part of the overall solution it is pushed to landing first.,4
1qfpglp,4,"Landing is wood layer. Which then loads audited rows to tin layer, and then bronze, silver, gold (don't forget to update data vault in Titanium), and mithril.",3
1qfpglp,5,"Landing 

Staging

Business Logic

Final Tables

Views

Business views

Now lump them together into 3 stages and you got medallion architecture. 

Names don't matter. People want to sound fancy and introduce new names for age old things. We don't need to remember everything, but we need to know what's the use of the underlying mechanism",4
1qfpglp,6,"Sadly we have a landing zone for files, a choice that bites us constantly but the requirement was to keep file deliveries.


I prefer to extract directly to delta, instead of managing bad loads.",2
1qfpglp,7,Landing for raw files then tables. Both part of bronze.,2
1qfpglp,8,"To me they're the same.

Last place I worked at had raw and standardized buckets, then data was loaded into the data warehouse where further transformations were done.

Personal project is also kinda doing that. I process my raw files a little bit as dataframes because it's easier for certain logic than to do so in pure SQL, but that's all silver, as it were.

Bronze is my raw data.

Maybe that's the wrong idea though since the dataframe- and SQL-based transformations are explicitly decoupled, but idk.",2
1qfpglp,9,Data lake is bronze but you can keep the raw files,1
1qfpglp,10,It depends how you name them. For me I'd have landing as 1:1 copy of the source. Bronze would be all the basic feature and cleaning.,1
1qfoxst,1,"For me it‚Äôs quite often to all of the above. Maybe 10-15hrs in meetings. I wouldn‚Äôt call them interruptions, it‚Äôs usually resolving/debugging things on call",1
1qfoxst,2,"Me: 35-40 hrs per week (director)
My team: 5-7 hours/week. More if you are a SME¬†",1
1qfnogc,1,"If the client mainly cares about ‚Äúpapers‚Äù, then yeah AZ-203 + DP-700 is a reasonable combo. AZ-203 gives you general Azure credibility, DP-700 lines up better with actual DE work. Non-DE certs only helped me when they matched what I was already doing. DevOps stuff was useful once I started owning pipelines and deployments. AI certs looked nice on paper but didn‚Äôt really change my day to day. Certs won‚Äôt make you better overnight, but they do help with client optics and billing, which sounds like your main constraint anyway",3
1qfmrd6,1,But why? Isn‚Äôt their whole product pitch ‚ÄúOLAP at the speed of OLTP‚Äù? Curious what use cases this aids.,21
1qfmrd6,2,"only makes sense when you see

1. NVMe performance

2. Availability guarantees",2
1qfmrd6,3,Great job!,2
1qfmrd6,4,Nice,2
1qfjxea,1,"TBH, the dbt approach is what a lot of companies follow.  If anything there is more use of jinja in models to make the SQL dynamic, but at its core it's basically the approach they laid out which is just writing CTEs.",13
1qfjxea,2,why do you think the dbt examples from their documentation are not good enough?,5
1qfjxea,3,"The gitlab data team guide ia pretty nice. Learnes a lot from reading it. Not only on dbt standards but also setting standards on ways of writing ELT, ingestion orchstration etc.",4
1qfjxea,4,Clone enterprise DBT repos from GitHub. Reverse-engineering production models will teach you more than any tutorial about real-world implementations.,3
1qfj8hg,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qfj8hg,2,"Non negotiables in Data Engineering as of today: SQL, Python, Apache Spark or Databricks, Cloud Computing(AWS,Azure or GCP), Apache Airflow, One Cloud Datawarehouse(Snowflake, Redshift, Synapse or BigQuery).

This should be more than enough to start giving interviews. Other tools and technologies are good to have!",7
1qfj8hg,3,I think you are still early in your career and can take this risk. Learn DE and get hands on projects in your resume. Get into learning with 1:1 mentors if required and do it. Connect if any issues!,3
1qfj8hg,4,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qfir7d,1,"Sounds like you're looking for something like a backend web dev position. Python is not as dominant in that area as it is for data jobs. I'm sure Java has more market share.

But also, some companies cast a wide definition of data engineer. It's not all ML pipelines. If you're like, building the API that serves user data to an app, that's data engineering.",30
1qfir7d,2,"Yeah Python isn‚Äôt as dominant a backend language anymore unless you‚Äôre in the data space. 

Most places that do webdev are doing TS/Java/Dotnet/etc. That said Python used to be way more popular for backend and there‚Äôs lots if legacy and senior python roles to maintain those systems.",4
1qfijmf,1,"You can hide any tech you want to learn behind an AI initiative. 

‚ÄúI want to create an MCP Server for our data so execs can interact with it, but I need to leverage this new tool called DBT to clean the data into medallion architecture‚Ä¶‚Äù",5
1qfijmf,2,"SMBs are usually the easiest to centralize all info to a single DB platform, because not too many software vendors and maybe only one silo per department. 

However they often don‚Äôt have the required budget. Needs leadership from the CEO, or else, I‚Äôm out of there after one sit down. Only one of CIO or CTO or CFO is not enough. 

Silos are the bane of our industry and they are too easy to build. 

However the silos are wonderful if they are streamlined and working, all I do is convert each one from the source, centralize, and give them a SQL statement for PowerBI or Excel.",2
1qfgxsh,1,"Never. I have Reddit, that‚Äôs enough. LinkedIn is only for tracking former colleagues. Zero participation.",8
1qfgxsh,2,"Are you cold-emailing? Those vendors go on our blacklist. If we didn't give you our contact info DIRECTLY, and you reach out to us through e-mails we didn't explicitly provide to your company, you will never see a dollar of our money.

Goodbye Datadog. Goodbye Manage Engine. Goodbye everybody else who does this. We have enough other options that we can be choosy. Spam us and your sale will die.

By the way. Got a whitepaper but you want our e-mails to read it? Not reading your whitepaper then. Buzz off.",4
1qfgxsh,3,"In generally I'd just happily do it, but i do not get that much outreach as to get bothered by it already. In the cases that i do, if i think it's just a spam bot I'll simply decline quickly, and if I do not think so I'll try to give the right answer. In general people either have a couple of doubts and it's a quick one, or i have a nice convo with someone about the tool/service they are developing!

Im also a founder, so I love chatting with other people with ideas and energy. There aren't so many!",1
1qfgxsh,4,"I appreciate their hustle but there is no chance I'm going to respond. I often don't have the time to respond to contracted vendors and internal emails that are low importance. If I magically had 15 minutes, I'm sure I would use it to go to the bathroom or eat lunch in peace.¬†


The type of headline or email body that would grab my attention in a cold reach out is the same type of content that would make me immediately suspicious of your offering.¬†",1
1qfgxsh,5,It‚Äôs the exact same chance as me buying internet from the jackass who keeps knocking on my door.,1
1qfghbk,1,"Are you seeing a lot of job openings for ""AWS Data Engineer?"" If the focus is on AWS is more likely to be a platform engineer job and that takes a long time to learn. 

If you want to get into data engineering, focus on SQL, dbt, Python, 
and Airflow (In that order). It's the most common skillset and easiest to obtain.",3
1qfghbk,2,"Read in the Wiki here, Learning Resources. What code do you know, what's your coding knowledge / background?

DE requires coding, not just SQL stored procs, though most of it happens here.",2
1qfghbk,3,"have solid foundation of ec2, s3 and vpc and how they interact with each other. thats your starting point. then emr: basically bunch of ec2 in clustered. learn concept of serverless, main selling point of aws. thats all you need to work with as data engineer. other services were exist to make things easier and convenient.",2
1qfghbk,4,"I recently went from a Tableau analytics developer to a data engineer. Much more SQL. Much more Google Cloud type stuff. No front end/dashboard/DAX shit.

Oh and Python. Lots and lots of Python.",2
1qfghbk,5,https://www.youtube.com/live/vQbReDn3GTs,1
1qfghbk,6,"I got AWS certified last year (2025). I would 100% focus on Athena, Redshift and Glue. I have some notes if you want them.

Is it worth it? It depends, I think ir can open some doors in big consulting companies in specific projects on AWS cloud but that's all. I don't think it will give you strong bases for Data Engineering.

I'd rather build a project while reading Fundamentals of Data Enginering or Data Engineering Design Patterns as knowledge resources. Use AI for all coding stuff.",1
1qfghbk,7,YOE?,1
1qfghbk,8,We need more details.,1
1qfghbk,9,"If you are preparing for a Data Engineer role, you should build at least 2-3 end-to-end project.

Build pipeline 

This really helps in understanding concepts and explaining them in interviews.",1
1qfg4ey,1,"If transformation needs very by customer, it may be easier to see how much of this you can offset to analysts. They can create custom views and smaller transformations based on their own needs. Maybe you can consolidate similar transformations into a module that gets called that reduces the overhead of each glue job? Just brainstorming with the information given. Every company has different structure and needs based on a variety of factors. You may be able to design a common mapping file structure that is built by your customers (or by you) that a single glue job pulls in depending on the customer (many different ways to do this). Each of these mapping files would instruct your glue job what to do. Once you have your common mapping file structure you want to use, you can have AI help you make change ls or tweak it on a per customer basis when they ask for changes pretty reliably. Don‚Äôt care if I  get some hate for that comment since it‚Äôs the reality.",1
1qfepkd,1,"I'd spin off by learning technologies that are related to your past experience and current stacks such as Airflow or dbt, rather than learning something totally unrelated like Spark. And learn some Azure Data Factory, just to show that you can do data-ingestion side of works.",2
1qfepkd,2,"Depends where you are and really what size organisation you are targeting. 
If its large scale then yes you are probably going to be heavily into spark, python, streaming etc‚Ä¶

However, small to medium companies can quite easily get by without enterprise solutions. Basic ADF and sql do perfectly well on lower volumes.
In those cases you are more likely to be in a smaller team and I guess need to be able to demonstrate more skills around effective data modelling, orchestration, strategy etc than language specifics.

Tbh in five years DE will probably be in a very different place than today. Personally I wouldn‚Äôt be chasing specialist knowledge on areas that are going to be even more infested with ‚Ä¶ let‚Äôs call it ‚ÄòAI support‚Äô",2
1qfepkd,3,Data engineers also need to become cloud engineers. Don‚Äôt be pigeon holed into a ghetto. Keep learning and expanding horizons and tools and techniques.,1
1qfepkd,4,"Spark is a distributed compute engine, Pyspark is high-level API for working with Spark. Python is an interpreted, dynamically typed programming language. 

Old school rule: To learn any big data framework always begin with Hadoop. Most of the other frameworks like Spark, Kafka, Beam, Flink draw their design philosophies from Hadoop.",0
1qfannn,1,Remindme!,1
1qfadud,1,"There's two absolute worst-case approaches you can take:

  
1) Calculate a hash over the entire row-contents - something like an MD5 or SHA hash ought to give good uniqueness-properties for a unique input. If you \*still\* get clashes, it means there's duplicate data - at which point maybe try  
2) Brute-force a row-number id on load.",74
1qfadud,2,"Write a query to pull out sample duplicate records. Review these with the SME for the source data producer. If they are indeed duplicates and can be discarded add a step to clean them out and then proceed to defining an appropriate PK. 

P.s. If this is event data and there is a unix timestamp field; having the source increase its precision (e.x. seconds -> microseconds)  will resolve duplicates.",12
1qfadud,3,"Definition of PK should come from business imo. Then you can go and detect duplicates, and see what to do with them",24
1qfadud,4,"can always cache to create a PK and if you need it create a view on top of it to cover the ugly.

PARTITION and ROW\_NUMBER() used to solve this problem but idk if new grads are familiar with it.

(it was useful for me literally yesterday when optimizing an ugly SQL query)",6
1qfadud,5,Combining multiple columns for a PK is an option. Try a composite key instead?,14
1qfadud,6,"you need to ask the business team or whoever own the data. they should know which column is a pk. if they dont have the answer is bad db design and process to make one (with composite pk or generate increment number). if they have, most likely you have duplicate in your data.",3
1qfadud,7,"Maybe ""the data has no primary key"" is also a valid conclusion? You may be able to generate an artificial key, but when new records come in that are supposed to be updates, you'll still have a problem. If you don't understand the data, how will you know you are processing it correctly?


Try contacting the producer of the data and have them explain it.",3
1qfadud,8,"Garbage In, Garbage Out.",5
1qfadud,9,"It‚Äôs possible that you also just have duplicates, or bad source data.¬†",2
1qfadud,10,This is when distinct is actually acceptable,2
1qfa0ii,1,"Why would you transfer internally and go down a level?  You're already at the company so you have the advantage of either a) being in possession of specific domain knowledge that only someone at Amazon would know, or b) being equipped with resources to help you learn it much easier than an external candidate ever could.  You also understand the culture and what sort of skills are valued there better than an external candidate could.  So focus on your transferable experience and do whatever you can to learn technical skills you feel you have gaps for.

If you don't have any technical skills at all from the job post above then I would recommend looking for a way to network with a team doing work you're interested in and seeing if you can convince your manager to let you collaborate with them on a stretch project.  That is how I have successfully career transitioned twice now in the last 6-7 years.",29
1qfa0ii,2,"Sometimes there are roles that doesn't have DE in the title but have really close functions. There are such roles in major banks. You'll work with Spark + Hadoop/Greenplum, create dashboards and etc.",14
1qfa0ii,3,"I transferred from a support role to a DE role. Usually it's the manager's call if they want to give opportunity to someone outside the job family.  

My manager has considered many such candidates. But it has always been a tech to tech conversion. Never seen a non-tech to tech conversion for DE.

As for down leveling, managers are generally against the idea because if someone with more experience is at a lower level, they are more ambitious for promotion and might not stay in the team for long.",6
1qfa0ii,4,Ur an L5 DA? Or de?,4
1qfa0ii,5,"I don‚Äôt see any harm in moving from L5 to L4 when you‚Äôre completely from a non tech background. Even if you apply for other companies, you would have to start with an entry level role, so why not start the same at Amazon and that would give you an edge because of the company tag and the pay would be comparable (or maybe more) as compared to non tech roles. Have you already prepared for the DE interviews? If yes, then could you please suggest some good resources. What is your YOE in total?",3
1qfa0ii,6,"As usual, it looks like a catalogue of tools/frameworks/platforms !!!!",1
1qf856d,1,"It really depends on the interviewer.

If the interviewer comes from a Data Engineering background, you will likely be asked to design a Data pipeline covering Data ingestion, transformation, storage & loading in a distributed system.

If the interviewer has a Software Engineering (SDE) background, the discussion often shifts to application-level system design, such as designing Twitter, Google Drive or an e-commerce platform.

From a pure Data Engineering point of view, evaluating candidates on application system design may not always make sense but that‚Äôs how the process works in many companies today :)",14
1qf856d,2,"I moved from software development to data engineer. 
The bridge between the two I find really interesting is distributed compute. The idea/goal is to structure your data and functions in a way that you can break the data apart, run in on twelve  VMs, and collect it all back together again. Also interesting that order of the collection is non deterministic.",3
1qf856d,3,"Study your previous projects, check some youtube videos, get some mock interviews with Director level folks. Think of joining a cohort where they build real world data engineering projects, that way you will learn and remember well. It will give you a lot of confidence.",1
1qf7eoh,1,I have a coworker who suggested using LLMs to do the actual transformations in a data warehouse. They also use it to generate all code and responds to all questions by sending us LLM outputs instead of researching anything themselves. This is a senior person.,101
1qf7eoh,2,"We have terabytes of data on average daily coming in. Hired a contractor to fix a small bug on batching logic. He's the type of guy that always reply with ""chatgpt said..."". And his codes are full of the typical obvious GenAI slops. I kept raising this issue to the management that he's fully reliant on AI on decision making. Gave more chances. 

Until one day he slopped, causing infinite loop of same batch being loaded repeatedly over and over thru the weekend. Costed us a year of cost in just 2 days.... Fired by next week.",62
1qf7eoh,3,"It is definitely showing up, mostly in the form of overcomplicated logic that nobody can explain anymore. You can tell when something was pasted in because it technically works but has zero consideration for edge cases, performance, or future changes. The scary part is not bad code, that has always existed, but the confidence people have in it because an AI produced it. The teams that avoid real damage are the ones that still do reviews focused on intent and data correctness, not just whether tests pass. I have seen pipelines where one small schema change would silently corrupt metrics, and nobody knew why the logic looked the way it did. That is usually the moment people get a lot more skeptical of blind copy paste.",17
1qf7eoh,4,"It's quite a big issue I think. All the AI produced code some of the team I'm in has used and tried to implement has been absolutely crap. It gets a job done but not the job done and it also doesn't fit well within our systems

Every other ai/vibe coder I've come across has been wank as well. Those that are decent are ones who use it for quick 1 liners when syntax is forgotten or they're exploring something new but never implement the ai code into prod",24
1qf7eoh,5,Much much Less of an issue than offshoring 85%+ of a companies tech staff.,25
1qf7eoh,6,"Gosh. I was ranting about it yesterday. One my coworker has gotten addicted to AI usage. Their role is a bit on the lowcode side(Dashboards mainly) so it's not as much of AI slop code, but everything else.


He was supposed to document a certain onboarding process for a new product and dude made a fuckin 30 page doc. Obviously AI. It has like 40 different steps in 5 phases. Then he set up a 2 hour call with the team to review the doc. Manager postponed the meeting to 3 days later cuz no one had the time to read that. 

In the end, maybe only the manager read the doc. Not even sure of that.
Rest of us just gave a go ahead without reading the whole shit. And the dude is so proud that he has contributed to documentation in extreme detail. 100% no one is going to follow through that shit.

Now when you slack the guy, he'll reply with obviously AI generated messages. The meeting invites have AI slop agenda with üëâ‚ùå‚úÖüìÖüìçüöÄ emojis 

Other teammates have started to use AI to summarise his messages and emails into human readable summaries",6
1qf7eoh,7,"On my team, it's a serious issue. We laid off most of our team and immediately turned around and offshored those position to India. Now, our new Indian coworkers are copying text from Jira tickets, pasting it into Claude, copying whatever Claude generates, and opening PRs without testing the code. The code usually doesn't run, and these people are completely closed off to constructive criticism, so nothing has improved for weeks. They do like to bitch about us, though. 

My manager came to me yesterday to tell me I needed to be more patient and to train one of these guys more. There is no training taking place, because they're not actually attempting to do what is asked of them. Copying and pasting things into and put of an LLM is not learning or trying, it's just creating slop. Ive gone over the changes that need made to a file three times now and have shown them the exact lines where changes need to be made. Something that I've told them is only a 3 line update turns into a 600+ line diff.

Last week, I got on a call with my tech lead and one of these guys and went through every single line he had updated in a PR and asked him to explain his design decisions. He literally couldn't explain what the code did at all. 

So, at least on my team, it's a huge issue. We are hiring people that aren't competent because our leadership assumes that any idiot can generate code via an LLM, so they're hiring the cheapest idiots they can find. We have guardrails on everything, so no one has brought down dev, let alone prod, but jfc it's an exhausting, neverending mess around here.",11
1qf7eoh,8,"DAX in data engineering? What?


But bad data quality lack of governance lack of documentation is a much bigger issue for me than a sql query written by AI.
AI wont solve my people problem and data illiteracy",7
1qf7eoh,9,"My former team lead generated a copious amount of slop for a system that's meant to replace something running on a mainframe. But no worries, he's since plopped the project on one of the teammates with more time on the company and he'll be taking over now, with minimal coordination.

He did create a voluminous, but ultimately insufficient, test suite for the contraption. But he did try, there. The code itself is atrocious and the pipeline flounders when ran against data outside the test suite.

Anyway, I'll probably look for a new job by mid year, I'd switch now but I started three months ago and I can't be assed to go through interviews for weeks on end again just now, the silver lining is that I'm seeing that I can do whatever I want amidst all this chaos, and my overtime is regulated so what gives.

Edit: also this is one of the largest and better known multinational corporations of its type of financial services lmao",3
1qf7eoh,10,"Humans make slop too. I‚Äôve seen tons of ‚Äúdata platforms‚Äù fail as they‚Äôre just a collection of disparate pipelines with poor docs and no tests. If there are standards and conventions in place, a tool which outputs pretty robust code after a few iterations can 10x a team. I don‚Äôt think AI is the problem, it‚Äôs teams happily letting the tail wag the dog.",7
1qf6xgx,1,"Personally it's difficult for me to tell anything without understanding the context and looking at the tables etc. 


From what I understand for your new problem you can always go back to your raw layer snapshots to recreate it right ? 

E.g. why can't you take Aug 2025 from your raw layer and build dataset B ?",2
1qf6xgx,2,"This reminds me of a job at a bank I once did. We solved it by doing a sort of scd2 with 2 date ranges (valid from/to), for data_delivery_date (when did we get the data) and snapshot_date (which period does the data belong to). Would that work?

If you dont want to duplicate the data (probably very similar over time) you can also do datavault.",2
1qf5k36,1,"What kind of data are they emailing back and forth? The answer to that question will help me a lot. But scraping email attachments seems like a rather wild endeavor if it‚Äôs just exports from SQL Server. Someone should probably talk to a DBA and make a materialized view, or a view.",3
1qf5k36,2,"I am assuming when they train the model and deploy for inference all of things would be from SQL Server only ? 


You can develop a simple cron job that syncs on a daily basis or every few hours and loads those drive files to SQL Server. 

Since SQL server is already there there would be some sort of schema present. Load your csv files to the same schema present in SQL server. 


That way your SQL Server acts as a single source of truth for data present both in SQL Server + Drive folder.

Your ML team can then query the DB do whatever they want with data. 

Whenever a schema changes you copy the table as backup with a version and create a new or modify the existing one. Etc.",3
1qf5k36,3,"For an org with this tech maturity, I‚Äôd be looking at azure data factory or similar.",1
1qf5k36,4,"Are you using any cloud services? If so a storage account or blob storage of some form is needed. 

Cron is good but it‚Äôs not very efficient. 

You can have a local installation of airflow running for orchestration it‚Äôs much better and extendable in all sorts of directions. If you follow their code guides it will also provide structure. 

Finally you should set up a metadata database with all the configs for the sources to catalog what you have etc. This will be helpful as the project grows. It can be a repo with config files like JSON or yaml with source file info.",1
1qf5k36,5,"The most effective approach is to separate storage from indexing. You should treat your SQL server as the Control Plane and a dedicated file store (like S3, Azure Blob, or a MinIO instance) as the Data Plane. Your Python script functions as a gateway that pulls data from your various sources and subjects it to a strict validation suite before it ever reaches the ML team. Instead of raw CSVs or SQL dumps, have your script convert the data into Parquet files; this format is compressed, schema-enforced, and significantly faster for ML frameworks to load than standard text files. Once the script validates and saves the file to your central storage, it should write a record to a ""Manifest"" table in your SQL server. This record includes the file path, a unique version ID, a checksum to prevent corruption, and the results of your quality tests. The ML team then simply queries this SQL table to find the URI of the latest ""Verified"" dataset, eliminating the need for email threads and manual downloads.
This setup transforms your manual 70GB bottleneck into a robust, versioned data pipeline that maintains the SWE standards of your core application.",1
1qf5k36,6,"70GB???? Looks like data governance failure. Pick a single source of truth first, either SQL Server or object storage and stop letting files float around untracked. If you stay on prem, you can use scheduled ingestion to pull CSVs and DB tables into a controlled landing zone and then version and validate before ML touches anything.

A cron Python script will work short term but will break on retries, partial loads and schema drift. If you want something more durable without building your own pipeline framework, tools like Integrate.io or even SSIS can standardize ingestion from file shares and SQL into a central store with basic validation.",1
1qf5k36,7,You can centralize and handle your data processing using SSIS. The SSIS module is already included as part of your SQL Server license.,0
1qf5488,1,"Step 1. Talk with your org‚Äôs legal team, because if you‚Äôre using claude code on any work machine you‚Äôre probably in breach of FADP since it can run bash.",4
1qf5488,2,Thank you very much the YAML formulation was a half main problem but it is true that I will see with them how they want to formulate all this,1
1qf0y19,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qf0y19,2,"I don‚Äôt think you‚Äôre wrong to be frustrated, and you‚Äôre definitely not the only one who‚Äôs felt this with Fabric. From the outside it looks less like ‚Äúnetworking is unimportant‚Äù and more like Fabric being designed as a very opinionated SaaS first, and an Azure-native platform second. MPEs feel awkward because they‚Äôre basically abstracting VNET concepts into something Fabric can control end to end. That gives Microsoft tighter isolation, simpler UX for less technical users, and easier multi-tenant guarantees, but it also means every new resource type needs explicit support. That doesn‚Äôt scale well for power users who expect networking to just work like it does in Synapse or Databricks. The slow rollout of MPE types is probably a mix of security review, product prioritization, and yes, commercial incentives. Fabric clearly pushes ‚Äúbring data into OneLake‚Äù patterns first, and anything that bypasses that path feels like a second-class citizen. Whether that‚Äôs intentional cost pressure or just product focus, the effect is the same for customers. I don‚Äôt think it‚Äôs a lack of technical capability so much as Fabric still deciding who it‚Äôs really for. Right now it feels optimized for managed ingestion and analytics inside the ecosystem, not for complex private networking scenarios. Until that changes, Fabric is hard to recommend as a full replacement for Synapse/Databricks if deep network control is a requirement. Curious if others have found workarounds or just accepted this as a current limitation",2
1qf0y19,3,"I think you can use vnets in fabric, but it has to be set up on a workspace level.",1
1qf0y19,4,">¬†¬†it is frustrating to use a SaaS where the vendor puts their own interests far above those of the customer.

The objective has always been profit, not service.¬†Tale of the scorpion and the frog is appropriate. ¬†

Fabric definitely shaping up to be the Zune of 2026. ¬†Playsforsure",2
1qf0iqk,1,"Looks like managed¬†Kinesis Data Firehose is good. It‚Äôs priced per Gb processed, so you pay for what you use. It also sinks to S3,¬†",1
1qf0iqk,2,Surely the cost of parsing lineage metadata is a drop in the ocean compared to the cost of running Spark and Glue?,1
1qeyych,1,"https://duckdb.org/2025/03/14/preview-amazon-s3-tables

https://github.com/duckdb/dbt-duckdb/issues/672",2
1qeyych,2,"Have you tried out dbt-duckdb yet, now that DuckDB supports S3 Tables?",2
1qeyych,3,"Have you tried asking the [dbt Community Slack](https://getdbt.slack.com/sso/saml/start)?

I‚Äôve asked specific technical questions there and, if you give more context about the setup and issue specifics, folks are very helpful in trying to figure things out.",1
1qeyych,4,We use dbt and iceberg tables in S3 and the dbt-Athena adapter works well. What api call or what error are you getting maybe can help troubleshoot?,1
1qeyych,5,"The dbt-athena adapter does not currently support S3 tables due to how AWS creates a new federated catalog called s3tablescatalog. There has been an issue open for quite some time for exactly this https://github.com/dbt-labs/dbt-adapters/issues/1186.

Your best option is to use regular iceberg or hive metastore tables. Or as others have mentioned S3 tables together with duckdb. dbt-glue is a pain in the ass from what I remember last looking at it.",1
1qeyych,6,"In my company, we use Trino as the query engine on top of Iceberg, and dbt-trino is used further.",2
1qexmyu,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qexmyu,2,"I think it's a more or less common problem that's hard to solve.  It's called ""entity resolution"" in practice and is generally accomplished with deterministic rules, probabilistic models or machine learning.  Companies have been doing this long before LLMs so there are some other approaches, some of which you mentioned.  LLMs may seem reasonable, especially for a one time match, but I guess I would want to see it in action compared to other approaches.  I have seen the LLM approach shit on in some industries like healthcare where an incorrect match could lead to patient records being divulged by sending it to the wrong person.   Like most things in data engineering though, the approach you choose is largely determined by your particular circumstances.",8
1qetgdg,1,"This is way too vague. We would need to know the size of your datasets, the type of data, the budget, and the security needed.",3
1qetgdg,2,"Personally, I need more context.


I would say one, if I need to take actions quickly. But without knowing more is hard to say",1
1qetgdg,3,"Spend 10 million and hire the most expensive data agency on Earth and trust them to over build and screw everything up royally and then blame you.  Or hire a few seasoned Data Engineer pros to take the time to analyze, develop requirement docs, and then build lightweight, solid  and flexible.",1
1qes5tx,1,my job is to solve business problems by typing them into the ai.,327
1qes5tx,2,"AI makes me more productive at work, at the cost of losing a bit of intelligence every day...",174
1qes5tx,3,"I was a huge AI skeptic when all the LLMs started to drop, but since then they have become really good. I use AI every day to help me through my work. I ask chatGPT to write scripts, read logs, research solutions, etc. THIS is the new iteration of software engineering and by extension data engineering. If you‚Äôre not using AI at your job, you‚Äôre falling behind.  
  
Having said that, I‚Äôm still up to date on technology, I still read documentation, and I review every single line of code chatGPT writes without exception. If I don‚Äôt understand what it‚Äôs doing, I research it. If I‚Äôm skeptical on what the script does, it does not make it into prod. No exceptions.  
  
A word of caution, we had someone on our team write a script with AI and run it without reviewing. They deleted a bunch of data from the database without knowing. We had back ups, but there was data that went missing. When asked about the obviously ChatGPT written script they could not explain what it did or why.",156
1qes5tx,4,"Back in the StackOverflow days (which I still use today, but sad to see others no longer do) I‚Äôd always use it to research a problem and apply it to my own thinking.

What I keep is a learning log on my GitHub, my own collection of code snippets and solutions to problems I‚Äôve had over the years, which I always go to first over an AI tool. This collection is my own, sure I may have learnt some of the technique from Stack or documentation, but the code in the log is applied to my own circumstance so the knowledge is retained better. I can also trust it more so than the AI tool.

When I do use an AI, I always ask it to provide me with the citations it used to formulate the response so I can go there to back up the outcome. Gemini is particularly good at this.",27
1qes5tx,5,"I‚Äôm in the same spot! I realize I probably forgot how to code. The other day I had to start a script from scratch, and was literally staring at the screen unable to remember what to do. I think this is a serious problem.

To be honest, what we have learned before AI stays in our brains. All we learned after, it doesn‚Äôt. At least, this is the case for me. 

And yes, company do coding test, but it is way too easy to cheat that you end up doing it. I believe everyone use AI at work nowadays",37
1qes5tx,6,Everyone else please keep relying on AI to do everything.,60
1qes5tx,7,"Same, with a big BUT:

I try REALLY HARD  to understand the context and use case for keeping in my pocket later, because you still have to understand why you choose to do one thing over the other.  I always make my LLM justify the action.  Lots of times,  I've asked ""why not do it x way?"" and it'll be like ""oh yes,  that's much more simple"". It can get over complicated fast if you don't pay attention.  

I have definitely learned a lot of new things.  But you're right about interview questions man. I just don't allocate my brain power to recalling an exact name of a command anymore because the only reason to do that is for an interview now üòÖ",11
1qes5tx,8,"You could try going into management? Seriously, this is basically what management is, telling somebody to do something and supervising their output.",9
1qes5tx,9,"I don't understand something about OP and most of the people in the comments: is all you're doing as a data engineer 'writing code'? 90-95% of my time at work is spend reading code other people wrote, not writing something myself, since I have to maintain legacy systems. I don't see how AI can help me with that if I have to explain to the client how a certain column is computed by reverse engineering a pipeline to figure out how it works.",6
1qes5tx,10,I use AI but still have to do lot of work based on my understanding of the domain and internals of the data. AI still heavily relies on what I feed to it and still can't figure out the core of what's being asked.,11
1qericu,1,"You‚Äôre not as far off as you think. What you‚Äôre describing is a very common gap between academic data science and industry roles. The main shift is going from notebooks and models to systems. Instead of just asking if a model works, start thinking about whether it can run reliably, be monitored, and be understood by others later on. A good place to start is core software fundamentals like Git workflows, writing clean modular Python, logging and basic testing, then Docker basics. After that, focus on understanding end-to-end pipelines (ingest to transform then store then serve) rather than individual tools. If you can, build one small but real pipeline on your own. Even a simple project teaches more about engineering tradeoffs than a lot of courses. During your internship, pay attention to *why* things are built the way they are and ask about failures, costs, and deployment. A few focused hours a week alongside the internship is enough if you stay consistent, good luck with everything",1
1qeqfk7,1,Seems like a bot post,2
1qeq2d3,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qeoqfo,1,"Some advices for the start: start small (one or few data sources / pipelines), dont overengineer (dont try to cover every possible edge case), build stuff that it is easy to tear down, build LOOSELY coupled pieces (this way you avoid one small thing breaking upstream and blocking everything downstream), spend a good time sketching diagrams and noting down pros and cons for every architectural decision (important: there's not one perfect choice. Best-practice in terms of this kind of thing is to know the trade-offs), implement observability from day 1 (that can be writing logs and checking them daily in cloudwatch at the start).

Note that MANY of your trade-offs and architectural decisions will depend on what kind of requirements are required downstream. In fact, the very first thing you need to map is who are the data consumers and what are their needs - mind you that more often than not the end-users have very vague requirements (it is often a big part of the job to translate vague business requirements into technical choices - also, often what they want is different than what they need). If applicable to your case: also talk to upstream stakeholders (feks, in my company, the data is generated by services written by developers, so I have to sync with them to understand what is doable or not in terms of data serving/latency)

EDIT: also, if you are not super familiar with AWS, dont underestimate how much time you'll spend implementing things there :D

(I'm in a similar position, where I wear the architect, tech lead and engineer hat at the same time - and oh, Iam also brazilian ;) )",20
1qeoqfo,2,"Don't build or code first. Spend enough time understanding the data, planning, blockers, testing strategy.",7
1qeoqfo,3,"I'd say don't be afraid to ask for external professional help from a consultant firm, done through your boss that he budgets for.

A simple 5k USD budget can buy you a pro that has done this hundreds of times for a few days, he'll be your mentor. This can also reassure your boss, or his boss.

Personally I had nothing but bad experiences with AWS and zero with Azure. In either case, Snowflake is awesome.

OP is in BR, 5k USD goes further there than in the US. Here in Canada they charge 250$ Canadian an hour.",9
1qeoqfo,4,"DM incase you are stuck somewhere , I have 12+ years of experience and currently serving as an Architect !",2
1qeoqfo,5,"I have led projects like this (including banks).

- Look at and prioritize the business needs rather than trying building the perfect solution which has all the data from everywhere in a perfect unified glorious schema. Start with the most important system and go from there.

- There will be trade-offs all the time. Embrace it (meaning find ways to do it in a structured way) instead of fighting it.

- Do not, I repeat, do not underestimate the importance of stakeholder management.",2
1qeo2nu,1,"How much is market rate in your area?
How many hours?
Do they have to give notice to stop?",1
1qemxue,1,"Nowadays the market for juniors is almost zero in all the planet, for every IT specialization... companies just hire senior and give them AI.",1
1qemxue,2,"i'd say if you need a working visa as a junior, just forget about it. market is bad for juniors and a visa + possible language barrier makes it harder. fluent german skills are a must in that situation and it's probably easier to focus on a grunt-work job in a consultancy for little money. besides, from my experience AWS in DA is much tougher in Germany. Only GCP has less market share. If you want to increase your odds, learn some Azure stack, dbt, snowflake, maybe some other dbs like duckdb. any low cost certificate helps because it shows some form of work ethic.",1
1qemxue,3,"Hey, congratulations that sounds like a solid track record for one year of experience! 

I would be lying if I said juniors have it significantly easier overall than other countries, but I guess the overall biggest demand by far for data you will find in Berlin, it's our digital business central. 

Also, Berlin has the comparatively highest share of startups which will be rather favorable to young professionals. 

Hasso Plattner Institute in Potsdam is an excellent university for Data Engineering studies, propably TU Berlin as well. 

That said, Munich (TUM is excellent for CS), Frankfurt (finance) or Cologne/ D√ºsseldorf probably are also solid choices. But expect significantly higher share of enterprises/ huge corporates with accordingly longer hiring processes. 

At the end of the day, a ""good"" choice comes down to your field of study, preferred career field and preferred company type (startup/ corporate).",1
1qem905,1,"For simplicity and low cost I would look at fivetran + Postgres (supabase is a good choice, or any other low cost provider - if you are already a Microsoft shop you may have options via azure also). If you have large enough volume that fivetran is costly, look at airbyte. Powerbi will plug directly into your Postgres instance. 

I can give more specific advice if you estimate the size of your data set. Like you could just run sqlite and airbyte locally and then just trigger your jobs manually once a week  - which would be free (other than the powerbi part)",3
1qem905,2,"What's your database of choice? You'd want storage in SQL compatible database for storing and centralized retrieval, rather than hundreds of files.

Platforms like Snowflake have all kinds of integrations you can use with Snowpipe.

The lowest level of Snowflake is not very expensive.",1
1qem905,3,"If you do not even have a database yet that choice matters more than the tool. BigQuery is usually the easiest here since you can stay well under 1k a year at your volumes and Power BI can sit on top of it without much friction. Postgres works too but then you are also on the hook for hosting, backups and basic ops.

On the ingestion side, Airbyte is fine to test but expect to babysit it once refunds, deletions or API changes show up. For low code and minimal manual work, managed connectors are usually less painful. Something like Integrate.io or Hevo can pull Shopify and Amazon on a schedule, handle re-syncs and schema drift and land clean tables you can use directly in Power BI.",2
1qelsee,1,"Your organization decided to remove excel/powerBI without considering the replacements? The answer to your question is that your business people need an alternative to the tool they are using, you are never going to be pulling all data work into the IT department and just have your analytics only do data visualization....",2
1qelsee,2,Im interested in your on prem and no big tech stack. Which tools are you using?,1
1qelsee,3,"Would they be willing to learn SQL if not python?

There's a lot of visual tools out there for transforming data, it's just a question of taking the time to evaluate them. Start by asking what sort of typical use cases the analysts have (as well as some edge cases) and then compile a list of products with their pros/cons. Asking people for product recommendations here is a bad idea, because every product has limitations; if it fits one companies' use case it doesn't mean it will fit yours.",2
1qelsee,4,"We use Azure & Snowflake. PowerBI works just fine, the paid-for version.

I think your company needs to hire a BI consultant firm.

Even the free PowerBI desktop version can access any database if you do the setup for it. They can also use Visual Studio Code (A Microsoft supported Open-Source query/dev tool). Or DBeaver and setup JDBC.",-2
1qelnbt,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qel3sz,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qel3sz,2,"From a DE perspective, get very good in SQL. Get just good enough in Python or some ""data language"". Know enough about integrations, frameworks, tools. Give DevOps a smell.

There are several open datasets for projects out there. Google open big data datasets. Some even do streaming or apis that are hot.

HR is hard, but projects in github (or any other similar) help a lot. Pass HR and this is what they will look at.",2
1qel3sz,3,"I may have missed it, but you don‚Äôt have an ‚Äúorchestrator‚Äù for running ETL/ELT pipelines. If you want to keep it free, there are open source ones used by actual companies like Apache Airflow. You can use this on the open data sources mentioned. 

I‚Äôve been through many DE and DE role adjacent interviews, and SQL almost always comes up. Know some technicals like order of execution in a SQL query, CTE‚Äôs, Window functions. I have never been asked any python technicals (have done a few OA‚Äôs though that are typical leetcode style, but these were not the norm), aside from Spark concepts.

When you scour job postings, you‚Äôll see a company can be entirely on Azure, AWS, Databricks, Snowflake, etc. But I also see a lot of postings that are Airflow, DBT + Snowflake for example. Airflow and DBT are free and you can use a local RDBMS instead of a cloud DW for a project to teach you ETL concepts and data orchestration, warehousing, modeling.

Get good at one stack and apply for jobs that use those. Before your resume gets to a human, an ATS will scan for all the relevant key words in your resume with the JD. I have only worked with Azure & Databricks so I immediately stop reading a JD when it‚Äôs asking for AWS or Snowflake. In your resume, emphasize the results per experience (ex. optimized X by Y%) first, responsibilities second. I think you can definitely give your many different roles to chatgpt and ask it to reframe those roles to be more DE-specific.

As for certs, I have noticed consulting roles seem to value those more than product company roles. If you‚Äôre fine with spending money, a Databricks DE associate cert, AWS DE, etc could make you stand out more as an entry level candidate. If you want to work with the cloud, AWS is more commonly used over Azure and GCP. Having worked in consulting before, the certs are how the firm pitches you to clients and gets you staffed (and therefore make money off of you).

Lastly, use Linkedin and have all the relevant DE skills and experiences on your profile. It‚Äôs a lot easier for recruiters to find you first instead of the other way around. 

Good luck and I‚Äôm happy to answer any more questions you may have!",1
1qekr0y,1,"Fivetran is mostly just plug and play connectors, you can build custom connectors for it with lambda or edge functions in Python. It‚Äôs great for smaller but has a stupid cost model for any real volume (>100m/mo). You‚Äôll spend more time modelling and testing as fivetran is just move a->b, so look forward to more dbt/dq and less hudi.",8
1qekr0y,2,"Fivetran for most connectors is easy to setup.   You could learn to use Fivetran in an hour.   You shouldn't have any issues dealing with it.   You just have to be careful if creating new connections that you understand the volume of monthly changes as it impacts MAR consumption.  Every once-in-a-while someone complains on Reddit that they misconfigured something and blew threw $$$ in credits.   if replicating form large SaaS applications (SFDC for example), don't replicate everything (better to replicate just what you need).

Also, If you need to replicate from SaaS solution production and other sandboxes, keep in mind, all replication can contribute to MAR (doesn't matter if it's a DEV environment or prod).",5
1qekr0y,3,I don‚Äôt think you will face any issues with it.,3
1qekr0y,4,Could this stack get more complicated a buzzwordy...,-2
1qekr0y,5,"Dude, writing on the wall.  They are paying for Fivetran to lower their labor costs.  That might be you.

If it isn't, someone has chosen your area to save costs.  Time to sharpen the resume.",0
1qekncd,1,"This is awesome‚Äî‚Äúls with superpowers‚Äù is exactly what I didn‚Äôt know I needed. The row count estimation for huge CSVs right in the file listing sounds like such a time (and context) saver, especially if you bounce between unfamiliar data directories daily. Customizable terminal colors are just icing on the cake. If you‚Äôre fleshing out python extension support, maybe consider a plug-in system where people can easily share their own data ‚Äúinspectors‚Äù or formatters. Subscribed to the repo‚Äîlooking forward to updates!",2
1qejnn6,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qejnn6,2,"Not going to give practical advice, but from my experience deliverables = trust. Show you can deliver what they want on the shorter term, and once they see you as trustworthy and competent, you can gently work the idea of a proper clean up into their mind. 

You‚Äôll rarely achieve success by stubbornly trying to do ‚Äúthe right thing‚Äù if you don‚Äôt have support and buy in from upper management.",8
1qejnn6,3,"Put the problem in dollars. How much money is this problem losing them? Externalize the pain. It also sounds like you're new and not in a senior or management position. That means you don't have the sway to start rolling out big initiatives. If it's truly a problem, other people are likely having the issue. Find out what other teams issues are, start building a rep by fixing problems, put the value in dollars. No one cares about best practices or anything like that if its not costing them money",1
1qejnn6,4,"If you want a data warehouse, you could build a POC on some subset of the data to show to management and describe the benefit (e.g. more info available, cleaner data, flexibility). Remember though that likely you will have to deliver and build that future.¬† Building an analytics application on bad data can be not fun.


Another approach is automating your cleaning and extraction steps to another database and still treating each data product as an independent pipeline (or reuse pieces that make sense).¬† It's less ideal long term, but it's incremental so you don't have a massive side project to build a data warehouse.",1
1qejnn6,5,Man I feel your pain. The situation you described sounds eerily similar to what I walked into at my current job a couple of years back. We had this sprawling CRM system that had been customized over the years by like 5 different consultants none of whom seemed to talk to each other. Marketing was using one set of IDs for customers sales had another and finance was just throwing their hands up in the air. Trying to get a single reliable customer view was a nightmare. We spent weeks just trying to map out the different entities and how they related to each other. It felt like unraveling a giant knotted ball of yarn. And then of course once we thought we had it figured out someone would drop a bomb like Oh yeah that table We stopped using that three years ago but nobody told anyone. The worst part was the constant requests for data pulls. Sales would want a list of customers with X Y and Z marketing needed a segment with A B and C and the data team was stuck writing custom SQL queries all day long. It was completely unsustainable and we never got to work on anything strategic. What finally saved us was this virtual data platform we implemented. The thing that impressed us most was its ability to automatically discover all these hidden relationships between entities that would have taken us weeks to untangle manually. The AI engine identified and resolved data quality issues too duplicate entries mismatched data types the whole nine yards. We didnt have to move or duplicate any data either which was a huge plus for security and compliance. Then once the data was unified we started building no-code ETL processes to push cleansed data to the different systems and to our data warehouse. That part really freed up our data team. Instead of constantly fulfilling ad-hoc requests we could focus on building actual data products and analytics. Seriously the reduction in headaches alone was worth the cost. I know it sounds like Im selling you something here and I kinda am. The company that makes the platform has a referral program and yeah Id get a kickback if you ended up using them. But honestly based on what youve described I genuinely think it could solve your problems. Id be happy to make an introduction if you want to explore it further no pressure either way. Just let me know if you think a connection would be helpful.,1
1qejnn6,6,"What I'm working with is stunning. Just found a duplicate record and I'm not stupid enough to bring it up because it doesn't affect the columns I need... Great culture and exactly what you want from your data team right? Right? 

I'm dying for business to wake up and realize why we took time to build data warehouse that actually follow the rules. Those ""let's just do x for now"" always bite you in the ass. It seems today's strategy is to limp things along until the business is fed up enough to wait for an entire redesign. Then the tech team and manager are replaced and you don't even fix all the tech debt because the people aware of it left! Tech debt NEEDS to be tracked and talked about regularly. Where I'm at they pushed anyone technical out of those discussions and it's only been band and worse. The consulting team they brought in pointed the finger at the data/tech team, promoted mid level managers with no tech background into those leadership positions (now they have to lean on the consultants for EVERYTHING AND EVERY DECISION! So much wasted money and time but theres no one technical to explain anything, even WHY things are as messy as they are. 

I saw this happening and tried to get my boss to understand what was happening. Instead he fought her about design differences instead of division of responsibilities. The consultant got 2 years to work with the analysts (who from my outside perspective were much further behind the engineers) while pointing fingers at the engineers/data. He needed to be bringing up all of the things the analysts and the business side need to do to make teams like this successful.",0
1qei0lu,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qei0lu,2,"Man, your experience is way more valuable than any specific tool.
You can always take a Udemy course to learn Databricks and build a project using Databricks with Azure since it‚Äôs free.
Tools change all the time and they‚Äôre usually easy to pick up.
What really matters is having strong fundamentals, real use cases, and knowing data modeling and warehouse design ( which you have).",60
1qei0lu,3,">No Azure, no AWS, no Snowflake, no Databricks..

Tbh, you really want to lean on your 25 years of experience.  Strong fundamentals and being able to learn quickly  whilst making very few errors I would say are two of the most valuable skills you can have.

Don't think of everything in data as tools.  Go and look at GCP and compare Azure and AWS to it.  If you identify a lot of things in broad categories, you'll be well on your way to catching up.",15
1qei0lu,4,"This sounds more like a positioning problem than a skills gap. You‚Äôve already done the hard parts: ETL, SQL, cloud DWs, orchestration. Those fundamentals transfer.",4
1qei0lu,5,"Just move. You will adapt to any environment if you write python.

Just dont stick with ssis.

You just need to know the cloudplatform you are working with.",5
1qei0lu,6,"Honestly, this reads less like you are behind and more like you are underselling yourself. Twenty five years of data work plus real cloud experience already puts you ahead of a lot of people who only know one stack. BigQuery, Python, and Airflow are not entry level skills, even if job listings act like they are.

A lot of the market noise is checkbox driven. Azure vs AWS vs GCP differences are smaller than they look once you know one well. Same with Snowflake or Databricks. The core ideas around modeling, pipelines, reliability, and cost do not change much. You can close perceived gaps with targeted hands on projects, free tiers, and reading architecture docs without paying for courses. More importantly, try to move internally if you can or shape your current role to touch adjacent tech. Momentum matters more than collecting logos.",3
1qei0lu,7,"Dude I'm in the same boat! I have about 4 years AWS experience but recently (3 years) took a job that's 100% on prem with the idea we were eventually going to the cloud and my experience would help shape it. Well they delayed my start date a few months and by the time I got there decisions were made that basically locked us into an onprem database. Now we have a new CEO and they offshored us so I'm looking for a new job. I had no idea this was the absolute worst time to try and edge myself into larger business by stepping away from the cloud a bit. I'm close to my AWS DE certification but small companies are using that less and less and everything is azure. I'm terrified, I've been #2 or 3 in a handful of job opportunities but I'm definitely fighting an uphill battle. At least when it comes to the jobs recruiters are calling about.",2
1qei0lu,8,Most people right now feel like we falling behind. Don't sweat it. That's what social media was meant to instil. A sense of FOMO.,2
1qei0lu,9,"Even if you don't want to pay for course, all these cloud platforms will ask you to pay even if you are just making tests and playing with it. Also, in my opinion you are not missing much by not knowing about the cloud platforms. I think the hype around them is starting to die off because the costs are thru the roof. There will be soon a reverse trend where people will want to move back on-premises.",3
1qei0lu,10,"The concepts of data warehousing is still the same, independent of the platform you are running on.  
You can see Snowflake and databricks as a data warehouse, that are built different than what you are used to, but you can use it the exact same way as traditional databases if you want.  
  
If you have only done database works in the past I would advice to read more upon software development best practices and how you can implement this in your day to day work, like more advanced git usage, ci/cd in all forms, typing etc",1
1qehorv,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qehorv,2,"A suggestion I give to people. Its difficult at first but it will kind of make you learn it along the way.

Setup your own orchestrator.

Dagster, Airflow, Prefect.

Learn to set one up.

You will be bombarded with jargon and tech that will force you to learn",6
1qehorv,3,Joe Reis book has the principles of DE. You can apply them with projects. I recommend DE zoom camp.,6
1qehorv,4,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qehorv,5,I'm in the same boat as you. [this](https://www.audible.com/pd/Fundamentals-of-Data-Engineering-Audiobook/B0CN1WNQ5G?ipRedirectOverride=true&overrideBaseCountry=true&bp_o=true&language=en_US&source_code=GPAPP30DTRIAL5480813240005&source_code=GPAPP30DTRIAL5480813240005&gclsrc=aw.ds&gad_source=1&gad_campaignid=20983884474&gclid=CjwKCAiA4KfLBhB0EiwAUY7GAYx4_JaB-j5xZ72wCPnX0oydYCdOcy5tmGfoK83GXx0KC1oDPR-rkBoCSSMQAvD_BwE) has been recommended to me by my DE,2
1qehorv,6,What the Auto Moderator links to is excellent.,1
1qehorv,7,"I was also a BI analyst before who was able to transition to DE two years ago. I think you already have a good foundation given that you're already a BI analyst. While books & courses are great, practical experience will teach you better than any of those. I'd suggest just look into your current company and see how data engineering is applied there. Consume your DEs' entire docs to get ideas or get shadowing sessions. This is what I did to transition, I just reviewed what I already know from experience. If you can transfer internally, that would be better.

Understand your ETL/ELT pipelines like how data moves from point A to B (from source system up to the analytics layer) in your company. Learn how & why certain tools are being used in the system, their trade-offs in terms of scalability, costs, reliability, their impact, etc. Software engineering plus systems analysis & design concepts help a lot like SDLC & Agile methodologies for example.

If you're proficient with SQL and be able to explain different situations such as knowing when to use subquery vs CTE, view vs materialized view, handling duplicate records, optimizing queries etc. then you're more or less good to go. Knowledge of Python/PySpark helps a lot as well. Understand and differentiate between a data warehouse, data mart, data lake, data lakehouse, dim vs fact tables, normalization vs denormalization, file formats (csv, tsv, parquet, avro), etc. Knowledge of cloud also helps. These are the usual concepts being asked during interviews and what you do on the job regularly.",2
1qehorv,8,"I think the others have already shared some valuable Insights.

My take may be a bit unpopular but since your org is msft heavy, you could try exploring the Fabric data engineer associate cert.

I am not saying prepare for it in such a way that you are ready to give the exam and acquire the certification but browse through some of the related content on msft learn and that process might really help in understanding some of the jargon and related concepts. 

The cert is msft centric but the material does touch on some of the concepts that are foundational and can benefit you regardless of which cloud provider you're with. 

I've had a similar trajectory as yourself and found going through msft learn in addition to hands on experience helped a lot.",2
1qecu5b,1,"Short answer: there's no easy way to link those (that I've seen).

If you click into the part of the job that's slow and check the physical plan you can sometimes see the df name or what cols are being joined together and that'll help narrow it down, sometimes you can get lucky with the counting different points but its pretty manual in my experience. There's some tools out there that claim to do this but i havent used them. You can probably guess what part is killing performance looking at the types of joins (broadcast joins are fast vs big fact to fact table joins will be slower, sometimes range joins can be slow if they're not optimized, etc), knowing your data and how it's being joined will be super helpful too.",2
1qecu5b,2,"i don't know if directly. Likely no. Or not in a way that would be tangible. You can dwtermine though if you have issues with spill, what takes long time and why. But that would be for show and tell rather than a post honestly as the ui is so ready to be reworked",1
1qecu5b,3,"If you just want to use the spark UI, you'll have to run each part in a notebook and display a sample result, then reload with a clear output and do the next step",1
1qecu5b,4,"Here is what I would suggest:
1. Find the stage which has the maximun time/ shuffle output records

2. In the overall Job DAG, search for the above stage id. You would see potentially and Exchange operator

3. Check the stats of the exchange node to determine skewness/ disk spill etc

4. Hovering above the  exchange node provides infi about which columns are going into hash partition, and this is a clue to identifying the joins. You can also trace the tables and operations feeding into the exchange",1
1qebwl9,1,No,2
1qebwl9,2,AI slop post,1
1qebwl9,3,"this could def help, but only if the Power BI model is actually solid. AI reasonin over a messy model just automates bad logic faster. if your facts, dims, and measures are clean and the grain makes sense, then stuff like lead enrichment or ARR estimates can work pretty well. at that point the AI is reasonin over business rules, not random tables. but if the model already fuzzy, AI just gonna amplify the noise. They talk about this idea in r/agiledatamodeling ‚Äî model first, automation second.",1
1qebb1m,1,"
This is not new.

The concept ‚ÄôGarbage in, Garbage out‚Äô is probably 70 years old and has, to my knowledge, never been disproven.

One of the positives of AI is that the recent fashion for chucking out as much garbage as possible because data is a product ( apparently ) is starting to be questioned. 

Data is, and always has been, an Asset.


Assets need to be managed throughout their useful life.",35
1qebb1m,2,"This was the same even before AI but with data science

You had a bunch of companies wanting data scientists to do X and Y but their data infrastructure was a bunch of excel sheets and their management was only asking for a bar chart or something. Basically every initial consultation for small - mid sized customers just involved telling them they need to start using an actual database before worrying about data science and ML techniques",11
1qebb1m,3,"It's almost always management. They look for the ""New Thing"", and you can guess what this is.
We even have some projects where we ""enrich"" data (have AI generate it, from thin air), it's kinda interesting üòÅ (fwiw we make that clear to users too)

If you take a chill approach, properly separate made up stuff from the real thing, and imagine that this is just some POC for the Real Thing that you'd do if there's actual interest, then it's actually a decently fun way to prototype ideas",2
1qebb1m,4,"100% agree but in the past 6 months AI agents have really changed how my team and I work.

As I was writing the stuff below I realized it became too long... so here's a summary.

TLDR;  going from first using agents to improve our data pipelines and stack, to eventually creating a more self-service system where our data consumers use AI agents to interact with the data.

  
For some context, I'm leading the DE team of 4-5 people (1 mid/senior, 1 junior, 2-3 interns) at a company where our end product is predictions for highly specialized industries (e.g. renewables, transportation).

At first it started with using our data platform's MCP with cursor to just lookup documentation, then I started asking cursor to read our data pipelines and query the dwh directly to find the specific part of a query or script that was causing some issue.

But in the past couple months, after creating some extensive agent rules/instructions documentations as well as creating a md file for each pipeline that contains some business context, I have been able to fully rely on cursor to build pipelines or make major changes.

A recent example is that I spent \~1 hour creating a details requirements document that I gave as the prompt, the agent make changes to \~10 files (mostly sql models and some yml configs and a python ingestion script). About 3-4 hours of back and forth with the agents to make some adjustments, update documentation, and runs tests and validation. The entire process was done in a single workday, whereas normally this would've been 2-3 days of work.

Its not always about ""saving"" time for me but rather its about doing things the ""better"" way - a clear example of this is having the agents create/update documentation, build/run tests, perform adhoc validations, etc. which translates into time saving in the future

I know what I've said above is not related to ""AI on top of broken data stack"", but the reason why I'm talking about how AI is helping data engineers is because I think it that is the sole reason/contributor to speeding up the process of fixing and improving our data stack.

By fully utilizing the combination of cursor, mcp integrations, internal rules/instructions/context documents, and access to query our data warehouse, we were able to ""slap AI on top of our data"" as in:

\- other teams (i.e. data science, software engineering, product, etc.) to also close our data engineering repos and have conversations with cursor to ask it about the logic behind data models (e.g. does table\_xyz contain data from source\_abc? how often does table\_xyz get updated? what is the calculation method for kpi abc?)

\- integration of an AI Slackbot (provided by our data platform provider) that queries our dwh and returns insights - as a result, we have seen people in data science, sales, execs, etc. move away from using dashboards and instead asking the slackbot directly inside threads things like ""what was our prediction accuracy last week?"" ""how did model A perform vs model B last year?""

I think this only became possible because we were able to quickly (within 2-3 months) clean up our data pipelines and data models, create a lot of documentation and context for AI to utilize, and break down the barrier between data consumers and the data itself - this way as the data engineering team, we are no longer a bottleneck and things have finally trending towards the ""self-service"" utopia we've always dreamt about.",4
1qebb1m,5,"The problem goes even deeper. Traditional software crashes when data is broken, whereas AI tries to make sense of it and smooth over the edges. If you have a fragmented stack with duplicate semantics (e.g. three different definitions of churn\_rate in different tables), the LLM will just pick one randomly or hallucinate an average.

Without a rigid semantic layer or metrics store, deploying GenAI in the enterprise is just an expensive way to generate plausible nonsense",1
1qebb1m,6,When will the business side learn? We go back and fourth between rigid data models and the siloed wild wild West where everything is done in the reporting layer,1
1qebb1m,7,"Ummm yes can confirm this is exactly what is happening‚Ä¶ I‚Äôve seen some truly tragic things with mid-market SaaS companies that made it through a GAAC period, sitting on a gold mine of first party intent data in their shitty old MAP just blow it all up ü´†",1
1qebb1m,8,Dealing with this now. Instead of a single relational database - we decide to split things into micro services with 3 dBs. And then you use ai on top. Makes it worse. Vs just a single db ai can introspective,1
1qebb1m,9,What do you mean by fragmentation?,1
1qebb1m,10,"No one wants to do Data Governance because it‚Äôs basically the janitor work of the data world. 

Then they are mad that the toilets are clogged, the sinks are broken, and the halls are full of trash. 

Take as old as time",2
1qeaukw,1,Never do anything for free üòÅ,2
1qeaukw,2,Something that you can do is iteratively share progress and how you use it! Maybe you see if the interest picks up and see what'd be best for those interested!,1
1qe4fvt,1,"Healthcare data feels messy because it blends medicine, billing, and regulation. As a data engineer you usually deal with clinical records, claims data with CPT and ICD codes, and quality metrics like HEDIS layered on top. HIEs sound clean in theory, but most of the work is normalizing inconsistent data across systems, CPT says what was billed, LOINC says what a lab measured, ICD says what was diagnosed, and none of them align without heavy mapping. A big mental shift is that ‚Äúcorrect‚Äù often comes from policy and compliance, not pure data logic.",1
1qe02ol,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qe02ol,2,"It‚Äôs not clear from your post what you‚Äôre trying to do. You mentioned you‚Äôre trying to ‚Äúmove away‚Äù from your current implementation. Why? What is the problem that you‚Äôre trying to solve? Is it poor business intelligence? Speed? Cost? Compliance? User adoption?

You are the PM. No one should be engineering anything until you can articulate a desired business outcome. Figure out what success looks like first and that‚Äôll make it easier to know what‚Äôs ‚Äúgood enough‚Äù",2
1qe02ol,3,"I think it's always beneficial to do at least some data modeling work. It's ""enough"" when all of your DEs can talk confidently in business language around data, at org level.


Without data modeling, I don't think you have a data platform. You just have a platform of tools.


Leadership will not question this if you can make standardization meaningful for business.¬†


Also, standardization should mean more speed. If not, it's just tech bureaucracy. Be careful if modeling work is led only by engineers.",1
1qe02ol,4,"If you worry is how the company will see the step back to improve, try to sell it to them in their own words. Maybe put someone alone to answer urgent requirements, but tell them throughput will be slower in the following X weeks to optimize the way you give them the reports. Unsure how much freedom you have, but if you give some headsup and define milestones i think people tend to be quite ok with it",1
1qe02ol,5,"I came into data in a similar way, coming from finance directly to lead a data team with limited technical knowledge other than decent sql.  I have also been involved with several data warehouse builds in my career.  You are thinking about it in the right way as this stuff is always a series of tradeoffs.  What does feature X do for us, how long will it take and what value does it provide?

It seems like you are vague on requirements, which is the norm, so I would focus on that as it will inform what you build (i.e. you will be rebuilding and reloading if you miss a bunch of stuff).  Start with what you have today and come up with a list of columns and the concepts they related to as a starting point.  I would also get with whoever does the budget modeling / FP&A work for the company as they know the KPIs and drivers of the business, which you will have to align to.  At the end you should have a list of columns or metrics with a domain and column (e.g. domain is customer and columns for account number, name, address, etc).

Then start with that as you talk to other stakeholders.  A good meeting might involve say someone from marketing who knows the views they  want to see, a data analyst who might know what columns/calculations are involved in producing that view and then you and your DA can bump that off against your model.  It's easier to ask them for what's missing rather than come to them with a blank sheet of paper.  Also get a sign off from the department because they will inevitably come back with 20 more requests on the week you drop the finished product.  The DA should be able to work from that artifact.

The data model is important because ideally you want one data source that feeds 80% or more of your data requests.  This ensures that data is consistent across all reporting and dashboards as it uses the same core data and calculations.  If there is an error in a calculation, you can change it in one place and it propagates across all the dependencies.  If you build custom data models for each dashboard or request, then you can have inconsistencies where the developer used the wrong formula, or when you need to make a change you have to find it and change it dozens to hundreds of times.

In terms of important things I would say requirements is first (though you can often do this incrementally), then figuring out your infrastructure.  Like what is your approach to building pipelines and does it provide velocity to develop, is it maintainable, observable, etc.  Does your data platform align well with the likely use cases you know about?  You sort of need this initial planning in place to get going, but afterward you can build incrementally as MVPs or by data subject to show off progress.

You need to balance delivering value and insight to the business with the necessary things to support a robust platform.  Things like automations should be considered, but it's a question of prioritization.  How much time does the automation save vs time to build right now?  Can we defer it without major consequences?  Which ones are the priority?  Can we produce demoable work each sprint or quarter to show progress, and better if the platform can be partially used to deliver value ahead of being fully fleshed out.

\-sorry for  the essay.",1
1qdya1r,1,Managing tables in TF is a nightmare. I let DBT handle everything except the initial landing/raw tables which we handle with handle with custom Go and SQL,1
1qdya1r,2,"You‚Äôre definitely not alone in feeling the pain with Terraform and Iceberg tables‚Äîespecially when it comes to partitions. It‚Äôs a known headache; Terraform‚Äôs support for complex Glue table definitions is still lagging, so most teams I know either maintain pretty gnarly workarounds or split things up.

A pattern I‚Äôve seen work well: Use Terraform strictly for the infra ‚Äúshell‚Äù (buckets, Glue databases, IAM, etc.), but let something like dbt or custom scripts handle actual table creation and evolution. Glue jobs with bootstrapping logic (or even AWS SDK/CLI scripts) can generate and update the tables as needed, while the Terraform state stays clean and manageable. It does add an extra step to your deploys, but it‚Äôs more transparent and less prone to silent breakage than bending Terraform out of shape.

For versioning, a lot of folks lean on dbt‚Äôs schema versions and migration patterns, even if it means treating Glue like more of a catalog pointer than a true source of state.

Curious to see if anyone‚Äôs found a less clunky solution that keeps everything declarative‚Äîbut so far, the split-responsibility model seems the most maintainable.",1
1qdya1r,3,Terraform for DB server deployment and super admin creation. Liquibase for all db migrations.,1
1qdv9lx,1,I would use postgre mostly because you are likely to find a lot of scripts/plugins/tutorials for it.,4
1qdv9lx,2,"Id say just keep it as simple as possible, try something out (id lean towards PostgreSQL) and if you want to make changes down the road, you can.

Just take your time with planning and try to get a good l foundation in there. Otherwise you'll regret it later on.

You've got this!",3
1qdv9lx,3,"Postgres!

Coupled with ATTACH in DuckDB it's a match made in heaven",2
1qdv3wh,1,Looking for the guys that told me fabric is the future,134
1qdv3wh,2,"Fabric is fine for the right use case, but it sure as hell isn't low-code / no-code if you want good, cost-conscious, efficient performance.",33
1qdv3wh,3,">Ingestion from SQL Server relies heavily on Copy Data Activity

If most of the data is from on prem SQL Server (I'm assuming this is your transaction database) why aren't you mirroring the SQL server database into fabric (or into an Azure SQL database)?


>I'm looking at Databricks and Snowflake as options

If you have small to medium data, why are you looking at big data platforms?",9
1qdv3wh,4,"My company uses ADF to load data into Fabric. From there, we transform the data as needed via Notebooks written in Python and SQL. It works very well, and is stable unless there's an update that borks the self-hosted integrated runtime. 

I think moving your ingestion from Fabric to ADF while keeping fabric for everything else would get you the most bang for your buck. Databricks provides similar functionality with their notebooks, but it would be a larger effort to move to them.",7
1qdv3wh,5,"Another one of these. 

You know you can put your ETL items like notebooks and pipelines in a separate capacity right? So it doesn‚Äôt affect your BI items like reports and semantic models. Feels like more of an implementation issue rather than the product itself. 

Also have looked into mirroring which is free for upto 5TB of storage? 

I get it‚Äôs cool to hate on Fabric or whatever and I am ok with criticism of Microsoft but this just screams lazy to me.",6
1qdv3wh,6,"OP, reading your context was like looking in a mirror. I'm basically in the same situation across the board regarding history and experience. However, I've had a totally different Fabric experience. Never had anything fail, been working super efficiently, and easy to debug. Granted, coming from a 3rd party tool with zero visibility. 

My data complexity, scale, and user base is a little higher than what you listed, but my entire backend fits easily on an F4.

For my approach, I only used Spark (pyspark or spark sql) and a medallion approach. All the cdc pipelines work great, and I haven't had any issues.

I have to ask, how come you can't use mirroring for your on-prem sql? Or at least spark if not mirroring.

For the front end, I just approached it via segmentation of capacity. So, each business domain only throttles themselves. But that doesn't happen much since nobody uses paginated reports, and the data models are pretty clean.

Honestly, I'd still rather use Databricks, but my problem is I can't justify it to my organization since I made the Fabric site work too well on its own...",7
1qdv3wh,7,Why not use SSIS to push your data from on-premises? In this way you will use your local server computing capacity.,4
1qdv3wh,8,I used to build data warehouse on Fabric and agree debugging was cumbersome. The Data Factory within Fabric portal didn't have the same features as DF on standalone Azure. But I find the UI better than Databricks though.¬†,4
1qdv3wh,9,my heart jumps for joy each time i read about fabric being bad. third year now and i cherish every post. thank you and i hope microslop tanks 50% in value over the next 2 years.,20
1qdv3wh,10,"I found the copy activity, especially for high frequency jobs more expensive than ADF. To make the best of the platform, I‚Äôd recommend mirroring for SQL tables.",3
1qdqe76,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qdqe76,2,Just use Azure Data Factory. It streamlines the entire SHIR + connector configuration process.,2
1qdq9bq,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qdq9bq,2,"Try a HR headhunter firm like Robert Half. They meet with you, polish your CV, take your requirements.

Robert Half is worldwide so if you're ok with contract work, you can get something interesting outside the US, or inside.

Typically a HR firm will glean 15% from the place that hires or contracts you. No cost to you, other than maybe no salary increase after 12 months since the company paid 15% to the other firm. By year #2 you're usually ok.

There are other firms too, you can be a candidate at more than one.

The Navy or any other branch also have DE needs - you can re-enlist and finish your University degree.",3
1qdq9bq,3,">Does anyone have any guidance for where I should go from here?

To be upfront, this is definitely not a DE specific question.  This is a person and mindset question.

There is literally no secret hack or special strategy.  There is can and can't.  Hone your craft, continue applying for jobs, and keep going.

>but every entry level job posting

Be brave and aim for mid.  The difference really isn't that big although the entry/junior end of the market is, in my opinion, massively saturated because so many people feel like they are not capable of a mid level role.  Whether that's fear of underperforming or thinking they ""need"" somebody above them to take responsibility and look after them, it completely squishes the market.

Purely anecdotal - my first ever DE job with 0 years of experience was a mid level role.  I am not a super special DE.  At the very most, I'm above average.

If there were a lot more confident people, there would be A LOT less mid level jobs.",2
1qdq9bq,4,"Your profile is good, just keep applying. A lot of those requirements on the job applications are aspirational and not always necessary. Your 1.5 years experience is more valuable than many of the candidates out there with a masters from a visa mill school but no experience.

The job search experience is brutal and you‚Äôll get a ton of rejections, but keep applying and you will find something you like.",2
1qdq9bq,5,"I‚Äôd learn AWS and system and don‚Äôt just move laterally 

Move over and up 

Meaning you‚Äôre starting to have enough skill and experience where you can also command a higher paying data role",1
1qdq9bq,6,Do some practical projects with peers who are learning like you. That will help you a lot to gain back that confidence and crack interviews.,1
1qdp2sl,1,"`sensitivity of hash functions` I'm interested in this. They're deterministic by design, right? So if anything looks different at all between the two after reducing to these two hashed values and comparing, would you have your answer? Also, is there a date column or value somewhere in there you could use to cut down to a reasonable test set at least?",2
1qdp2sl,2,"You can use SSMS to get the count of records in each table interactively, from the table properties (right click menu on each table). That doesn't run a count(1), it's getting it from metadata. So it's O(1) time relative to table size. 

Obviously a PITA to do this if you had 100s of tables, but since you only have two tables, dead easy. 

In theory you could query the metadata directly to get this programmatically. It's one of the sys views, but I never remember which.

Other than checking table counts, I guess the important question is how is the server migration being performed? If it's a backup of the old DBs being restored onto the new server, then this is a pretty bombproof approach, so long as:

- writes to the DB have been completely stopped first (e.g. put DB into read only mode first)
- the backup is run with the checksum option

So in this case, I'd focus more on making sure the process for the migration has been thought out fully - then you don't have a lot of validation to do. Otherwise you're just validating that a widely used quarter-century-old enterprise RDBMS doesn't have a certain type of seriously fundamental bug.

If the data migration is happening via some bespoke approach then you do need to validate further. But you first need to ask why they're not doing the more obvious simple backup/restore. 

For further validation, you really need to take the hit on slow scan queries and make sure the time needed for that is built into the migration plan.

Gathering stats is the right approach but I'd focus on sum() of numerical columns that mean something to the business (currency values, quantities, durations, etc). Just sum the whole table with no group by. It doesn't have to be a useful KPI /metric that way, it's just a number that would be very sensitive to anything going wrong in the migration. And if all those numbers are unchanged, you can be pretty confident that whatever you use the data for will still work.",2
1qdp2sl,3,Get the table hash before and compare to the one after,1
1qdmier,1,"ELI5

I have database. I don't understand what database says. Driver knows database language. I know driver language. I talk to database through driver as translator.",19
1qdmier,2,This is the basic building block for accessing and working with OLTP databases.,23
1qdmier,3,"You need a connector. It translates your string into something the DBMS understand. Even in Python (my case), I have to use one, unless I want to reivent the wheel and implent low level code.",10
1qdmier,4,The choice of ODBC vs JDBC can also vary between use cases. For explain in SQL Server I might need to use an ODBC for running DML commands but extracting and pushing data might be faster for the JDBC driver. I have plenty of use cases where I use both in the same process.,5
1qdmier,5,Said this in another thread recently. But one big advantage is that there is a JDBC driver for almost everything you would think about connecting to. So if you have a JDBC flow going you just need to swap in a new driver and you're done.,5
1qdmier,6,"I would highly recommend looking into the new Arrow-based ADBC database drivers. Newer and faster than the old school ODBC/JDBC stuff. [https://arrow.apache.org/adbc/current/index.html](https://arrow.apache.org/adbc/current/index.html)

Lots of good support for Python and common databases like Postgres, etc.",3
1qdmier,7,Reminder for MSSQL users that we now have python-mssql which means you don't have to install odbc packages to access the db straight from python.,2
1qdmier,8,"It provides the communication channel to the Dataset. It allows tools to directly communicate with a database.

You send commands/data through this channel from your application.",1
1qdmier,9,"Every database supports a different network protocol or API to access data.  ODBC and JDBC are Application Programming Interfaces (APIs) that specify how a driver can be built for different databases that allows client programs to use the same API on different databases.  The way you send a query, read results, execute statements, etc, then can be the same no matter which database you are using.  This API layer doesn't standardize the query languages you need to use; you still need to send valid SQL commands for the target database, but the code you use to send those queries doesn't need to be specific to the target database.

The difference between ODBC and JDBC is that ODBC is a C-style API and the drivers are platform-specific.  IE you always to build a Linux version and a separate Windows version.  And some ODBC drivers are only available on Windows (where ODBC originated).

JDBC is specific to Java (and other JVM languages like Scala), but a single pure Java (type 4) driver can be used across platforms.

In data engineering JDBC and ODBC are the default, legacy connectors to get to data from various systems.  Both suffer from their heritage as drivers for client/server database systems, and so typically will use a single TCP/IP connection to send queries and read results.  In big data engineering this can be a bottleneck, and so things like native Spark connectors have been built for more scalable data connections to big data systems.",2
1qdmier,10,"For interconnecting databases that are remote to each other and not the same vendor, like connecting Oracle or MSSQL to MySQL.

Another use-case is non-typed languages (not C, C++, C# that requires compiling and binaries), so like JS, Python, PHP, Java, that cannot do direct binary connections to a database, they can only use JDBC or ODBC.

Example, DBeaver is a java based app, and it uses only JDBC to connect to absolutely anything. It's great.

Where I work I use the MySQL v8 ODBC in a Windows server running MSSQL in prod, and push tables across the network truncate/load in a remote staging tables of a MySQL server (that the Wan IP addresses & ports are fixed and whitelisted for security). Once the load is completed, a stored proc is called remotely to signal the MySQL to process those staging tables.

It's an annoying process though, I would rather create flat files, push them somewhere, and an event triggered at the remote end (like a datalake container/folder) processes them. I could then make JSON data and send that, also based on events.

IOW, JDBC/ODBC between servers is a ""work-around"" solution, and it's slow. Like 45 mins to send 2 gigs of data.

HTH",1
1qdk0i0,1,"I work for a large aerospace company known for lousy leadership.

It's so frustrating listening to the company ""leaders"" talk about AI.  They are throwing huge sums of money at it with little understanding of it's practical uses and no plan on how to effectively apply it, but I guess they get to say AI during earnings calls and board meetings.

I can't wait for this bubble to pop.",71
1qdk0i0,2,Imagine what you could do if they took all that money they pumped into AI and just ... paid their staff.,34
1qdk0i0,3,"Idk if you‚Äôve seen this but you will love it 

https://ludic.mataroa.blog/blog/i-will-fucking-piledrive-you-if-you-mention-ai-again/",17
1qdk0i0,4,"One of the best things we can do with AI, is replace leadership teams with it. The line of work is right up AI alley; spout some waffle then hazard a guess at what to do.",8
1qdk0i0,5,"Look. 

Forget AI

It‚Äôs Agentic. 

But wait! Agent can mean anything from an LLM to a SLM, to an AI/ML model, to some python. 

With some orchestration. 

Think of them as Orchestration DAGs with objects of varying capabilities. 

Take the hype money, deploy meaningful code, just like we all did in the GDPR panic of 2016",5
1qdk0i0,6,"I get triggered by AI because I'm still half a bioinformatician where I'm actually building and training deep learning models, not shovelling prompts via API to pre-trained LLMs which is basically web dev.

Our management took the AI bait hard last year but the failing PoCs have actually been very beneficial. People were lobbing a transactional DB designed by academics into Databricks and getting LLMs to write SQL. What was became obvious (and what is very obvious to anyone here) is that is you don't have decent clean data, accessible and well organised, the LLMs don't do so well. 

Since senior people staked their career advancement on this, it has redirected focus hard onto DE. Recruitment and also stopping every project doing its own thing YOLOing blob storage and Databricks. We are finally producing a cohesive approach to data I've been pushing for when no one ever cared previously.",4
1qdk0i0,7,I know. The moment I see AI on a website or in a video I'm closing the tab.,3
1qdk0i0,8,I'm so burned out on AI you can't imagine.,2
1qdk0i0,9,Man if this AI bubble bursts it‚Äôs going to be bad,1
1qdk0i0,10,"Company executives are chasing AI for one reason and one reason only.  They hope to replace nearly all their employees with it.  They are licking their chops at finally laying hands on the unholy grail:  a nearly infinite profit margin (revenues with nearly zero costs).  All other reasons given are just commentary or, more likely, obfuscation.

Snake-oil salesmen like Sam Altman should, when they die, be buried face-down so they can see where they are going.

Add: I am hoping that AI and its evil disciples meet a fate similar to the Tower of Babel and its builders.",1
1qdi3k9,1,"Iceberg is much better supported on AWS than Delta. Also, most analytical services on AWS support Iceberg (including Redshift). If you go iceberg first , then you're future proofing yourself 

Don't choose Delta unless you're on Databricks",5
1qdi3k9,2,"I think you might have a better time using [S3 Tables](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables.html) and then using [Redshift Serverless](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables-integrating-redshift.html) or [SageMaker](https://aws.amazon.com/blogs/aws/amazon-s3-tables-integration-with-amazon-sagemaker-lakehouse-is-now-generally-available/). This allows you to separate the storage and compute considerations. S3 Tables is relatively cheap, compute agnostic, an AWS native. Then you can choose one or both of SageMaker or Redshift and even play around with both to determine which is better for your use case without having to migrate the data. And if you decide you want something like Snowflake or Clickhouse later, you don't have to migrate the data.",3
1qdi3k9,3,"Don't expect redshift to be bigger, more powerful postgres. With enough workload, your queries and even your commits may take seconds instead of milliseconds.

Also, be ready to manually vacuum and partition your tables.

See this pdf: https://redshift-observatory.ch/white_papers/downloads/introduction_to_the_fundamentals_of_amazon_redshift.pdf",3
1qdi3k9,4,Do you have cyclic loads where for few days 2x-10x compute of daily average usage is needed?,1
1qdi3k9,5,"I actually just encountered something similar at work. Our RDS IOPs cost was insane, so I built a datafusion query service in an ECS container and migrated all our tables to parquet files in S3. There‚Äôs obviously a lot more to it, but the service is excellent so far and dropped our cost 90% for data storage and querying.",1
1qdh5me,1,"250 employees

Team size: me",108
1qdh5me,2,My feeling is that analytics staffing has more to do with the type of business than the number of employees,97
1qdh5me,3,"It depends on the demand and how stable is the system. I joined my current company 1y ago and we were 4 DE + 4 DA, besides BAs, Tech Lead, and PM. We progressed and the systems are much more stable, lost 2 DE and 1 DA and doing alright.",21
1qdh5me,4,"That‚Äôs crazy. At my current org of ~300 I am the data team in its entirety, but at my previous job (gov agency of ~4000) there was a core DE team of 5, and then about 60-70 other dedicated analytics staff distributed throughout different business units in the agency, not counting our GIS teams which comprised another 3 engineers and about 30 analysts/DS staff. 

This ratio of analytics staff to core DE staff definitely was not ideal - DE bandwidth ended up constantly being a blocker for analytics.",18
1qdh5me,5,"150 employees, 5-person data team.
Hard to say who has what role, we all do a bit of everything",11
1qdh5me,6,"Org of 10,000 people.

Central data team of 10 engineers, 30 analysts, 5 scientists, 20 data managers/admins/cleaners. 

A number of analysts also embedded in business teams.",11
1qdh5me,7,"Currently, two data engineers and one team lead. It is a mixed role when you work on everything. It is the smallest team I have ever worked for.",7
1qdh5me,8,"If you have a good data architecture, good semantic and metric layer, i think that size enough for your company, most company's data teams has a huge tech debts bad practices decentrilized garbage lakehouses, and they have huge data team for paying the tech debts which created by bad managers, and that teams manages like dashboard factories without any data infra/arch conceptual design",8
1qdh5me,9,"Nearly everyone at my company does data eng or data science because that's what the company does.

As for my team, it used to be me and 5 other data engineers spread out across the US but they all got laid off and no I have 4 offshore replacements.

I need a new job.",5
1qdh5me,10,7.3 internals? That's brutal. Automate what you can - your team's stretched too thin.,5
1qddfz7,1,"Your post looks like it's related to Data Engineering in India.  You might find posting in [r/dataengineersindia](https://www.reddit.com/r/dataengineersindia/) more helpful to your situation.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qddfz7,2,"Have you tried Polars? Polars supports streaming larger than memory  datasets. 

https://docs.pola.rs/user-guide/concepts/streaming/",12
1qddfz7,3,Tried duckDB?,4
1qddfz7,4,"You're working in google colab. Do you update the library versions at the start of your notebook?

You're using random sampling but are concerned about missing outliers/rare categories. Have you tried using duckdb/polars streaming engine to identify the outliers? You could then pull the outliers or a sample of the outliers into your overall sample. You could even do it in proportion to the overall size of the data.",2
1qddfz7,5,"You‚Äôre absolutely right to be wary of random sampling‚Äîif you care about rare events, long tails, or uncommon categories, sampling can miss them entirely. Chunking with pandas is a solid approach for memory management, but you do have to be careful: some operations just don‚Äôt make sense chunk-by-chunk.

Chunking is great for row-wise, stateless preprocessing (cleanup, type conversion, basic feature engineering). But for anything that needs a full view of the data‚Äîglobal normalization, deduplication, calculating rare category frequencies, finding global min/max‚Äîyou‚Äôll either need multiple passes or a clever accumulation/aggregation step.

A typical workflow for your scenario:
- First pass: Scan over chunks to gather global stats (means, stds, value counts, rare category thresholds).
- Second pass: Use those stats to transform the data in chunks. Outlier detection, normalization, etc. can be done chunked, but with knowledge of global parameters.
- Save intermediate files as Parquet if possible, for speed and compression.
- Tools: Dask and Polars both handle these kinds of workflows better than vanilla pandas, especially Polars for very fast I/O and multi-threading.
- For Colab: keep an eye on disk (mount Google Drive if needed), and be ready to restart if you hit session limits.

If you ever need to empower less technical folks to safely interact with this data (without risking direct DB access), there are platforms nowadays that let you build internal apps over your datasets via drag-and-drop, or even AI chat interfaces. Could help with democratizing those big data insights beyond just the Python crowd.

But for now: chunking + multi-pass over Parquet files + Dask/Polars is the combo I‚Äôve seen work well in similar RAM-constrained settings. Good luck wrangling those 30 million rows!",2
1qddfz7,6,"Honestly that plan to concatenate all chunks at the end is going to trigger an immediate OOM crash in Colab because the list overhead plus the final DataFrame will blow up your RAM usage. Since you mentioned Polars is choking on the sorting required for rolling windows (which is a known bottleneck in their streaming engine), DuckDB is likely your best bet here. Unlike the others, it handles heavy window functions fully out-of-core by spilling to disk, so you can just convert your CSVs to Parquet and run SQL queries directly against the files, saving the output straight to a new Parquet file without ever loading the whole thing into Python memory",2
1qdd2gp,1,"Yeah, this is a very known annoyance with Glue visual ETL. The visual sink defaults to append and doesn‚Äôt expose overwrite properly. The usual workaround is to explicitly delete or truncate the target S3 prefix before the write, either with a Glue job pre step or a small boto3 call. If you‚Äôre using Iceberg, you can rely on table semantics instead, but for plain Parquet on S3, Glue won‚Äôt manage overwrite for you. Unfortunately, if you need real overwrite semantics, dropping down to a standard Spark script is often the cleanest option",1
1qdb1gg,1,"As for me, at some point people stopped supervising me and started to ask me to own data pipelines and mentor others. At that moment I understood that I was (at least) a Senior DE in that company.",52
1qdb1gg,2,"Probably ruffling lots of feathers but I think YOE is antiquated. It matters more about what you can do your ,decision making process, architecture choices, stakeholder management. Naturally this improves with YOE but some are goated out the gate.",33
1qdb1gg,3,"Being very simplistic:

* Juniors work on tasks and ask more than say. Juniors say YES a lot.
* Mids write tasks, discuss solutions, help Juniors... Mids say PROBABLY a lot.
* Seniors decide what and how to do unblock others, talk with other teams. Seniors say NO a lot, and write less code than the previous ones.

To know deeply about something can make you an expert without being a senior. Funny, right?

You can be a senior after 2y in a company, but not yet outside of that company. Be careful with titles.

![gif](giphy|tGED68BogsXXcocK8l)",15
1qdb1gg,4,"It just kind of happened out of the blue for me. After about 8 years, I joined a new company where I was on a team of 5 other DEs. I had mostly been a team of one and wore many different hats up until this point, so I was never actively comparing myself to other DE's. After 2-3 months I had established myself and most of the team would run things past me/listen to my recommendations on how to tackle things. I felt like the grown up in the room and slowly leaned into it until my boss suggested that he wanted to put me on track to be the senior at the next evaluation period.  
My quick answer would be that I went from ""my ideas are dumb compared to these guys"" to ""these guys ideas are dumb compared to what I would do""",7
1qdb1gg,5,"I started telling people I was a senior when I realized My skills were better than the senior colleagues I was working with, including my manager.",5
1qdb1gg,6,"When i started complaining about the problems before we ran into the problems. Also being able to talk about different options and work through things in meetings.

I feel a lot of people think its how many tools you know or writing an AI generated SQL thing on Linkdin.",3
1qdb1gg,7,Lots of folks here identifying total valid moments when they realized. I realized after coming back from PTO and lots of people had questions for me about stuff that you wouldn‚Äôt reasonably document. I think a common theme here is seniors drive up team velocity either by writing a ton of code or with more soft skill approaches.,3
1qdb1gg,8,"This is based on my experience living the data-centric role all these years.

YoE is subjective. Some people have 5 years and that mean different problems with higher and higher scale every year. Some other have 5 repeated same years. That said, to achieve certain title, sometimes you do need to ""wait"" for certain amount of years to pass, unless you are able to **get** & **execute** a wide-scope highly visible project.

As you grow throughout the years and solve more complex problems, inevitably you will encounter new tech stack or rethink your data modeling approach, so you will have organic grow. But more important than arsenal of long list of tech stack is your structured problem-solving skills.

Last but not least, communication skill. Usually senior-level engineer is able to communicate values and communicate complex tech in simpler terms. *Personally this is something that I keep on striving too* :).

  
Good luck!",2
1qdb1gg,9,It varies at every company. It‚Äôs when your corporate  overlords decide you‚Äôre ‚Äòworthy‚Äô of the title.,2
1qdb1gg,10,After I can handle a whole complex tech design and colleagues start to ask me for help when they encounter some problems.,1
1qdat9l,1,Better venue = more attendees? You pick whom to invite?,2
1qd6cg5,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qd17gs,1,"Honestly, I have essentially the same solution, except I do it in Notepad++ so I have little blue ""important"" bubbles for the things that absolutely must be done next. Other than that, yep, markdown, numeric prioritization, and daily lists of what I've done with little custom tags for later analysis, since I definitely don't have time to write up actual tickets, and every once in a while somebody asks me to justify my continued existence. Every couple of years I save it off for archival purposes and start a new one.

It's about 16k lines at the moment, as I recall. 

Solidarity, friend",5
1qd17gs,2,"I highly recommend you move all your work to something with visibility to anyone that is asking you to do anything. And any work you do has to be on there. You can treat them like generic work items with tasks. 

If you don‚Äôt have visibility into your work load there is almost no chance it will get better. If you have a visible prioritized workload it is much easier to say ‚ÄúBob said this is top priority and put it at my #1 as you can see. Cindy if you want this sooner go talk with Bob or whoever is at #2‚Äù

Plus the better visibility and tracking makes it easier to generate justifications for more people. 

It turns the conversation either your manager from ‚Äúwhy aren‚Äôt you getting anything done and people are complaining to me‚Äù to ‚Äúwow I didn‚Äôt realize you had so much work‚Äù",3
1qd17gs,3,"* Say no to new work. Or make it clear that it is low priority and is unlikely to be looked at.
* Track investigation and validation work on your team board. It is real work and should be visible.
* Even if there is no official process, set up a regular meeting to agree a realistic list of priorities for the next week (or two weeks) with your manager. Then stick to this.",1
1qd17gs,4,"markdown file for tasks gets wild fast, i switched to Notion for a bit more structure but not overwhelming, search is a breeze, also DataFlint can handle spark job alerts if that‚Äôs a pain, i keep my TODOs there too.",2
1qd0f6o,1,How are you authenticating to ADLS in the container? I've got it working with GCS by authenticating as part of setting up ducklake as a resource.,2
1qd0f6o,2,"Are you adding docker-compose env vars to the dagster.yaml run_launcher config?

Add env_vars under  run launcher like this

https://github.com/dagster-io/dagster/blob/master/examples/deploy_docker/dagster.yaml",1
1qd04ke,1,I usually get calls for junior mid and senior. Recruiters just throw out emails/calls,7
1qd04ke,2,"Just use common sense.  If you're getting better offers, take them if not, take what you can get to pay your bills.  It's a bummer, but it's not rocket science.",3
1qd04ke,3,I just got a midlevel offer this week but i didnt see much for midlevel. Plenty of senior which i applied but didnt make it far.,1
1qd04ke,4,A few,1
1qd04ke,5,"It‚Äôs definitely a weird market right now. Recruiters seem super focused on very senior hires or want folks to run before they can walk. For mid-level or ‚Äú#2/3‚Äù roles at smaller companies, you‚Äôll probably have better luck networking directly‚Äîthink reaching out to founders on LinkedIn or poking around in industry-specific Slack groups and subreddits. Smaller companies often don‚Äôt even post those jobs publicly, or they post in niche places like r/startups or on platforms like AngelList. The ‚Äúblack hole‚Äù apps rarely lead anywhere unless your resume matches every bullet, so targeting your search can save some sanity.",1
1qd04ke,6,"  
sometimes recruiters just skip over profiles that don‚Äôt scream their exact wishlist but upskilling a bit changes the game. i jumped into spark optimizations with DataFlint recently, pretty chill learning curve and small teams dig those skills right now. keep your cv a bit tweaked for each app, kinda annoying but it works, and don‚Äôt sleep on those direct-company career pages or even folks like Datacamp for a quick brush up.",1
1qcz83k,1,"You're going to be asked to explain your answers. I'm guessing the questions will be set up to have multiple answers and they're going to ask you why you chose one method over another. They'll ask about the efficiency of the code and get you to explain what each line does.

There's two reasons for this. 1) Interviewers know that interviews are stressful. Writing SQL queries on the spot is very difficult and because of that it is not reflective of your actual performance. 2) They know people are going to LLM the answers. This is a very easy way to catch people who dumped the questions into an agent but didn't bother looking at them. They won't be able to explain what the code does, or they'll do it slowly as they look at it for the first time. Either way, easy to catch people.

Getting someone to do the task first, then getting them to explain it in the interview both takes away the stress of trying to problem solve on the spot and lets you display your problem solving and communication skills by discussing how and why you chose the solutions you did.",5
1qcz83k,2,"Yeah, that seems odd, unless they plan to have some ad-hoc questions tossed in too. Kinda like an early filter step, like ‚Äúif they can‚Äôt even figure these out when we give them ahead of time, then we won‚Äôt proceed with the second round of questions‚Äù. Beyond that it does seem kinda odd.",1
1qcz83k,3,That certainly isn't typical. Chances are high that you'll be asked to explain your solutions in some detail or there are follow up questions on your solution.,1
1qcw5qe,1,i feel like people who say we dont need facts and dimensions anymore are engagement baiting,109
1qcw5qe,2,"I work as a data modeler and can say this, data models are more needed now than ever.¬†

Most of the companies build pipelines without models, now they're all facing issues and they don't have backward traceability. Everyone rushed pushing pipelines into production without proper models, processes, conventions and standards in place. Data Modeling is not easy skill to obtain and requires lot of effort, time and multitude skills.¬†

My current teams uses data vault and dimensional modeling frameworks, it takes time to get to final data marts and views on top but we rarely have pipeline issues (DBT, Snowflake). We spend lot of time upfront, which saves lot of money and reduce development time and effort down the line, which is the right way of doing things.¬†

When we face any ELT issues, then we go back to the data model and analyze on how to de-couple, optimize the model without breaking the grain at times. It saves lot of load times in some of those big fact tables. ¬†The issues I also noticed and I made those mistakes as well, shove tons of metrics into a fact table and calculate them at the fact table level. Instead those metrics should be calculated at one layer up (business vault or raw vault) layer and just load them as is into fact table. Fact table should be a simple select * from xyz tables.¬†

There's so many things that can go wrong in a pipeline and data model can solve many of those. We normally do a hands off to our DE's of our data models and mapping docs, it makes their life whole lot easier and efficient at times.¬†",34
1qcw5qe,3,"It's going to have to make a major comeback. As these companies realize NONE of their metrics (maybe the core metrics are ok) across departments line up. It's like a 10 year cycle, numbers are bad, spend 3-5 years moving towards strict data models and standards. As the business grows and no longer remembers those problems points the finger at slow development, leaders get replaced and the silo/tech debt starts over. 

I'm in the middle of one thats blowing my mind. Working on core metrics that all source from 5-6 dates, calculating the time between timestamps. Instead of defining those 5-6 dates with proper labels we expect the devs to get that same date whenever it's needed for a metric.... This isn't clean data and I could calculate these data points several different ways using different columns to filter. Sure they'll be close but those minor differences have cost companies millions when it distracts from the actual conversation.",16
1qcw5qe,4,"Totes. The idea that we don‚Äôt need modelling anymore could only be pushed by people who have no clue what they are on about, or grifters trying to sell you bullshit snake oil. Unfortunately CTOs will lap it up because data modelling takes time and expertise, and therefore money. AI produces superficially easy results that appear reasonable enough on first glance. But AIs don‚Äôt actually know anything about your business so they will confidently tell you things that are completely wrong, or which are true but not the correct answer to your question.¬†

Data modelling is not primarily a technical discipline, it is about extracting human-understandable meaning from what is otherwise just a bunch of ones and zeroes. It requires understanding your business deeply, and understanding what the business cares about and needs to measure. ¬†This meaning is often not directly available in your raw data. You have to translate it for a business user.

Now this is where the AI grifters will tell you ‚Äúoh sure the AI isn‚Äôt great at that now but it‚Äôs going to get better with better models‚Äù. And sure, it will. But even when you have better models that can more accurately translate the user‚Äôs intent into a correct answer, you will still have the problem that each individual user is having a separate conversation with an AI. Bob might ask for revenue figures for last month, the AI asks him how to define revenue and gives him a correct answer based on his definition. Jenny has a separate conversation with the AI and gives a slightly different definition and the AI gives her a correct but DIFFERENT answer.¬†

So how do you fix this situation and ensure that Bob and Jenny get the exact same answer for revenue? You have to make sure there is a source that has the correctly calculated and verified definition of revenue ready for the AI to use. And what do we call this process? Data modelling!

If anything, data modelling only gets more important in the age of AI. Since in the past when you had highly technical data and BI analysts answering the questions for you, you could rely on them having enough knowledge and expertise to work around the complexities and problems of the data. But in a world where every Bob, Jenny, and Harry with no technical knowledge expects to be able to ask the AI for answers themselves, you better be damn sure that it is working off a highly curated and verified source.¬†",6
1qcw5qe,5,"I agree with your point, LLMs actually demand more modeling complexity, especially when you are now adding structured data along with parsed documents and metadata. Maybe also coupled with a feature store for model inputs/outputs


It would actually increases the surface area for data modeling. Someone needs to decide how to represent extracted entities, where embeddings live, how to join LLM outputs back to source records, and maintain consistency across all of it.

On a side note, LLM on a kafka queue for analytics sounds like a classic ‚Äúthis type its gonna be different because AI‚Äù kind of a take, cannot even imagine how bad its gonna be",6
1qcw5qe,6,"The data hype train seems to be intent on forgetting and reinventing each and every wheel it has. That's because the people on the train want to get somewhere, but the train is owned by a wheel-selling company. This metaphor got away from me, kind of like a runaway train. 

The best use of off-the-shelf AI products is pointing them at really good semantic models. It's dashboarding that's dying, if you haven't fired your dashboarders yet it's time to start teaching them a little Kimball.",4
1qcw5qe,7,"Great topic to bring up. I share the same feeling that there is this hard push to avoid the problem altogether. 

Yes, let's jump on the datalakehouses, as medallion architecture is the one-size fits all solution. Forget about DWH, analytics, we'll leave the business logic to the visualization anyway. Only later they find out the dashboard slow and useless, metrics are useless but by then it's too late...

  
Every two weeks there is a new tool or concept that is rebranded that the community hypes about, LinkedIn is a primary example of this. Then it dies and people forget about it as something else comes up.",5
1qcw5qe,8,"Facts and dims is really more UX first then for whatever engineering reasons (even though less joins are a performance boost)¬†

People forget this. In kimball, he always talks about how the models should really cater for the end user and the qns asked

In the advent of AI, this is no different. If you really model well, the AI can easily write queries for common metrics because the base table is well name and understood. Then the rest of the refinement are really a bunch of where clauses¬†",3
1qcw5qe,9,"Data modeling always has been and always will be important.  It's not just one pattern though, it's sticking with whatever pattern makes sense for your company and ensuring it doesn't end up in a pile of irreconcilable shit.  I don't see a huge reason to do dimensional modeling anymore other than your downstream tools may  like it and it's useful if you have a lot of cross-subject rate calculations that benefit from conformed dimensions.  A lot of businesses don't need that though and are fine with stuff like OBTs and master data and those help them move faster.

I am also skeptical that we are going to see a lot of usefulness with LLMs and data any time soon.  I think AI is very useful as a tool to help build pipelines and tools, but not as much for accessing data right now.  Data models can be massive with lots of tables, columns and semantic descriptions for the content, so you end up flooding the context window and confusing the model (most of which is irrelevant context to the problem).  A lot of the definitions are weird so don't align well with the model's training and people also ask the same question 20 different ways, which can lead to inconsistent answers.  

The way I have seen people trying to make AI systems that work is by slowly building up the model and keeping it small (i.e. not the enterprise model you are used to).  So in the end it can only answer simple questions like ""tell me how much revenue we earned from product X in the last three months"".  Given how much time you spend building and tuning the model until you feel like it works most of the time, isn't it faster to just give them a dashboard with the same capability and better accuracy?  Also the user community is thinking that they will be able to prompt the model to do a deep dive that an analyst would spend a week on and that's just not reality.",3
1qcw5qe,10,"Lol! 

I came from being a controller to being a data analist/scientist to being a n00b data engineer but i can tell you one thing from my domain expertise, data modelling is key for success in any organization that leans heavily on data. 

Biggest issue i see with people just rushing the data from A to B is that they basically construct jungles in where employees end up defining their own version of the truth and thereby undermining a core principle of why you had that warehouse/lakehouse in the first place. Departments will build their own vision on top of the mess you supply them with. I had a discussion lately with a professor ranting how multiple versions of the truth can coexist but believe me, companies just want 1 version. In the end, you don't want to have senior level management have a discussion in a meeting about what a cost center is, what data is correct, on what date FTE stats are calculated for reports and what the source is of all of this (that, in effect, will surge your indirect costs, drop the trust in your solution and eventually have management question why in gods name they hired those engineers).",3
1qcuk7k,1,Standard? No. The documentation will tell you that new optimizations in spark and/or databricks make this not necessary.,3
1qcuk7k,2,"I can‚Äôt say whether or not it‚Äôs ‚Äústandard,‚Äù but I‚Äôve done this before. Although I only didn‚Äôt salt the others because I was lazy. If salting only certain keys gives you better results, then go with it. Otherwise salt everything. Compare your results. Salting should be relatively cheap too, and either way the benefits of doing so when necessary should make up for any overhead it induces.",2
1qcuk7k,3,"I would recommend that approach, or remove/isolate, if you can reliably identify the problematic key.  With Spark 3+ AQE does a reasonably good job at adjusting the plan if you have multiple or inconsistent keys to worry about.",1
1qcuk7k,4,What is the purpose of salting all instead of some? Or the purpose of salting at all? Asking for a friend. üòé,1
1qcuk7k,5,"salting only the most skewed keys works better, salting everything just drags performance. DataFlint helps by flagging heavy skews right away, you just get a clear picture of which keys need work without second guessing. PySpark can feel clunky when debugging by hand but having something like that makes it less of a pain.",0
1qcuk7k,6,"We have massive skew in tables because we store country wise data and certain countries obviously are 100x bigger than others. Downstream we need to store partitioned on country after processing by country so i usually leverage the partitioning in tables to only load a country's partition at a time in a loop. Not very best practice adherent but it works quite well for my use case. 

I'm fairly new and I've read a lot about salting but didn't need to apply it!",1
1qct0b0,1,"If you‚Äôre comparing it to GCP certs, Snowflake‚Äôs path is a lot more structured and gated.

SnowPro Core is the real entry point. Snowflake themselves position it for people with \~6+ months of hands-on use, and it‚Äôs explicitly required before you can sit any of the advanced exams. It‚Äôs broad but Snowflake-specific: architecture, micro-partitions, cost/performance tradeoffs, security, time travel, data sharing. Not hard conceptually, but very ‚Äúdo you know how Snowflake actually works.‚Äù

The advanced certs are role-based rather than general:

* Architect / Admin are very environment and governance-heavy
* Data Engineer is focused on data movement, pipelines, performance tuning
* Analyst / Data Scientist are narrower and more SQL / workflow oriented

They‚Äôre aimed at people with \~2 years of real Snowflake experience, and honestly feel more like validation than learning tools.

For recruiter visibility, Core is the one most people recognize. The advanced ones mostly matter once you‚Äôre already operating in that role.

If you‚Äôve done GCP DE/Architect, the biggest difference is that Snowflake exams go deeper on platform mechanics but stay inside the Snowflake box instead of testing broad cloud design patterns.",4
1qct0b0,2,"They are tough. Questions are ambiguous and exam time is very limited compared to the number of questions.

If you want to take any advanced cerification (like Architect), you must have a SnowPro Core first. And for every of them you must have previous experience in that area.",2
1qct0b0,3,"I only have experience in the SnowPro Core cert. I'm a regular Snowflake user and limited administration and I'd say 80% of the test was on concepts that I don't normally use in my day-to-day. However, all questions and answers are all in the Snowflake Docs so there's really nothing that you shouldn't expect if you look at the exam topics and any 3rd party practice exams.",2
1qcs359,1,"the CDC on MSSQL is not heavy if you have a low CUD occurring during the day OR if you have a server that can handle heavy loads

never did a server benchmark by myself but when I needed to create a CDC application connected to MSSQL just the first load that increased the DB usage by 30-35% based on what the DBA told me (because the full load of a lot of tables), after that, something between 5-10% of server usage increased",3
1qcs359,2,"It should have a minimal impact on your SQL Server.  We use this to track deletes, updates, inserts from SQL Server into our Snowflake environment.",1
1qcs226,1,"Retention is one of those things that looks easy on paper then becomes messy in real systems. What helped us was documenting retention by system type and sticking to one approved narrative, even if the answer is this is our current constraint. Consistency matters more than having a perfect retention story",6
1qcs226,2,"The only way to prevent it is to have it be a focus.

What I mean, is that the retention and destruction rules are collected in one place, and there is assigned responsibility for seeing that they are applied correctly.  This can be a specific person/team or it can be one of the checklist items (if your company is small enough) that every system owner is required to certify for acceptance.

And it has to include backups, logs and analytics.  It is a lot of work up front, identifying the rules and what that needs to look like, but the maintenance part of that is generally straightforward.

Honestly, in my experience the hardest part is getting the requirements clearly defined, as business users seldom want to give up old data; which is exactly what this is about.",5
1qcs226,3,"retention gets messy fast once backups, logs, analytics events, and archives are involved different systems, different people giving different answers one solid fix I've seen work treat retention as part of your core data model early on add explicit retention fields usse domain driven boundaries to group data by policy apply agile evolutionary modeling so changes are easy to iterate his cuts down on the inconsistency and makes audits way less painful check out r/agiledatamodeling small community but focused on practical patterns and war sttories",1
1qcs0x0,1,"I wouldn't bother myself doing so tbh, please ask your account team to raise that with the product team as this should be kind of managed network rule. please check this [https://docs.snowflake.com/en/user-guide/network-rules#snowflake-managed-network-rules](https://docs.snowflake.com/en/user-guide/network-rules#snowflake-managed-network-rules)",1
1qcs0x0,2,"Yeah, Snowflake Task + Snowpark External Access for those Azure IPs is a good move. For the internal static ones, you could try an S3 bucket and have Snowpark pull from there. Beats SharePoint, honestly.",1
1qcs0x0,3,"See [Snowflake Automated IP Address Extraction and Network Policy Update for Azure | by Dhaval Avlani | Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science | Medium](https://medium.com/snowflake/snowflake-automated-ip-address-extraction-and-network-policy-update-for-azure-a45e8a23212f)",1
1qcpucj,1,Try out marimo notebooks. AI integration is just one of many reasons to switch from jupyter to marimo.,2
1qcpucj,2,VSCode with a jupyter Kernel and Claude code open in the integrated terminal does everything you need.,1
1qcpucj,3,https://jupyter-ai.readthedocs.io/en/v2/,1
1qcoxbb,1,Use the open source tool ‚Äúdb extractor‚Äù - it‚Äôs crazy useful and comprehensive,5
1qcoxbb,2,"DataGrip does this. But if you lack foreign keys, how do you expect the modeler to infer/diagram the relationships?",4
1qcoxbb,3,"Completely flippant, and unhelpful.    
But if your schema is like mine (external contractor code we took over), just substitute a picture of a pigeons nest and nobody will see any difference.",8
1qcoxbb,4,">uses some degree of AI to read table schemas and generate ER diagram

Why tho? If you must, you can grab DDLs and ask an LLM to generate code-first diagrams in something like PlantUML, but it *will* require manual editing in my experience.

DBeaver ER diagram is decent but falls apart in complex schemas, as, honestly, do all schema ER diagrams in massive databases.",5
1qcoxbb,5,"I‚Äôve been in this hell just recently, trying to reverse engineer an undocumented model out of a gigantic pile of unindexed excrement in SQL Server 2014.

If you‚Äôre thinking ‚ÄúI can only optimise when I have the entire logical and physical model mapped‚Äù, you‚Äôre kidding yourself. It‚Äôs never happening, don‚Äôt try.

My advice: be practical and treat the old stuff like it‚Äôs cancer that needs to be cut out.

Don‚Äôt try to document the mess you have. Instead build or fix things one piece at a time¬†and document that as you go. Build in parallel, dual run against the old things, and when you know the old tables and batches are redundant, take them out back and shoot them in the head.

Also, forget AI. It knows when I‚Äôve forgotten to close a print statement in my shitty code, but it doesn‚Äôt know architecture or systems design.

If you want to automate this, try the missing index reports.

¬†https://learn.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-db-missing-index-groups-transact-sql?view=sql-server-ver17

Be wary, this reflects what queries are written and run (probably by the same useless engineers that made this mess) rather than the actual logic and definitions that matters. So a badly written query moving data no one needs will be boosted in the output ranking. But it‚Äôs the clearest indication you‚Äôll get of what the entry patterns into your tables are and what is missing and will help you prioritise.",2
1qcoxbb,6,Recommend dbdiagram.,2
1qcoxbb,7,"Regarding the part about all tables being heaps: note that, depending on your workload, those heaps may cause the rest of your workloads to take [more time](https://vladdba.com/2025/12/30/the-hidden-costs-of-heaps-in-sql-server/) than they save during the bulk load.",1
1qcoxbb,8,You don‚Äôt need AI to read table DDL. It‚Äôs explicitly laid out in the script for the tool to import. That said erwin does this extremely well.,1
1qcoxbb,9,What ever happened to viso? They used to have this market cornered. I guess maybe the fact we no longer build out databases with FK and etc that made it powerful,1
1qcoxbb,10,"Erwin and ER Studio are great at reverse engineering databases and documenting them (as well as designing new ones).  If you can find a cheap copy of either somehow I would check it out.  Can't remember the name, but there was a newish saas provider doing that too, but they changed their pricing model to something stupidly expensive when they used to have reasonable individual account options.

You  could also use AI to generate something if you just want pictures and the metadata.  SQLGlot is a great library for dealing with SQL conversions and generating code and there are plenty of visualization libraries that an AI coding tool could probably slam together a reasonable app for you.  This was actually one of my back burner coding ideas with Claude, but don't have the time to do yet.",1
1qcmttf,1,"There's >10 ""easier and faster GA alternatives"" out there. Just to name the 3 most popular ones:

1. [https://usefathom.com/](https://usefathom.com/)
2. [https://plausible.io/](https://plausible.io/)
3. [https://rybbit.com](https://rybbit.com) 

Many are open source and can be self-hosted for free, have free-tiers of their hosted platform, or are pretty cheap to start.

Are you building just to learn? Then go for it! Take a look at the repos and see how these folks did it.

Your suggested architecture is pretty close. You need a script in the browser to capture events, somewhere to send the events to, a database to store and query the events, and a frontend to display charts to the user.

If you're thinking about building a product, what are you going to do differently than any of these have already done? If you have no technical background, you probably won't win on experience, performance or cost. I'd consider finding a different area where you can add value.",1
1qcmttf,2,"You have the tip of the iceberg, your problem is what's below water, and you absolutely need an engineer, preferably several to get started. You can't 'vibe code' this and be successful. You have the absolute bare minimum outline, and it can work for personal web site scale. Beyond that, you will be cooked. Cloud autoscaling is much better than it used to be, but only at a per-service level, and the deeper into your pipeline you get, things are like dominoes. Schema can also kill you; you absolutely need an experienced engineer to help you get that schema right. There will also be countless integration details and knobs to turn throughout the pipeline to ensure it doesn't crash, and you'll need to know the exact implications of each one. Data has a shape, both horizontal and vertical, and neither one likes sudden changes midstream. How will you handle deploying those changes? What happens if you need to roll back a change? How will you remember what changes you made? Do you understand CI/CD and why it was developed? How many tenants do you expect to onboard to this platform? How will you keep their data separate and secure? Some of these are very basic engineering skills, but we have them for good reason, and they're skills learned over time, and often painfully. Lost data is lost money, and when it's gone, it's usually gone.

I don't mean to discourage you. Something like this is a fantastic learning project, but you really need to learn not just the code behind it, but exactly how the data flows through the system. We don't worry so much about Big O problems in the age of cloud computing, but similar math problems are still there (unless you have an unlimited budget). An SRE can also be helpful.

You can have this fast, cheap, or reliable. Pick two.",1
1qcmttf,3,ClickHouse is absolutely the best for this so you're right there.,1
1qcmttf,4,"for big data scale, spark always pops up and things can get messy fast, dataflint is like that backstage helper, checks your spark runs, flags weird stuff saves headache, if you ever shift gears into spark land, this tool is peace of mind, but yeah love your stacked choices",1
1qcmttf,5,"Fivetraner here! Your understanding is broadly right üëç  
One thing to add: tools like Fivetran wouldn‚Äôt help with the SDK or event ingestion itself, but they *can* be very useful once events are in your warehouse/lake. They handle syncing ‚Äúeverything else‚Äù (Stripe, CRM, ads, support tools, DBs) into the same destination so users can join product events with business data without you building/maintaining tons of integrations.

For event collection specifically, you might also want to look at Segment/RudderStack/Snowplow as reference points.",1
1qcl76i,1,Apache Superset,2
1qcl76i,2,"well, sometimes it helps to keep things super simple when picking BI tools, i did this last year for a team about your size and started with Metabase again but also tested Tableau Public and Preset, all work fine with Redshift. for checking why some dashboards are slow or weird, especially if you use Spark to load Redshift, you should look into something like DataFlint, that helps you see where data is stuck or wrong, makes fixing faster. a lot of people forget to check the ETL side until things break, so i think using a tracker tool is smart. try not to pick only one tool right away, maybe let a few people try both and see what feels faster or easier for support. it‚Äôs cool you‚Äôre asking for ideas, the little details matter more than brand names most times.",2
1qcl76i,3,"Honestly given your size and stack your instinct toward Metabase is solid. It checks a lot of your boxes without overcomplicating things.
The only time I‚Äôve seen teams move beyond it is when non technical users want more guided exploration and standardized metrics across teams. That‚Äôs where tools like Domo start to make sense but they come with more governance and cost. For 70 or 80 people I‚Äôd start lightweight and evolve",1
1qcl1rh,1,My team uses open source version of Cube. Just spent several days to setup our mcp server for querying data and building charts.,4
1qckbji,1,When the skills will bring you more money in the future,8
1qckbji,2,"Personally I made that choice when I was making the most money. When it started feeling a little too comfortable, I chose to expose myself to harder skills on the human side (management/sales). To each their own, but personally for me it‚Äôs paying off now",4
1qckbji,3,"don‚Äôt think of it as skills vs money. 

you use skills to get money.

so if you‚Äôre learning new skills then in theory you are investing in getting future money.

so you have to determine how much the new skills you‚Äôre learning will be worth in the future.

if you can jump ship now to get $20k is it worth staying to jump in two-three years and get $40k? that sort of thing.",2
1qckbji,4,"Impossible to give a general answer, but there will typically always be an opportunity to keep learning regardless of current employer. It‚Äôs typically a trait of the individual to keep challenging oneself. If you are unhappy with your compensation, test your value in the market and trust your gut.",1
1qckbji,5,"The fact that you're already asking that question means it's probably a good sign to start moving on now lol.  
  
If you're already confident enough that you can command a higher salary with your current skillset, then you can start putting yourself out there already just to check your market value or if an employer can pay you with an amount that you're happy with. Based from your statement, it seems that you already are.

It's not always one or the other (skills vs money). You can always learn highly sought after skills in another company that pays you well. It's more rewarding to work this way. If you only have one of them while the other is lagging, there will always be a lingering feeling within you what if you could be more. Trust me, I've been there a couple of times already.",1
1qckbji,6,"Every two years in any IT field. 

Use an headhunter HR agency and they will tell you what you‚Äôre worth based on last person they placed. Like Robert Half. 

In a very large company, hired as a programmer/SQL dev 15 years ago, my starting salary was twice that a woman there for 10+ years and she trained me. 

When I left I sent her my new accepted job offer and given 2w notice, I sent by email to her on my last day, with a pdf of job offer, with the text: this is why I‚Äôm leaving. I was underpaid. That‚Äôs when she told me her current salary. Half of my starting. 

F big companies. Managers get bonuses for not giving salary raises to their employees.

HR doesn‚Äôt make market adjustments for engineers.",0
1qckbji,7,"skills matter but money does too, if you work with spark look into something that can automate fixes like DataFlint or check out what others use, that saves a lot of time, you need to make sure new skills get you better jobs or more pay somewhere else if it‚Äôs not happening now, keep learning but don‚Äôt let companies take all the value without giving back.",1
1qcdtes,1,Why is everyone saying dbt is that not an elt tool rather than etl?,18
1qcdtes,2,"Not many reasons to pick a single ETL tool anymore. The common pattern is managed ingestion + SQL first transforms. That keeps pipelines simple and also debuggable. Also much cheaper to maintain.

For ingestion, you go with Airbyte (open source) or maybe Fivetran (fully managed, pricy). Integrate.io if you want solid connectors without running infra. Then transformations live in dbt or native warehouse SQL. This setup handles schema drift, retries and incremental loads without locking you into a giant proprietary stack. This should make the most sense for your case.",25
1qcdtes,3,I suggest an LLM chatbot that makes up numbers.,6
1qcdtes,4,SSIS - /s,10
1qcdtes,5,"Spark declarative pipelines?

https://spark.apache.org/docs/4.1.1/declarative-pipelines-programming-guide.html",6
1qcdtes,6,Dbt + airflow or spark + airflow would be good depending on your use case.,13
1qcdtes,7,I‚Äôve used both dbt and airflow but I prefer sqlmesh and not airflow. I know they both have big market share but don‚Äôt any of you guys hate maintaining airflow? It‚Äôs miserable. And as a side note Spark is overkill for most companies that use it.,8
1qcdtes,8,Best tool goes with your business requirements and SLA like one tool fits all businesses.,3
1qcdtes,9,Python & prefect OSS and Airbyte OSS deployed kubernetes,4
1qcdtes,10,"I think using Azure services like ADF, adls gen2 with Databricks beats any tool in 2026. Databricks has upgraded its capabilities in a fascinating manner with things like Delta live table using autoloader. I would highly recommend learning databricks if you want to stay relevant in 2026 and forward.",2
1qca3r2,1,"I'm in a similar position tbh. 

Honestly I think a lot of people are actually over skilled for the jobs they're doing. Management say 'I want real time' and a data engineer hears 'use Kafka' - but what management actually mean is 'i check this every morning and want it to be up to date' so you could probably get away with orchestrating a batch update every morning.

I've also seen people use pyspark for data that is easily small enough to handle without spark. They just think 'but this is what everyone else is using so I should use it'.

The annoying thing is, if you have good judgement and figure out the most appropriate tech to use, that can get in the way of upskilling if you settle on a simple, cheap but more mundane alternative.

If you want to learn some of the things you listed:
- if you're using snowflake, why not just start using dbt + git to manage your SQL?
- how is data getting into snowflake in the first place? If that looks quite manual at the moment, maybe try setting up airflow to schedule data extraction / load

I think you could probably find opportunities to do things you want to - obviously not all of it but maybe have a think about what's possible",13
1qca3r2,2,"You're fine IMO

Understanding what breaks, what costs real money, what eats all the memory / CPU / disk IO and how to fix it is a useful starting point, the tool you choose is just a detail of that.  Once you have that understood the next big thing is fixing stuff which is broken / diagnosing issues / defensive coding in advance before it breaks - here you learn patterns, what can go wrong, why backing loads off and re-running isn't magic.  From there you should be able to design decent overall data models, talk with people in detail about what approach should be taken and point out issues in advance before anything has been coded, at this point you're working as much on people skills as hard technical skills.

It's tempting, because of the flood of shitty job ads, to think you need tool X or intricate knowledge of language Z, but the reality in terms of being capable has nothing to do with those, unless you're a typical recruiter.",3
1qc8xe0,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qc8xe0,2,Python is they key. You may use Java if you want to create a custom connector in spark of spark-based system(Databricks).,6
1qc8xe0,3,"Not.


Most DE prefer python and sql. Go/java/scala are nice.

For spark most teams write python, because its what most other DEs know. You shouldnt have to write scala or java for it, unless you plan on writing low level stuff.",3
1qc8xe0,4,Python and SQL are all you need. Everything else is noise,3
1qc8xe0,5,"I think you are fine with the skills you have. Most industries DE have swung to python.  Scala came around in the early days of spark and had slow adoption. Pyspark came along and those already using python for othe projects switched to it.

As for java, a majority of your coding training with Gen AI default to python.

Edit: forgot to add Snowflake badge training has you do some coding in python also.",2
1qc8xe0,6,"Scala is not dead yet it's still used for Spark related jobs. I write Scala code for different Spark jobs. However, it does seem like organisations are moving away from it. Databricks for example, mostly focuses on Python and SQL when it releases new features. Scala is more or less pushed aside. 

I think for DE knowing Scala could still be useful but it's probably more advantageous for those who are aiming to go in other software niches. Scala is a different beast altogether when coming from Python only background. 

As for Java, it's a widely used technology, not particularly useful for DE anymore. It's still one of the major tech used for backend development. It'll be fine.

As for Go, I don't know how widely spread it is in Data Engineering.",2
1qc8xe0,7,"I love Scala. It‚Äôs my favorite language. It hurts me to say it, but you really don‚Äôt need it. Python is far more important.

I can‚Äôt think of anything you‚Äôd need Java for tbh. If I‚Äôm doing anything in the JVM I‚Äôd default to Scala.",1
1qc8xe0,8,"Java and Scala are mostly used at companies developing other software products. Mastering these programming languages is very different from DE. If you know DE principles the programming language used doesn't even matter that much.

For DE stick to Python, much nicer to work with. If you want to become a backend/full stack software developer go do Java and stuff.",1
1qc6zzq,1,"If you have visa issues then maybe opt for MS, otherwise it's not worth it. Most of the MS these days are cash cows and don't provide much ROI.¬†

MS in Applied Math/Stats or GIS are good.¬†",2
1qc6zzq,2,I don‚Äôt see any logic in doing masters when you already have 3-4 years of experience in DE. You either go for certifications or find good mentors for 1:1 learning. Feel free to connect for any help,1
1qc67yw,1,"Excel for ingestion, Excel for transformation, Excel for serving, all orchestrated by Excel.",127
1qc67yw,2,"Snowflake and databricks are the two cloud warehouses I would focus on. I would also want a hire to have some onprem SQL experience. In this realm PostGres makes great sense to learn. Other skills I would want a candidate to have are scripting language experience Python being the most inportant. Powershell and bash being great as well. In Python I would like experience with the common DE packages like SQLAlchemy, pyodbc, polars, pandas, requests, pyspark, etc.",23
1qc67yw,3,Microsoft Excel + VBA + Windows Task Scheduler,40
1qc67yw,4,"My general impression from job postings is

Snowflake for tech type companies

Databricks for more established companies trying to be high tech. 

My sense is you can't go wrong with either, so pick which one works best for your company. Do you have a clear idea there?",28
1qc67yw,5,"just master sql and any etl.tool, with a bit of python and you'll be fine. super advanced sql will never go out of style",12
1qc67yw,6,No duckdb/duck lake fans? Seeing a lot of companies who have a lid on over engineering going with that.,8
1qc67yw,7,"Airflow and Spark (obviously Python and SQL). Bonus points for Table Formats like Delta and Iceberg is what‚Äôs hot right now from my perspective. Also dbt. BigQuery is another one I see often. People always talk about Snowflake but honestly, doesn‚Äôt seem like it‚Äôs super in demand right now (unfortunate for me lol)",6
1qc67yw,8,"you won't want to hear this, but knowledge of a legacy system like SSIS, powercenter, or ODI, and on-prem mssql or oracle sql, can get you a lot of jobs. there will be organizations stuck here who don't want to change, and others who want to do a conversion to something modern. boom. lots of jobs. that you probably don't want. but the conversions especially are good career builders.

it seems so random which target data warehouse an org will be interested in that I don't think it mattes that much. I am at an org that is moving from a legacy system to GCP, and we have added colleagues who worked on a completely different legacy system and a completely different modern product. it works out.

for the modern stack, focus on the free squares. airflow and dbt are ubiquitous and not going away. mastery of basic python and bash is also helpful.

bigquery has an always free tier which is quite generous, and their offbrand version of dbt is also tightly integrated. it's an easy way to learn for free. they charge money for composer, so you'll have to use a trial or local docker if you want to experience airflow. I do have a lot of colleagues who previously worked with snowflake and loved it, and I haven't met a soul who ever worked on databricks or is interested in having it at my current org.",6
1qc67yw,9,"To optimize for the *number* of available job applications, I'm thinking:

- data ingestion: Fivetran or Airbyte or maybe even Meltano (which is probably a bit more rare, but good for very cost sensitive companies)
- orchestration: Airflow
- warehouse logic: dbt
- warehouse engine: Snowflake or Databricks, I do see a lot about BigQuery and GCP, but I don't have enough knowledge about how prevalent it really is.
- cloud platform: AWS
- transactional db knowledge (not always required for DE): I still think PostgreSQL is king here


I think most companies don't truly need streaming, but it you're interested in it from a resume-driven-development perspective, then perhaps RabbitMQ Streams or Kafka or Flink",3
1qc67yw,10,"Dbt and Airflow. May not be great for the future, but good for now",5
1qc4x46,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qc4x46,2,"Learn what they do, not their names. SQS in AWS, Pub/Sub in GCP, Events in Azure... same functionality, less scary.",13
1qc4x46,3,"Databricks and Snowflake are on Azure aswell. For more code heavy teams AWS is better, Azure is meh in that point unless you go C# route",6
1qc4x46,4,"At 5 YOE, the stack matters less than you think. Most decent teams don‚Äôt hire because you know ADF vs Glue, they hire because you understand pipelines, data modeling, failure modes, and tradeoffs.


If you already invested in Azure, don‚Äôt throw that away. The concepts transfer really well. S3 vs ADLS, Glue vs ADF, Redshift vs Synapse are different names, same ideas. Picking up AWS on top of Azure is way easier than starting from zero.


For getting your foot in the door, I‚Äôd stick with Azure as your ‚Äúprimary‚Äù stack, but get just enough AWS exposure to be conversational and not blocked in interviews. You don‚Äôt need to master both.


Also, product companies preferring AWS isn‚Äôt a hard rule, it‚Äôs just more common historically. Plenty of good teams care more about how you think than which cloud you used last.


So no, you don‚Äôt need to panic learn everything. Deep fundamentals¬† amd one strong stack beats shallow knowledge of two",2
1qc4x46,5,"If you want to be cloud agnostic, then go for Databricks/Snowflake.
If you want to specialize in cloud specific technologies then pick any of them.",2
1qc4x46,6,"I‚Äôm always a fan of choosing job orchestrators over drag-and-drop tools or vendor-locked, cloud only orchestration services. You can self-host Airflow/Prefect/whatever in Azure just the same as you can in AWS, or else use a cloud version of Airflow/whatever like say with Astronomer.io. Although AWS is bigger, Azure is still huge and I see a lot of Azure posts in here.

By the way, if you DO like job orchestrators and your company you go with uses C#, I‚Äôve been building a C# job orchestrator (inspired from the Pythonic ones) called [Didact](https://www.didact.dev). Would love for you to consider it if it fits your use case/coding preferences.",1
1qc4x46,7,"We had years of issues (mostly dns) on AWS until Jan 2023 when we moved out into Azure. 

Also to comply with data privacy rules, Azure has Canada only redundancy. 

Azure was cheaper overall, VMs better selection. Especially for legacy SQL Server we still need to support for another year or two. 

So for countries with privacy laws on citizens, Azure all the way. Plus, eff u Amazon.",1
1qc4x46,8,Anything but Azure.,-1
1qc0nwe,1,Quite low for senior,11
1qc0nwe,2,"Unfortunately, CDN employers are just cheap ass, with an exception to FAANG. CDN employers just can't get their heads pass the $90k mark and they like to stick with it. It's almost like all the HR's have a secret meeting with other HR's across the nation and fix on that number.¬†

I worked at Air Canada with $90k salary. I later came to know that my senior lead was paid around $120k or slightly higher, despite of his much seniority level and the depth of experience he had.¬†",11
1qc0nwe,3,"This is ridiculously low. I noticed this pattern with Canadian companies. I used to work for a US based startup as an intermediate data engineer before covid, and they paid ~140k + equity. 

Take the job and keep looking for alternatives. Low paying companies don't deserve your loyalty. I really hope you find a better paying job soon.",4
1qc0nwe,4,"Juniors at my company in the Canadian division makes that range with 5% bonus.

Senior with 5+ years of experience should start at 120k especially if it‚Äôs Toronto area.",3
1qc0nwe,5,"He‚Äôs seriously lowballing you yo. 

Senior data Eng should be closer to 130-150+ minimum in To depending on your seniority. 

Also banks likely pay on the lower end.",3
1qc0nwe,6,"It depends on how much equity and bonus you would get.

For a senior role $150k-250k all-in (with equity and/or bonus) is doable with top roles making $300k+. However, Senior is a loaded term and I'm assuming you have many years of experience. If you only have a few years, low 100s is in-line.

Use levels.fyi for more detailed salary information.",1
1qc0nwe,7,That sounds low to me for Toronto,1
1qc0nwe,8,"IDK man, I'm a Senior Data Engineer in Texas and I make $175k a year.",1
1qc0nwe,9,For senior it should be around 145 - 160K + bonus + equity if applicable,1
1qc03rf,1,i think you're overvaluing certificates if you don't have prior DE work experience,1
1qby5px,1,"You already spent 3 years in data engineering, practise more, learn cloud technologies, build projects, if you are stuck find good mentors and keep applying for jobs, you will be in a better place. Happy to help if required",11
1qby5px,2,"Looking for job is a job itself. Your credentials look fine. Practice interviews, both regular questions and tricky tech questions.",5
1qby5px,3,"I would recommend spend time preparing for the certifications and create a lot of hands-on project. After each project, share your learnings in the form of a blog. 

You will learn a lot of new stuff, gives you more confidence, creates online visibility, keeps you busy and avoid being jobless getting into your head.",2
1qbx8ay,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qbx8ay,2,"You do not need to know how to do everything without AI. What matters more is knowing which tools solve which problems. Using AI is completely fine as long as you understand the code it produces and could reason about it yourself.

Same idea with tooling. A GCP or AWS cert is useful, not because you will be an expert in every service, but because it expands your mental map of what tools exist and what they are good at. You can always go deep and learn them properly when you actually need them.",29
1qbx8ay,3,"You need to work on your salary negotiation skills. You're lacking that mercantalist attitude and, without realising that the market is pretty solid right now letting organisations pay you a junior level salary (yes, a junior salary for companies with a strong tech culture) with 4 YOE.

This isn't always a hard skills problem. This is also a softskills problem. Sorry to break it to you but markets do not operate on merit alone. It's also who you know, who you get to know, and timing. I was working a junior salary for the longest time (40k a year junior dev) and then I went right up to 170k/year as of today. I was lucky to have the Elixir slack group back in the day sit me down and have this talk with me and make me realise I'm letting myself get hustled (or fleeced if you're a northerner like me :) ) because I lack the self confidence to apply for positions that pay a respectable income, in the UK, above what a copywriter makes. 

I think you need to work more towards becoming a software enigneer and less of a tooling engineer. But even with your experience I'd estimate you should be able to pull at least 30k more in more of a devops role. There are a LOT of companies with broken data pipelines. Your job is to get street wise.",35
1qbx8ay,4,"!RemindMe 7 days ""Check this thread""",9
1qbx8ay,5,"After 4 years of experience I would say the skill that will level you up the most is ProductSense. ProductSense means that you have the ability to work backwards from the product to the data pipelines. Understanding what needs to be measured, how to measure it, and then building the pipelines.

Some core skills that you need to be really good at in todays modern data tech stack world are these:

- Strong SQL, i.e. its like reading and writing english
- DBT, don‚Äôt let a new tool intermediate you, it‚Äôs just SQL queries orchestrated in an autogenerated DAG. 
- Data QA, building pipelines where data quality tests are first class citizens. The moment your dashboard shows one wrong number people instantly loose trust in your data assets. Better to delay data availability then delivering and redacting later.

Some tools to look into:
- Airbyte for data ingestion
- Superset and Metabase for BI tools
- Duckdb",6
1qbx8ay,6,"Dude you literally got the basics right.

You know enough python and sql to make things work and you already work with one of the 3 big cloud providers (gcp). 

The big issue you've got is that of confidence. Unless you want to be in the top 1 percent of data engineers working for a faang like company, your skillset should be good enough to solve a company's problems from a data engineering perspective.

You could study a bit of data modelling (Ralph Kimball is the OG in this field, so I recommend studying his approach) but otherwise stop selling yourself short...

Another alternate career path is to look up Analytical / BI engineering and see if you are a good fit there...",3
1qbx8ay,7,What sort of problems/questions are you failing in the technical assessments? I would work on that first.,2
1qbx8ay,8,"For the learning part just find good mentor and emphasise on learning cloud a little more. Data ricks, azure, pyspark are basic necessity",2
1qbx8ay,9,"You have too many years doing the same thing. It explains your salary, which should be higher at this point. But professional experience is not about time alone, so my recommendation: SQL is everywhere. Get really good at it fast. All the rest you said is too much in one plate. Pick a lane. Stick to SQL and narrow down that list. Again: GET GOOD IN SQL.",1
1qbx8ay,10,"Any tips on someone who hasn‚Äôt had luck on employment within this industry? Trying to get into data analyst / science / engineering roles with little experience in data science apart from stats, maths, physics, and basic SQL / ML",1
1qbw9su,1,"Hey,

hopefully I can give you some inspiration üòâ. As always, there are many solutions, so here is one of them:

>using dbt with an orchestrator

Even though the question is more about scheduling, I recommend checking out the [Cosmos open-source project](https://github.com/astronomer/astronomer-cosmos) for Apache Airflow. It allows you to easily run your existing dbt Core or dbt Fusion projects as Apache Airflow Dags. Looks something like this:

    basic_cosmos_dag = DbtDag( 
        project_config=ProjectConfig(DBT_ROOT_PATH / ""some_project""), 
        operator_args={ 
            ""install_deps"": True,
            ""full_refresh"": True,
        },
        # ...
        # normal dag parameters 
        schedule=""@daily""
        # ...
    )

Based on your model, you then have a very nice Dag, with task groups and individual tasks all visible in the Airflow UI.

>reliably finish before that time detect and alert when things are late

Since Airflow 3.1, you have deadline alerts, which officially replace SLAs. They are quite flexible, let me illustrate an example:

Say your pipeline is scheduled to run at 08:00 am daily. It is queued a little later, say at 08:02 am and you expect it to run no longer than 20 minutes. If it runs longer than 20 minutes, you want to be alerted via Slack.

In this scenario, you take the schedule time of the pipeline as your `reference` (so 08:02 am) and your `interval` is 20 minutes. Which means the deadline is at 08:22 am:

    |------|-----------|---------|-----------|--------|
        Scheduled    Queued    Started    Deadline
         08:00       08:02      08:03      08:22

Within your implementation, this is how to define this deadline alert:

    deadline_alert = DeadlineAlert(
        reference=DeadlineReference.DAGRUN_QUEUED_AT,
        interval=timedelta(minutes=15),
        callback=AsyncCallback(
            SlackWebhookNotifier,
            kwargs={
                ""text"": ""{{ dag_run.dag_id }} missed deadline at {{ deadline.deadline_time }}""
            },
    )
    
    @dag(
        deadline=deadline_alert
    )
    def your_dag():
        # ...

You can also use `DeadlineReference.DAGRUN_LOGICAL_DATE` as a reference, which represents the scheduled date (so in our case 08:00 am).

Or, let's say you must be finished at a specific time per day, you can do something like:

    tomorrow_at_ten = datetime.combine(datetime.now().date() + timedelta(days=1), time(10, 0))
    
    deadline_alert = DeadlineAlert(
        reference=DeadlineReference.FIXED_DATETIME(tomorrow_at_ten),
        ...

Since you have full freedom in the callback, you can use any kind of notification.

And of course, you can combine that with the `DbtDag` from Cosmos üòâ.

While deadline alerts may work well in some scenarios, many cases require more advanced observability. If you are open to a managed solution, Astronomer offers Astro Observe. It allows you to define and monitor SLAs without code changes and supports root cause analysis, lineage, impact analysis, and more. You can also connect a self-hosted OSS Airflow environment by enabling OpenLineage with a single configuration change.

Apart from that, Airflow also supports sending metrics to [StatsD or OpenTelemetry](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/metrics.html), which includes Dag runtimes. In a previous job, I used this to monitor Dag runtimes via Grafana and have alerting on that level.

Again, many ways to approach this.

>manage dependencies

If you use Cosmos, you already have the dependencies within your Dag derived from your model definition. However, if we talk about dependencies between Dags (pipelines), Airflow has various ways to solve this.

You can use implicit time-based dependencies, by simply using time-based scheduling in a way, that the downstream pipeline runs after the pipeline, yet, this is the worst solution.

You can also use an `ExternalTaskSensor` that waits for the completion of a pipeline or task. Since Airflow 3, this also supports a deferred mode, so by using `deferrable=True` it will run async by the triggerer component without using any worker slot, in case you need to wait a bit longer üòÖ.

But the new way to solve this are data assets, introduced with Airflow 3.0. Assets are datasets, which have been there since Airflow 2.4, but renamed, enhanced and more integrated.

The idea is simple: in the upstream pipeline you define one or more asset outlets, and in your downstream pipeline you set the schedule not to be a cron expression, but to be an asset:

    @dag(
        schedule=Asset(""daily_sales"")
    )
    def monthly_report():
        # ...

Assets also support logical expressions and they have their own view in the Airflow UI, which is nice üòÅ.

There is also a shortcut to create a Dag, with a single Python task and an asset as an outlet, which is by using the `@asset` decorator.

>Is the usual solution just scheduling earlier with a buffer, or is there a more robust approach

Fun story: I used that approach for years, with many Dags and dependencies. This lead to so many issues, sometimes even without noticing because the missing data was not obvious right away.

Assets might take a bit to get into it, but it is worth it all the way.

Disclaimer: I work at Astronomer, so I am biased towards Airflow, but I also used it in my previous job for many years.

Hope that this helps in some way üòâ.",0
1qbs00s,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qbs00s,2,"I would have never done that like that. Sounds like you're a decent SWE doing DE stuff without formal DE & BI training.

Python to gather API - 100% - but the way you do it, that data with that format at that time is lost forever.

Python with API to a Datalake container, one folder per source, and subfolder per source API name. Store as JSON. Why? For SSoT and what if the supplier of that API goes out of business, changes their API in some way, that breaks your code? What if in your dataframe the tuple you extract as a table, you're not pulling all entites or all columns - then the business wants you to add those extra columns, retroactively from Day 1 you've been gathering. You're screwed - you don't have it anymore.

So E-L-T. Extract (Python + API), Load (into storage for 100% retrieval of 100% of that data), Transform (into a Staging layer, dedupe, business rule(s) for ingestion UpSert or Rejection, into the Bronze layer.

So Bronze has proper types and only valid data, Bronze can be destroyed (for a particular source) and recreated at any point in time in the future by rereading all the data in the Datalake and redoing the L & T of the ELT. 

Staging is usually truncate/load style, and uses Datalake functionality for ingestion - Python isn't needed here at all. Python only for reading from APIs and other distant sources, not for moving data around in the database.

Python has another function - for ML - where Python does extensive transformations by reading Bronze & Silver (or even the raw Datalake data) to ""invent"" new data that is then fed back into a Staging layer.  For example, a customer ""rating"" based on criteria that can span multiple source systems for that customer.",8
1qbs00s,3,"is this a rhetorical question? if no, then the answer is yes. maybe as a rule of thumb, just about every piece of technology needs maintenance and support.",1
1qbrxvk,1,"I think Data Science is more hot field compared to Cloud/IT as we wave of AI. yes, Cloud or IT Ops going to be there because AI needs data centers but I think still Data scientist are going to be more in demand. 

  
If you don't find Data science interesting, that's another thing. 

  
You can always do a quick search at the number of jobs for Data Science, Cloud Engineer, and IT Ops to see which has more jobs to give you an idea about the roles in the market and needs.",1
1qbrhgn,1,">This guy only works with SSIS and we are a python shop. He also hates python.

Most people who love SSIS absolutely hate Python.  It's a really sad trend.",81
1qbrhgn,2,"Condolences. How old are you?

One of two things are going to happen. You burn out / fall ill from stress / purposefully injure or kill yourself, and they fire you because you're now worthless to them.

Or you purposefully choose to recognise your limits, say a hard no to any work outside your capacity, knowledge, or schedule, and what happens happens. Maybe they'll ease up. Maybe they won't care and will still burden you and blame you and fire you after. But you'll be working 9-5, have your dignity, they'll suffer the consequences of their bad management (which is satisfying), you won't end up sick or dead, plus they would have fired you anyway. 

Young people typically fall into that first category. Older workers have been through that before and live according to the second category.",52
1qbrhgn,3,"SSIS people hate python. 

Python people hate SSIS.

UI vs code 

Lol classic",22
1qbrhgn,4,"Apparently it‚Äôs more common than you think these days. It‚Äôs really bad management from your boss and upper management. Sadly you don‚Äôt have much option here, either jump ship or bring this up in every 1:1 I guess. If possible name drop your company or at least a hint so fellow engineers can stay away. ‚úåÔ∏è",6
1qbrhgn,5,"If your financial situation is not dire, my recommendation would be to enforce boundaries and deliver at a pace that‚Äôs reasonable for you. If it is dire, then you might have to weigh mental health vs financial health. That varies from person to person. If it were me, I‚Äôd enforce boundaries regardless.

In the meantime, try to apply for jobs elsewhere. The market is bad, but it‚Äôs not totally hopeless for mid to senior DEs.",5
1qbrhgn,6,"Does sound like quite a flashing alarm situation. Good for you to be searching. I say keep it up. At least you have a job right now so just use this time to stay on top of things, learn some of that tribal knowledge and focus the most of your energy on the job search. Could take several months more but that's ok.",4
1qbrhgn,7,"im sorry to hear that man, hope your situation with that gets better though, im sure your talented and can find something new soon if you wanted to",8
1qbrhgn,8,You‚Äôll find something new just takes time sometimes,3
1qbrhgn,9,"Since you are about to quit, I would give one try to communicate the same with the leadership that how are you feeling about the work environment and you cannot sustain this long term and we need to fix it before I am forced to give up.",5
1qbrhgn,10,"You had an offer on your other thread, why don‚Äôt you go?",4
1qbq2eg,1,"Biggest lesson from teams that‚Äôve shipped this: text to SQL isn‚Äôt the hard part, trust is. A few design things that usually matter more than model choicee would be like always ground generation on the actual schema + metrics layer, not just table names. Most failures come from the model guessing joins or columns, Add a validation step before execution (dry run, limits, blocked queries). Don‚Äôt let raw generated SQL hit prod blindly. Start narrow, Constrain what users can ask for (time ranges, metrics, dimensions) before going fully ‚Äúchatty‚Äù. On the knowledge side, most teams I‚Äôve seen keep it simple, Use RAG for business logic, definitions, metric semantics,Treat schema and relationships as structured input, not embeddings.GraphRAG helps when relationships are complex, but it adds a lot of operational overhead.Solr *can* work for retrieval, but it‚Äôs usually better as a search layer than a true graph store. If relationships matter a lot, a dedicated graph or just explicit schema metadata often ends up clearer and more reliable. and correctness and guardrails beat clever architecture early. If users don‚Äôt trust the numbers, they won‚Äôt use it, no matter how good the UX is",5
1qbq2eg,2,"I would spoon-feed a private LLM specific queries for specific use-cases, going from simple to more complex, and make it pick & choose, not invent SQL code from scratch.

But that's just me, I don't trust a philosopher / poet to properly do cross joins without causing a cartesian product, not enforcing a LIMIT clause on all outbound queries that can cost the hosting company a lot of money.

Also commenting to see what others have to say / have done. It's something on the table for 2026 where I work at. We have a large number of ""analysts"" that lose a lot of time understanding the models and write inefficient queries or new ones when existing ones already exist.",1
1qbq2eg,3,"Just like another commenter suggested, the text to SQL is not the hard part, it‚Äôs the trust in the data, queries, and establishing context using metadata and business jargon. Example: ‚Äúgive me annual revenue for fiscal year 2025‚Äù will always fire a query for financial data from Jan 2025 to Dec 2025. What if my company‚Äôs fiscal year begins in March and ends on last day of Feb? That‚Äôs business knowledge that your solution needs. 

Full disclosure: I work at Databricks, and since you mentioned Azure in another comment.. just reminding that Azure Databricks is a first party PaaS offering in Azure. I have helped many customers with this problem and truly believe that what Databricks gives is a high quality text2sql solution with AIBI Genie. You can find lot of information on Databricks Genie and its benchmarking results.

Its a hard problem to solve especially around the biggest requirement which I see from analysts or power users, which is the need for quality and deterministic SQL code. This is where traditional BI does well, whereas LLMs could hallucinate.

Best practices in Genie include using well curated metadata about each dataset, column definitions,  semantic understanding about the data etc. These become non negotiable with agent based solution, since that's the only context for AI.

Something like Genie being a tool in your arsenal will help your overall agentic solution. And best part is that you don't need data to be in Databricks. Databricks can connect into your database via Lakehouse federation and understand the layout of data.",1
1qbq2eg,4,"you need: 1. a semantic model defining the facts and dimensions (a solid data model is non negotiable), the joins, the aliases, sample values of dimensions (if high cardinality, can retrieve from a vector database) etc. 2. a RAG to feed custom business instructions, metrics. etc. LLMs like Claude Opus can generate flawless SQL once you have these in place.",1
1qbq2eg,5,"I created this infrastructure (via MCP) last month and what helped a lot was to only allow the LLM to populate predefined queries via query templates. Futhermore, adding the data model and other information via tools seemed to help as well. 
Like I see mentioned here before, the hardest part is if the agent is not hallucinating, especially when table contents are complex.",1
1qbq2eg,6,checkout vanna AI pretty good,1
1qbq2eg,7,"texttosql isnt the hard part, trust is. once people start relying on the answers, small gaps in metric definitions or logic show up fast.¬†grounding generation on a semantic or metrics layer helps a lot. approaches like genloop focus on constraining sql to explicit definitions, while graph or rag layers are better for discovery than correctness.",1
1qbq2eg,8,just use [fabi.ai](http://fabi.ai),0
1qbpoh1,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qbpoh1,2,Start watching some of Hannes M√ºhleisen's talks on DuckDB and DuckLake. His inaugural lecture on the history of DBMS is also really nice.,70
1qbpoh1,3,"SQL Server and Postgres, my dynamic duo. RDBMS‚Äôs are some of the most powerful, battle-tested, reliable pieces of software ever produced by the entirety of mankind. Look at sql lite, they have orders of magnitude more tests for sql lite than actual source code to run the db - and freaking public domain/free, too.

I‚Äôve never been a fan of nosql dbs like mongo. Eventually when someone builds an app or whatever that gets enough users or data, they will want engineering to extract it and analyze it. SQL is just too good for this stuff.",25
1qbpoh1,4,Postgres is actually so great that it works even when most teams abuse it really badly.,13
1qbpoh1,5,"There are the cloud-only DBs and they are good to do stuff. But Postgres is everywhere. Even in cloud. And the older the company, the higher the possibility to find MySQL.",22
1qbpoh1,6,"What differs? One big difference. 

PK / FK management with enforcement of constraints. Mono-server or server clustering that essentially works like a single server, constraints are enforceable. This can be on-prem or in the cloud as a VM or equivalent, like a DB service that can scale but isn‚Äôt distributed, so you cannot break a constraint. 

Very different to distributed cloud DB computing where PK/FK constraints cannot be enforced, which is why we need to have a landing area for new data, staging, then Medallion. To not import dupes, and then have Cartesian product with Join.",15
1qbpoh1,7,I'm an RDBMS glazer and will be for life,6
1qbpoh1,8,OP preaching to the choir. üôè,5
1qbpoh1,9,Managed Postgres is a multi-billion dollar industry...,3
1qbpoh1,10,This industry is constantly reinventing the same things,3
1qbp417,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qbp417,2,"You try to make something work by doing X. You make it work, but I sucks to maintain. You say, ‚ÄúI should have done Y to make it better‚Äù. Go to top and repeat.

Eventually after enough dead ends you learn a thing or two and gain a bit of intuition. But there is no substitute for practical experience. There is a difference between knowing something and understanding something.",11
1qbp417,3,"The biggest shift for me was realizing that ‚Äúthinking like an architect‚Äù is less about tools and more about trade offs. In real projects you are almost never designing from scratch. You‚Äôre reacting to constraints like messy data, changing requirements, cost limits, and systems you didn‚Äôt choose. So the thinking becomes things like where this pipeline will actually break, what happens when volume doubles, what can fail safely and what cannot, and what‚Äôs expensive to recompute versus cheap to store. If you‚Äôre not on a DE project right now, the closest substitute is to simulate real pain. Take a public dataset and design ingestion, transformations, and reporting, then deliberately change assumptions like schema changes, late arriving data, backfills, or higher volume, and force yourself to optimize for cost and reliability instead of elegance. Books and blogs help with vocabulary, but most architectural thinking comes from asking what the simplest thing is that won‚Äôt hurt you later, then being wrong a few times. Reading postmortems and migration stories taught me more than polished tutorials ever did. You don‚Äôt really learn architecture in a vacuum, but you can train the mindset by constantly reasoning about constraints, failure modes, and long term impact",7
1qbo9zn,1,Nice write upüëç,1
1qbo9zn,2,Nice write upüëç,1
1qbnr9h,1,No youre setting yourself up for success. Follow the simplicity.,35
1qbnr9h,2,"I think Motherduck is a pretty decent offering. If it gets the job done, and sounds like you don't have a huge volume of data, I wouldn't be too worried. Worst comes to worst, moving to another platform wouldn't be too expensive. There is also a chance that Motherduck can improve on those areas throughout the year, and you might find yourself not needing BigQuery at all.

With that being said, if you have such concerns, my core suggestion would be to build your pipelines to be as decoupled from the underlying platform as possible, so that you can use another provider later on.",9
1qbnr9h,3,"I wouldn‚Äôt worry about it until it‚Äôs a problem. You can always move you of motherduck. 
You may also want to look into ducklake and I believe there are ways to migrate from it to iceberg. Anyway, if it works, do t optimize. Leave completely out for now.",6
1qbnr9h,4,"Disclaimer: I love MotherDuck and it's people too.

MotherDuck is quite cheap, so until the bill is a problem I won't worry.

Also, it's SQL, you can easily migrate to something else.

If your workloads are analytics, I won't worry much, if you are successful the money to pay Motherduck won't be a problem.

If your workload is not analytics, you can find something with better performance, but do it when it makes sense.

For now, just enjoy it :)",3
1qbnr9h,5,"Why do you need Motherduck for 4 Million rows? You can store this on a tiny EC2 instance (or any other VM) and use DuckDB locally. 

My iPhone could query this amount data from a browser‚Ä¶ üíÅüèº‚Äç‚ôÇÔ∏è",9
1qbnr9h,6,"The pros are what you already know, its a relatively simple stack that you enjoy. The cons mostly have to do with choosing an offering that isn't currently a market leader. You have to pay a premium to hire people who are already strong with DuckDB and/or MotherDuck, or pay a time premium for them to ramp up. There might be other tools (BI tools, proxies, etc) that have built in support for more popular platforms that won't work with MotherDuck out the box. None of these are particularly insurmountable, just costs you should be happy to take on in exchange for the experience you like. 



One thing you might do to hedge is store data at rest in Iceberg on GCS and self host a catalog like \[Lakekeeper\](https://docs.lakekeeper.io/). Then your data is completely portable. If you for some reason determine you need to switch to Snowflake or Databricks, they can read data form this catalog and you don't have to move anything. Or you may even find out you want MotherDuck for most things but have specific use cases where another engine like Clickhouse is useful. This approach lets you separate the concerns of how the data is stored from what product(s) you use to query it.",3
1qbnr9h,7,"Anyone who gives you a definitive answer is an idiot or partisan. We don‚Äôt know what your business looks like so we don‚Äôt know what the constraints will be in the future. If you are aware of all the use cases somehow and where the business might need more insights in the future and you are still comfortable with your current setup, then good! Don‚Äôt over complicate it. Being on two clouds is no big deal if the cost isn‚Äôt too much for you now and in the future, another thing we can‚Äôt know.",2
1qbnr9h,8,"I love duckdb and use it as much as I can. 

>Am I making a mistake by having two cloud providers like this?

Not necessarily, its either you use Motherduck as you are or you spin up some containers on GCP and manage the compute engine for duckdb and catalog on your own with ducklake.  It is not a super complicated stack but it *is* work and creates an additional point of failure, especially with the networking and security side of it imo.   
  
My recommendation is keep it as is if you love it since Motherduck appears to be a very affordable service, if you end up having to migrate later on it should not be a very heavy lift. Best of luck on your business!",2
1qbnr9h,9,"I find this question so interesting.  Like you I love DuckDB and I love using DBT with it and the only issue I run into is concurrency. 

I live in a Microsoft dominated region and I seriously hate data engineering work in Data Factory + SQL Server / Synapse. 

I'm interested to know if people have tied tools like Power BI or Tableau with good success.",2
1qbnr9h,10,"I would also suggest to stick with what is working for you, especially given the scale of data. Once you are hopefully scaling to thousands of clients and add more data points you can still switch to a different solution. I found BigQuery to be quite expensive on GCP, there are better alternatives even in the Google ecosystem. Every large analytical query engine is also available on GCP. We tried Exasol for scaling.",2
1qbkcbp,1,"Look up anshlamba on youtube, do atleast 3 of his projects with dbt as a starting point since you already know sql. Once you get comfortable with some pipeline designing, switch to more python based advanced concepts and then probably to databricks if you want to get certified soon",1
1qbkcbp,2,"I am also in same path shifting from DA to DE
Would you like to connect so we share knowledge and roadmaps?",1
1qbkcbp,3,"Here's a [comment I made a few days ago](https://www.reddit.com/r/dataengineering/s/T9sqVnB48d) on almost the exact same topic, if it's helpful. 

Designing Data Intensive Applications is the book I'd recommend, but I go into more depth about the tools I think are important specifically for this transition.",2
1qbkcbp,4,"Moving from data analyst to data engineer is a natural transition, imo. I think your goal should be to focus on two things: Python and creating pipelines. Then after that, become familiar with at least one major cloud provider. AWS, Azure, GCP. Be strategic here. I‚Äôve noticed specific industries prefer one vs the others. 

Your data analyst background should have given you ample knowledge on how to create tables (ideally in sql, but I think understanding excel tables and pivot tables help too). Data exploration is an important skill, but understanding schemas and how they fit together is the crucial part. Your data analyst background should help with data modeling as well. Try to focus on roles that highlight these things in the job description. 

Also be aware that there are different flavors of data engineers. Being analytics focused means you‚Äôre more on the business intelligence and dashboarding side. Highlight your storytelling and data vis exp. If you‚Äôre on the ML side, highlight how you productionize ML models and maintain workflows.",2
1qbkcbp,5,"Read ~ Fundamentals of Data Engineering by Joe Reis and Matt Housley
üòá Stay happy now",0
1qbilj6,1,Is the data not suitable for slowly changing dimensions?,1
1qbilj6,2,"Have you thought about validating the data as well, before sending it onwards?",1
1qbilj6,3,How many rows do you have in each CSV that you receive every 10 minutes?,1
1qbd771,1,You could look into DbtBuildOperator that cosmos provides. Cosmos integrates dbt well with airflow.,3
1qbd771,2,"I would advise to use the KPO, not the bashoperator. We had many, many issues with multiple dbt runs on the same worker which spilled into one another. Using a KPO you just have an image, some bash script that pulls the dbt project (or bake it in) and run it. You have perfect (testable) isolation, and with some setup you can create a local test environment as well (which was very important to us).",3
1qbd771,3,"u/fordatechy , I am genuinely curious, what is the driver behind choosing dbt for transformations? Like what pain points have you been running into that makes it worth it to add into your stack?",3
1qbd771,4,Can you try to Google 'dbt vs airflow'? there maybe opinions on this.,1
1qbaolw,1,Grab an IT support gig now. Boring but gets you real experience while you finish the degree.,2
1qbaolw,2,"Definitely transfer for the BS and get internships. Sounds like you want to something data related, but maybe not strictly DE, so I‚Äôd take a probability class and linear regression as electives if you go to grad school later 

My other advice won‚Äôt directly help on industry job applications, but it‚Äôs good experience. 

A lot of STEM professors need data analysis done, but also need the data to be cleaned before that can even happen. There‚Äôs a lot of opportunities to get hands on experience cleaning data in research. You definitely get exposed to problems that can occur and it‚Äôs a low pressure way to try new things. Academics often have really important data, but are awful programmers. You can automate it by building pipelines vs single repetitive and risky scripts, which are too common.

I‚Äôm not talking about computer science labs, but biology, physics, economics, statistics, psychology and even political science. Working with real life cancer research is way better than the n-th analysis of the titanic data set. I did this in grad school and it actually networked me into a research hospital",1
1qbaolw,3,"You won‚Äôt get a job doing DE as someone with an AA and no professional experience and you should be extremely cautious of any team that wants you with those credentials. Not shitting on associates there‚Äôs just a glut of people with CS degrees and experience that any company worth working for would prefer to hire.

Edit: missed the end about DA/DS/DBA my suggestion would be learning what these roles do, because not knowing the difference between DE and those other fields is key.",1
1qb7d5g,1,"Why gaming?  

Turning a hobby into a job doesn‚Äôt usually go well.",9
1qb7d5g,2,"Public to private is tough anyway, and gaming is one of the most competitive sectors. A lot of DE roles in gaming are very prod/live focused (real-time events, telemetry, experimentation, on-call). Public sector work often gets (unfairly) labelled as batchy, reporting-led, and low-risk, so hiring managers assume you haven‚Äôt lived in high-pressure prod environments.

Add to that huge applicant volumes and people willing to take pay cuts just to work in gaming, and cold applications become brutal.",6
1qb7d5g,3,"I come from the similar background. Most of the work was taking messy survey data, cleaning it up, building facts/dims + marts, adding dbt tests, and dealing with stuff like PII handling and data quality issues.
I asked similar question a few minutes ago. 

I want to better position myself to London analytics market.",4
1qb7d5g,4,Why limit yourself to gaming? We like what we understand. Just learn stay curious and learn new domains. What technical skill do you learn from games that is not translatable to other industries?¬†,1
1qb7d5g,5,jumping from public sector to gaming is tricky but possible with your background you already have strong azure and data skills but gaming companies care a lot about cloud security too exploring tools like orca security could help you learn about this part and boost your profile you don‚Äôt need to be an expert just add that you‚Äôve worked with some security tools and understand cloud risks this can catch the eye of hiring folks it only takes one good application to make the switch keep at it and mix in a little cloud security on your next few resumes if you can,1
1qb7d5g,6,Do whatever you want but going data engineering to game dev is retarded IMO. just build games in your free time AAA gaming sucks anyway.¬†,-3
1qb78rd,1,"I care about fundamentals, and yes it's not everywhere, making me join only a certain level of companies.",1
1qb728v,1,don't waste your money on them. /u/chrisgarzon19 is a scammer.,14
1qb728v,2,Get coursera plus for 199$ and u are good,3
1qb728v,3,"I always suggest people on getting mentored rather than buying courses or paying hefty amounts like 10-20 grands. Find a mentor, follow a roadmap, start with something, once you get in that flow, start building small projects to understand the practicality behind the concepts. Remember consistency and will to learn are your stairs to success. Feel free to connect for any suggestions or help",2
1qb728v,4,De Zoomcamp >,1
1qb3twi,1,That‚Äôs pretty damning and something I‚Äôll be pointing my databricks counterpart at in the morning‚Ä¶,3
1qb3twi,2,Good write up thanks for sharing.,2
1qb3twi,3,I miss the times of slurm and lsf,1
1qb3twi,4,"wild seeing these databricks numbers, but nobody talks about how much of a headache security can be with all those moving cloud parts, i have run into this when teams move fast and get blind spots, quick tip orca security covers a lot automatically so you don‚Äôt end up patching leaks late, you can also peek at others like wiz, best to set up early and not scramble when audit season hits, always feels like overkill till it saves you",0
1qb0ty1,1,"Screenshot looks neat, getting asked authentication details by an unknown online app of which I can't see the source code or build myself is an immediate no.",7
1qb0ty1,2,"I don't know if this is AI generated but this looks like every ""vibe-coded"" app I've seen. That's not a bad thing but they all look the same and its kinda of insane.",2
1qb0ba6,1,Did you check the model performance against historical data?,1
1qavf3s,1,Good read,1
1qaufq6,1,What the fuck am I reading,13
1qaufq6,2,"As a data engineer, you're data is missing the units :-)",16
1qaufq6,3,"You don't just 'ask' for a raise. You actually prepare a case, show your achievements, and bring a realistic proposal.",2
1qaufq6,4,">I faced a situation where I asked for a 100% hike, and the HR representative arrogantly responded, ""Why do you need 100%? We can't give you that much."" He had an attitude of ""take it or leave it."" Is it their strategy to round me in low pay?

This is fucking hillarious.

>How should I respond in this situation?

""Sorry, I said 100% by mistake.  For your arrogance, I want, nay, DEMAND a 150% increase""

>FYI, I'm de with 2.6yoe and currently earn 8.5, and my expectation is 16 .

If you want a 100% increase in salary, details matter.  When you say 8.5 and 16, what currency is this?",6
1qaufq6,5,"Even if English is not your first language, if this post is indicative of how you work abilities...... Well, hopefully you can fill in the rest",2
1qau2wr,1,"Your post looks like it's related to Data Engineering in India.  You might find posting in [r/dataengineersindia](https://www.reddit.com/r/dataengineersindia/) more helpful to your situation.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1qau2wr,2,"What do you mean by ""product company"", are you looking for a highly technical, ""product lead"" organisation, or something else as every company is based on selling a product. Do you mean hardware, and if so technical hardware like Oura?",1
1qasjr3,1,What is the source API? The design you have prepared appears ridiculously complicated for such a simple task.,3
1qasjr3,2,Schema per customer would give you 100 clients =  100 schemas.,3
1qasjr3,3,"I had multitenant databases with thousands of clients. It can be secure also like that.


Having multiple schemas is worth on the application level, but then I would also split the databases",1
1qasjr3,4,"This post feels AI generated given the formatting and the tell tale ""‚Äî"". Assuming this was a legit question from a user who just used AI to formulate their text . . . .  

. . . .  are all the users going to be doing exactly the same thing, i.e. cookie cutter, therefore the same schema for all ?",1
1qasjr3,5,"Why not use BQ?  
In BQ you can have Table per client and control access on table level.",1
1qasjr3,6,"You might want to check something like Datacoves, I think they have the isolation you need.",1
1qasjr3,7,"Hi, you can try using **COZYROC** to create a secure integration layer that exposes your source data as tables. Can you please let us know what is the source API?",0
1qasjr3,8,"I work for **COZYROC** and With **COZYROC Cloud**, you can retrieve data from your source APIs and push it directly into **HubSpot** using a custom or self-managed Gem.  
You can build the Gem yourself, or we can assist you with the design and implementation.  
Feel free to DM me if you‚Äôd like to discuss the approach and options in more detail.",0
1qasd31,1,Is anyone using spark 4 yet?,2
1qaraba,1,You cannot be honest with the recruiters when they are so not gonna be in the first place,20
1qaraba,2,"For live or proctored: don't unless they explicitly say it's okay


For take-home: don't ask, obviously do",4
1qaraba,3,Probably the recruiter came to the conclusion that even with all the ‚Äútools‚Äù at your disposal you couldn‚Äôt arrive at the solution,5
1qaraba,4,MCQ?,2
1qaraba,5,Was it live test or take home assessment?,1
1qaraba,6,"Dude, take home assignments always allowed googling. You don't need to use AI to do it for you.",1
1qaraba,7,"Did the assessment say what resources you were and were not permitted to use?


Generally short of hiring a contractor to do it for you for things done on your own time you can use the resources that will be available to you at the job (including Google and code assist tools)


That's not cheating. AI will give you code that very well may yield the correct answer.


But code giving the correct answer and code being good is completely different criteria.


Even if you use AI you are still on your own to determine what is good and ungood before submitting.",1
1qaraba,8,"Rejection can be attributed to many factors, doesn‚Äôt necessarily mean you are bad a the job. That‚Äôs why I don‚Äôt look for jobs in this economy, it‚Äôs a waste of time and effort¬†",1
1qaraba,9,You failed the common sense part of the interview,2
1qaoqlz,1,"This is the problem with AI.

Gives people a false sense of competence because they can get it to do their work for them, up to a point.

It also has the effect of making non-technical people who've used AI think those with actual skills/knowledgeable aren't necessary.

Recruitment is only going to get worse.",196
1qaoqlz,2,"I couldn't regex to save my life, it's something I also have to use a tool for.
I've been using it since before there were tools, I remember flicking through a huge reference book to write a solution for reference.


If you can't explain facts and dims you're in trouble.


Finding good candidates has always been hard.",154
1qaoqlz,3,"This has been very prevalent in our remote interviews, to the point of being nearly ubiquitous from a certain subcontinent.",78
1qaoqlz,4,"We have a coding interview in our process that tests rather basic python and SQL knowledge, so we generally expect people to be able to solve them without AI or searching. Had a crashout of someone recently that completely refused to code without an AI assistant. I don't know how you can work without understanding what you are supposed to be working with.",25
1qaoqlz,5,"I interview people frequently, almost all Indian contractors. Nearly all of them are using AI.",9
1qaoqlz,6,"And sorry to say this point but interviewed around 50 candidates and most of them were around bangalore and Hyderabad . The common pattern in their resume was they worked in some small company for 5 yrs and now they are in mnc don‚Äôt know how they got in and when you ask anything most of them are caught cheating . When you google their past company it will show some dummy website and no location for company or either some incorrect location pointed as company office.

This is not external hiring this is scenario in internal project interview of mnc big one like infy tcs ‚Ä¶",33
1qaoqlz,7,"Had a similar experience late last year. Some candidates tried inviting an AI ""note taker"" to the Teams call, that was denied access and was an immediate red flag (not one of them \*asked\* if they could do so).

We then had people who seemed to hesitate before answering any question. Once or twice, nerves, every single question? It's pointing the way of stalling before waiting to see the text appear for them to regurgitate! There's also the tell of the eye darting off screen to read something... could be notes, but... given what they then said, it just gave the impression of reading from either someone else googling on their behalf, or AI LLM output.

But every single one was derailed by this one simple trick... as hiring manager, I was interviewing the person for them, not their technical skills. There was some of that, but more in approach and mentality, rather than specific abilities, that would come in the next round. So I would just say to the candidate ""You have listed these ETL tools on your CV, which is your favourite and why?""

Every single time, the ones reading AI output would turn into a marketing bot, sounding like an advert for the tools, bullet point after bullet point. Not one of them highlighted a favourite without being asked again, and then couldn't actually answer definitively as to why they chose that above another, almost as if they'd just picked one at random. The humans answering that question would do so in a conversational manner, often immediately blurting out their favourite and then going through the pros and cons as to why.

I can see the appeal of using AI to help, to act as a pointer to guide your answers, especially in areas of theory where you're rusty because you've been at the coalface for a while. But don't rely on it 100% to the point where you cannot actually hold a conversation with the person you're talking to.",15
1qaoqlz,8,"Every DE job listing gets like a thousand applicants these days. This necessitates heavy automated screening, which only allows a tiny fraction of the most ""perfect"" resumes to even reach a human being at all. Real applicants don't have ""perfect"" resumes, so in effect this system rewards those who are the most willing to lie about their experience. The more shameless the liar, the more perfect the resume.¬†",8
1qaoqlz,9,"Seen this too many times. Am considering that for future interviews, telling the candidates 

‚ÄúWe have seen too many people cheating with AI so we will need you to share your screen, and also, put your hands where I can see them.‚Äù",8
1qaoqlz,10,"Would it have been okay if the candidate had said ""I'll use regex. Usually I ask copilot to make the statement and test it this way""? I know they probably had chatgpt make the whole script, but I am just wondering, as regex is one of the cases where AI is the right tool imo",6
1qamtsj,1,"For most part you create DAGS using python for your workflow, so you would just make a dag with steps that you want to automate with the correct data features api calls validations.  Airflow is a a powerful workflow automation tool you can do very complicated things if you daisy chain the dags just right,

edit. 

so your workflow is fectch data => create features => store to feast=> train => champion channge => publish => monitor 

  
you would just create a  dag or series of dags that you can in order once each step has completed and validated.",3
1qamtsj,2,1. Docker you can go a notch up and use the kubernetersoperators as well.,1
1qahyip,1,I agree with the SCD2 approach. You could hash all the columns and use that as your CDC anytime a change occurs,3
1qahyip,2,"I would go with option b (ish). Merge the records so you can handle the inserts and updates. If you need history you can set up an SCD 2 pattern to capture changes over time. 

Merge should be pretty efficient. You can even land the data in a full append table option c and then do option b. You can set up a small job to clear out older data in the append table periodically. This might be nice just to give you better visibility into the data and make sure your process is working as expected.",2
1qahyip,3,RemindMe! 5 days,2
1qahyip,4,"IMO option B is the best way to do it, regardless of merging isn't the best idea for ""bronze"" or whatever. There is 0 gain from making it a snapshot and eating the storage and eventually processing speed. I wouldn't worry too much about what others think of how and where the data should work in theory. Theory rarely ever applies in the real world, the data and how you get it dictates what you do and in this field most cases, you have 0 control over the source of the data. Which means almost nothing operates like it should and you just have to get the data as accurate as possible, as efficiently as possible space, and processing wise.

  
To me, Bronze always matches source, or at least as close to it as you can possibly get. If it doesn't, you are going to have a horrible time running down why things don't match up. Snapshotting like option A is really only good for if you need to see how things have changed over time, or doing day to day comparisons at the Bronze level and I'd rarely ever want to do that because that's honestly a waste of space and processing. 

  
As for Option C, I would avoid anything that makes data right after the fact and there's a point in time where non-dirty read data is inaccurate. That's asking for someone or something to query it when it isn't fixed yet and you running down nonsense for a day.",2
1qahyip,5,"bronze layer shouldn't have anything done to it. It's bronze, not silver.",1
1qahyip,6,"see,quick thought keep your raw loads separate and maybe use orca security or something for data lake visibility, makes life easier when those files pile up",1
1qafyy3,1,Just throw Claude Code at it,2
1qafyy3,2,"At that scale, the main issue isn‚Äôt AWS tooling, it‚Äôs the shape of the problem. Generating high quality Q&A from 100k word documents with a 70B model is always going to be slow and expensive unless you constrain it hard

What usually works in practice is breaking this into stages. First chunk aggressively not just by tokens but by semantic boundaries like sections or topics, otherwise reasoning quality falls off anyway. Then do cheap filtering or summarisation passes first, either with a smaller model or heuristics, to identify which chunks are even worth turning into Q&A. Only send the ‚Äúinteresting‚Äù chunks to a large model

For cost people often mix models like small or mid size models for chunking, summarising, and question generation, then a stronger model only for refining or validating a much smaller subset. Trying to run LLaMA-3 70B end to end on a million long documents will blow your budget no matter the platform

Also, be realistic about ‚Äúchallenging reasoning‚Äù because most synthetic Q&A pipelines degrade if you push for depth at scale. You usually get better results by generating simpler questions at scale, then curating or rewriting a smaller evaluation set manually or semi automatically.",1
1qafyy3,3,"Sir, can you reword/rewrite your post with the help of AI. 
No period, no comma. Hard to follow.",-1
1qafyy3,4,"This is a common question in data science, we in data engineering mostly handle data movement and indexing. You may find better help in a data science sub.",0
1qaamfb,1,"What is their plan if Gold needs to be regerated for some reason?  Are they fine at that point that the roll-up for ID=1 will change to the v6 value?

It sounds like it is pretty much a non-issue.  I think that is the business telling you they don't use data that is more than certain age.  

I'd be more concerned that the record can be updated and they don't want you to propogate the data up through the layers.",1
1qa5wt5,1,"Near about six to eight months easily. If you have a good roadmap, good content and mentors and of course will to make a habit of being consistent then expect 4-6 months",6
1qa55gv,1,Probably not.  I don't think that kind of content is publicly available unfortunately.,1
1qa2h7b,1,">The main reason Im considering this is because I find cybersecurity fascinating

Pay is near enough a secondary factor at this point.  We aren't around forever.  May as well spend your 8 hours a day doing something interesting.",35
1qa2h7b,2,You‚Äôre worried about the small things imo. Liking the job and especially finding it ‚Äúfascinating‚Äù heavily outweighs a title issue. And you can always just slap whatever title you want on there and clear it up if it ever comes up.,44
1qa2h7b,3,"Sounds fine.  

If you have a lot of experience with terraform, kubernetes, docker, kafka, etc then that'll get rusty.  But if you don't, and mostly do dbt & sql then I wouldn't worry so much.

I work in cybersecurity and often do the work you're describing.  And yeah, it's fun.  Even though you say these are one-off parsing datasets in my experience there's still a huge need for:

   * writing code to handle a lot of backfilling - go pull 20 years of whois data in daily csv files...
   * building *tested* library modules to do common things - log record counts at each step along with reject counts by reason to a common table that can be used for audit reporting - and that you go back to when people are using the data later & have questions.
   * rerunning the parsing *numerous* times until everyone is satisfied - on things like whether or not which fields get url-decoded, what to do about fields that were decoded or encoded multiple times, fixing fields with multiple timestamp formats, etc.  Recently we had to do this with a nasty vendor feed - and ended up writing code to loop through about two dozen timestamp format attempts until one worked - for every record.

If you spent a few years doing this and wanted to go back to engineering and interviewed with my team and then told us that you thought the work would be fun, and you learned a lot, we would have no problem with that.   But some teams might though - so I would see if you could get them to title you as an engineer.  Not a showstopper for me if they wouldn't though.",7
1qa2h7b,4,With 8YOE you could always switch back if it ends up not being for you. Would not call it a career killing move. Chase your passions.,2
1qa2h7b,5,"I‚Äôd take a data analyst role where work is more interesting and pay increases this much. You can potentially build domain knowledge and have more business impact as a data analyst. Call it a lateral move, very normal.",1
1qa2h7b,6,yes,1
1qa2h7b,7,"I‚Äôve recently come to the conclusion that the career progression of a DA is better than that of a DE. DA -> PM -> VP product -> CPO -> CEO. You are solving way more business focused problems than DE‚Äôs, learning a ton about the industry and your work is way more AI-proof due to the large amount of business context needed. If you find the work interesting, I say go for it",1
1q9ylqb,1,"Hi, I am from Polars. Original author and co-founder. 

Polars OSS is never going behind a paywall. It is open source MIT licensed and we're not changing that.

Polars Cloud offers a whole new distributed engine aside from Polars OSS and the managing of the compute.

If you're happy staying single node. Polars OSS is perfect for your case.

Polars OSS going unmaintained is just nonsense. I also read that on the fabric subreddit as an excuse to not support Polars. If anything development is increasing.",136
1q9ylqb,2,Consider DuckDB as well. They're at least backed by a foundation. License is MIT.,38
1q9ylqb,3,"No need to shame single node spark users, the most likely scenario is they really hated pandas syntax... It's not about resources haha",18
1q9ylqb,4,"Single node spark is a joke, almost anything outperforms it. 

Most of our work is single node. Polars is fanatic. Polars performance also pushed the line where you should be using multi node spark because the performance is better. You can get away with more (for better or worse, you‚Äôre pushing massive scalability problems down the road but if they will never happen in your use case then it‚Äôs perfect.)

The biggest issue with Polars is that‚Äôs it‚Äôs relatively new, and there is potential for more breaking changes across future releases. That‚Äôs part of growing pains. If you‚Äôre in  a company with standards and need to prioritize long term stability, then that‚Äôs another scenario. But things move so fast. Dbt and Duck would still evolve in 10 years. The only things that kinda cement over long periods are Apache products personally.",22
1q9ylqb,5,If you don't use multiple nodes spark is not the best and you should use polars.,10
1q9ylqb,6,"Anything less than 10TB of data is Polars for now, we run a similarish setup where we decide to do a lot of our computations with just polars+delta-rs on a vm.

New streaming engine is great as well, I use it on data of billion rows and its great. 

If delta tables specific, I would say look into the datafusion integration of delta-rs. Its the fatest query engine I have found for delta tables in azure. About 15x faster than spark.",8
1q9ylqb,7,Single node spark? Smh,4
1q9ylqb,8,"Some additional considerations which I have not seen yet:


- For easy transformations, single node spark is outperformed by both duckdb and polars. However, IME for very complicated jobs, the catalyst query optimizer behind Spark performs better than the other two - if you have more than 10ish non-broadcast joins in a given query, I would rather trust spark. You will need to spend more time on a dev setup (e.g. unit tests are awfully slow without a spark connect server which you fire up each morning), but you also get some goodies such as Spark UI/History Server and a really powerful and stable API.


- As you are using Delta, I would be cautious with duckdb. Afaict, write support is not yet there, and even for reads I had undocumented not-implemented errors bubble up from the internal c++ code (on1.41 if I remember correctly). The API there does not yet look super stable.


- On the off chance that you are in a corporate environment where your development machines run windows, strongly prefer polars over spark. Delta spark on windows is possible but a pain because of the Hadoop dependency.",4
1q9ylqb,9,"Single node? Polar, duckdb or whatever is fine. Multinode? Then just spark.

However, remember that single node polars can handle much more than single node spark. But consider time and effort and the task itself. I can rollup spark job with almost no effort so if the job is adhoc/one time, I will just use spark to do it.",5
1q9ylqb,10,Look into duckdb.,9
1q9v7cu,1,"What skills did you pick up other than snowflake at this job? Have you done any actual engineering? If yes, then you're in a real good spot, make sure to market yourself well in your resume. And Snowflake will look pretty good on your resume too, provided you are confident with it.

My advice: If you're looking to switch, reduce your productivity at work enough so that you still have some energy leftover in the day to job prep and learn new skills if needed. Start applying immediately in parallel to this. Ignore ""AI is gonna take our jobs"" crap.

If it was me, I would do some of that during working hours, instead of doing mindless work non stop. I'm gonna get downvoted, it's unethical. But if your work isn't paying you well, and not letting you take enough days off, this is fair game in my opinion.",1
1q9r69d,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1q9r69d,2,"totally agree clean demo data gives a false sense of reality, adding things like churn, failed payments and weird lifecycle behavior makes dashboards more useful for real world testing.",1
1q9r69d,3,Yeah perfect growth curves are a dead giveaway. we model everything in the warehouse dbt push messy lifecycle data into domo for dashboards and sometimes generate chaos data with mockaroo. posthog is also solid for realistic product event data.,1
1q9gsng,1,The project looks good!  I would‚Äôve used something besides lambda for the ingestion / transform with the 15 minute limit,2
1q9gsng,2,"Projects look good, only the first one is relevant for Data roles.",2
1q9gsng,3,RemindMe! 1 day,1
1q9gczg,1,"Better to materialize the view if possible since everytime you do query a view there is a computation time involved. 

Spark splits into stage whenever there is shuffle involved and shuffle is costly. For joins, are you joining with smaller tables? If so maybe can enforce using broadcast join using hint.

Are you on Databricks? Open source spark? EMR?",1
1q9gczg,2,"Check the explain plan first from the SQL tab in the UI or put Explain before the select to see the physical plan. 

Are you defining the views in your Spark job as temp views or are these views that are being ingested from your catalog? 

If it is the former and you are seeing source tables being referenced several times, then it may make sense to cache the base table(s) or the view itself if you are using it multiple times. 

As a warning, if you do choose to cache the view and the view query logic is really complex then AQE could invalidate the caching as the plan might change.

If it is the latter, then check the job and stages tab in the UI and find the one that is the bottleneck. Then use the job the SQL tab to see which portion of the query is causing issues. Again, try caching the view to reduce the IO if the issue is on read.",1
1q9gczg,3,"Big Spark DAGs are pretty normal with nested views + AQE, so don‚Äôt panic

Spark does not automatically cache views unless they‚Äôre materialized or cached, they‚Äôll get recomputed each time. If a view is reused, materializing it (or CACHE TABLE) often helps a lot.

In stages, look for long shuffle stages, big skewed partitions, and joins switching to SortMerge instead of Broadcast. The SQL tab ‚ÄúDetails‚Äù usually shows the actual join being used, even if the stage view is hard to read.

Start by fixing the biggest shuffle, not the whole DAG.",1
1q9f6p1,1,Zero-ETL to s3 iceberg,4
1q9f6p1,2,"Top suggestion: join related data into domains and lock these schemas down with data contracts at the earliest possible point in the pipeline, and have the team that owns the OLTP database own that process.

Otherwise, it's a never-ending sequence of surprises as changes show up in your data - resulting in breakages or errors.",4
1q9f6p1,3,kafka connectors and a managed kafka service can cost a bomb.,3
1q9f6p1,4,Why was this flagged as an AI generated post? I promise its not ha,2
1q9f6p1,5,why are you trying to stay away from 5t?,1
1q9f6p1,6,Uber uses clickhouse for their logging analytics platform for what that's worth,0
1q9df31,1,Gaps are fairly normal now.  Your health is none of their business.  Just put freelance or something vague on there for the gap.,9
1q9df31,2,"Depends on which market you wanna tap in. US, UK, India?",3
1q9df31,3,"Similar to you, 8 yoe in data science and having 7 months gap right now !!!",1
1q9c1a5,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1q9c1a5,2,"I think around 50-100GB. At these range, using BQ is still comfortable although the cost can explode fast if you frequently process dataset at this size.

If you are processing less than 50GB data, I think using BQ is still appropriate. Sometimes even using single node processing system like polars or duckdb work well too without all the complexity of distributed system.",7
1q9c1a5,3,"I don't use Pyspark but Spark SQL, my data volume varies from a million records to 300 million records ( when I backfill). We are on on premise, so have a bit more wiggle room than teams on cloud with cost watching. The tables are traffic data which are typically big..",14
1q9c1a5,4,Scala Spark processing ~18-20TB data per hour,9
1q9c1a5,5,"In all honesty, a lot less than justifies a distributed compute framework.
To be honest, I'm not sure why anyone needs distributed compute, or BigQuery for <1TB.

I can understand using Spark on smaller data if that data is data extraction from non-traditional sources such as extracting data from images and PDFs.

IT fashion is unforgiving. If you suggest it is not appropriate for the use case you become a parriah . Doubly so if you present evidence. 

I'm not saying Spark isn't a brilliant solution, because it absolutely is. I'm saying that its the equivalent of running a megawatt EV for shopping in the next street for many of the use cases I see",5
1q9c1a5,6,I use scala spark processing peta bytes,3
1q9c1a5,7,I am sorry where and how do you use SQL?,-4
1q99ffn,1,"has already been solved by different companies (eg azure devops, jira+github) but many organizations fail to have a proper process.",7
1q99ffn,2,"This is the kind of project that sounds like it would be easy at first but would quickly suck away years of your life, in an already oversaturated market. You'd be surprised how  long a project can go on for based on the simplest of ideas. I'd recommend not doing it.",8
1q99ffn,3,You‚Äôre asking folks to tell the future. Build it or don‚Äôt.,3
1q99ffn,4,"The answer to your question is no, it wouldn't help us, tools like this already exist. Instead of thinking you can build something new, try getting better at the things that already exist. This will prevent you from sinking time into projects trying to reinvent the wheel. Good luck to you.",2
1q97wup,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1q97wup,2,"I list my favorites at [Learning Data Engineering: Courses](https://www.ssp.sh/brain/learning-data-engineering/#courses:):

* [Efficient Data Processing in Spark](https://josephmachado.podia.com/efficient-data-processing-in-spark)¬†by¬†Joseph Machado
* [Learn Data Engineering with Courses & Coaching | Learn Data](https://learndataengineering.com/)¬†by¬†Andreas Kretz
* [End-to-End Data Engineering Project Online Class | LinkedIn Learning](https://www.linkedin.com/learning/end-to-end-data-engineering-project)¬†by¬†Thalia Barrera
* [Free Data Engineering course](https://github.com/DataTalksClub/data-engineering-zoomcamp)¬†by¬†DataTalksClub
* [Data Engineering For Beginners](https://de101.startdataengineering.com/)¬†by¬†Joseph Machado

You'll also find bootcamps, open-source DE projects, events, and other on that link. Please enjoy.",2
1q97wup,3,"Choose IBM if: You want a career-focused, industry-recognized certificate, are new to AI, or want to learn how to deploy models in a corporate environment.
Choose DeepLearning.AI if: You want to become an AI researcher or specialist, need a strong foundation in the math behind machine learning, or want to understand state-of-the-art models (like GenAI) from the ground up.",2
1q95bfj,1,"Because someone senior to you gave you the budget and mandate to provide live data, and you'd like to stay employed",238
1q95bfj,2,"Live data is only relevant if the business can react within a sufficiently short timeframe. While most real-world scenarios do not meet this criterion, certain senior stakeholders advocate for live data due to its perceived coolness.

It is important to acknowledge that live data is indeed relevant in specific operational contexts, personally I have worked on cases such as airport security check-ins, systems monitoring, and managing phone queues.",80
1q95bfj,3,"Live data is important if company tracks live metrics, though not every metric needs to be live/real time",21
1q95bfj,4,"You're somewhat answering your own question.

Live data would only be useful for operational purposes, such as keeping a patient alive, keep a power grid up and running, keeping an assembly line moving, keeping flights from colliding, managing traffic on the streets, giving traffic reports via a mobile app, trading stocks, etc. Very operational stuff.

If a manager asks for live data, you brain should immediately skip to ""this is operational"". If a manager is asking for live data with goals that are tactical or strategic, they might have done a poor job specifying a system correctly. It is possible they did it correctly, but it is very, very remote that they have.",20
1q95bfj,5,Things like operational monitoring require live data but essentially you‚Äôre asking the right questions if somebody wants ‚Äúlive data‚Äù the right question to then ask is why what are you going to do with it?,11
1q95bfj,6,"Live manufacturing monitoring, stock movement, server & infra monitoring.

But other than that, if your C-level request realtime data, what they meant is the data is recent enough (within last 6 or 12 hours) and available at 9AM when he/she feels like seeing the dashboard.",9
1q95bfj,7,"I work in infrastructure, and live data can literally save my company millions of dollars. Even if it means we can diagnose an outage 2 minutes quicker.",7
1q95bfj,8,"So when the exec opens the dashboard once a week he knows that the data is ""live"" (at least it was when it was ingested) 

In actuality, most people need very recent data at best. I usually say that unless someone will die or the company will lose millions of dollars a second, very recent is good enough. 

Some exceptions could be if you're ingesting sensor data and have processes that need to shut down a machine before it destroys itself, the data from several sensors will tell you when you're in a danger zone.",6
1q95bfj,9,"Call center amd field monitoring are the main reasons I've been asked for live data. We only do near real-time at this point, not true live",5
1q95bfj,10,"Within Fintech, live fiscal data is huge.",4
1q933el,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1q933el,2,"I wouldn't onboard them onto Fabric, Way too overkill for just file ingestion. The likes of Fivetran are pure ELT and assumes you will tranform in the warehouse. Inflight transformation with low code mapping should be solved by Integrate io. Handles S3> CSV ingestion pattern easily.",4
1q933el,3,"Maybe it's too sweeping of a statement, but I can't think of a data ingestion tool does not do this. That said, if CSV ingestion is the primary use case, OneSchema looked quite interesting to me. Maybe check this out? They had a data engineering post episode some time ago, and I found that insightful.",2
1q933el,4,"Migrating them to Fabric just for this one use case would me a major no no. Unless the business is already trying to move, I would avoid suggesting it in any case.",2
1q933el,5,Have you looked at Flatfile or One Schema?,1
1q933el,6,"Take a look at hellocsv, that‚Äôs an open source, free version of what I think you‚Äôre looking for",1
1q933el,7,Hey I work at [zetaris.com](http://zetaris.com) and we can help!,1
1q92eti,1,"You want to have a landing area for the data that is your single source of truth. This would be a Datalake if you cloud storage it. DLs have cool uses, like Events. Snowpipes can be triggered when a new file appears. Airflow deposits the file in the DL.

Then a staging area (schema) that links to the landing area and allows hashing to eliminate dupes. No transformations here. All columns varchar. A reject schema for data that cannot fit into bronze due to an error. 

Then Medallion architecture for schemas bronze, silver, gold. 

Bronze (column transformations to correct types) from the staging. Any data rows that do not conform go to reject schema. Also Bronze is a great place for SCD2 tables. 

Silver is new & inferred data that is valid but doesn‚Äôt exist in bronze. Like splitting customers by geographical zones based on their address. It can make sense for some silver tables to be SCD2, but that‚Äôs rare. 

For geolocation you go get another single source of truth, stage it and bronze it. Then the silver combines two different bronze tables. The silver geolocation could be scd2 for tracking the moves that PK did. And so on. To meet all Gold requirements, for dimensions and facts.  

Gold is dimensional tables that Kimball and Snowflake are famous for. DIM and FACT. 

Using Airflow for pushing the data in the Medallion layers is standard practice. 

To go from BigQuery directly to Snowflake FACT, this is called a Proof Of Concept. Nothing wrong with that. Suggestion, put it in a Dev environment and build a Prod environment. 

Dev & Prod (maybe UAT eventually) can simply be different database names, same Snowflake account, same Datalake account. 

You do NOT want the PowerBI people doing their own ‚Äúfill in the blanks‚Äú that‚Äôs how you get data silos. They Will Do This !!!

For example (true life example) the PowerBI people will go get their own geolocation data from god knows where and make mistakes, so that Sales Dept will have X$ in one zone and Finance Dept will have Y$ in the same zone for the same time period, as each Dept will have a different PowerBI person. 

Data must be centralized. For SSoT to work, and in every data row identity the source system for each PK. Maybe as part of your naming convention for the column. 

Don‚Äôt be afraid of typing long column names. Use acronyms when it makes sense. 

If you haven‚Äôt already, take an online course on Medallion and Kimball architecture.",2
1q92eti,2,"u‚Äôre running into the ‚Äúfounder says compliance‚Äù problem with zero policy attached. Before you redesign everything, force one clarity step: what exact requirement are we following? (‚ÄúEU data must stay in EU‚Äù, ‚ÄúIndia must stay in India‚Äù, etc.). ‚ÄúEach country in its own region‚Äù is usually not the real rule, it‚Äôs the vibe.

If data residency is actually required, then yeah: you can‚Äôt do Firebase ‚Üí Bq ‚Üí Snowflake US for everyone and call it compliant. The sane pattern is raw + PII stays in-region, and only aggregated/anonymous data can move cross-region (if allowed). Practically that means separate regional stacks (EU BigQuery dataset + EU Snowflake / or whatever) and your Airflow just orchestrates per-region configs. 

Start with 2 regions (EU + US), keep schemas identical, and create a ‚Äúglobal‚Äù layer that‚Äôs aggregates only. Biggest footgun is mixing PII across borders and hoping to ‚Äúfix later‚Äù, get the rule written down first, then architect.",1
1q92eti,3,"Man, I feel your pain. Being the only data person at a startup is a trial by fire. Your founder is right to be concerned about regional compliance, it's a huge headache waiting to happen if you don't address it early. We had a similar situation at my company. We're a SaaS platform, and we started getting customers in the EU and APAC. Suddenly, we had to deal with GDPR, various data residency requirements, and a whole bunch of other fun stuff. Our initial setup was a mess data scattered across different databases and cloud services, no clear understanding of which customer data resided where, and absolutely zero auditability. It was a compliance nightmare. The initial thought was S3 and Iceberg like your mentor mentioned. That adds a ton of complexity in terms of infrastructure and data management. Imagine having to manage multiple S3 buckets and Iceberg catalogs, one for each region? Plus, you'd still need compute instances for processing, potentially Snowflake instances per region as you mentioned. The operational overhead alone would be insane, not to mention the cost. What we ultimately did was implement a virtual data platform that sits on top of our existing infrastructure. It basically created a unified view of all our data, regardless of where it was physically stored. The really cool part was that it automatically discovered relationships between different datasets and identified Personally Identifiable Information (PII). It even flagged data quality issues, like duplicate customer records with slightly different spellings. This would have taken us weeks to untangle manually. Once we had that unified view, we could easily define data policies based on region. So, for example, any data tagged as EU customer data automatically adheres to our GDPR compliance rules. The platform also provides a complete audit trail, so we can easily prove to regulators that we're compliant. And here's the thing that really saved our bacon: we built automated data pipelines using a no-code interface on top of all of this. Before, marketing and sales were constantly bugging the data team to pull reports, create custom dashboards, and integrate data from different sources. It was a huge time sink. Now, they can do most of that themselves with the no-code tools, freeing up the data team to focus on more strategic projects. It sounds like you are running into very similar problems. The company we used has a referral program, so I'm happy to connect you. Full disclosure, I get a little something if they end up helping you out. But honestly, it was such a game-changer for us that I'd recommend them even if I wasn't getting a kickback. They really helped us get our data under control and avoid a major compliance disaster. Happy to make an intro if you think it could be a fit.",1
1q9016g,1,Word of warning on airbyte. They seem okay for small amounts of data but once you get to any kind of scale the wheels fall off. You also need to implement row checking QA and sampling between source and destination as sometimes airbytes replication actually fails but shows success.,7
1q9016g,2,"save your raw data on gcs first, then load it to bigquery, much cheaper",6
1q9016g,3,"This stack is pretty sane for your priorities. Bq is a good fit at 150k orders/yr if you‚Äôre disciplined about partitioning/retention and don‚Äôt let Looker queries go wild, and Airbyte‚ÜíBQ is a common pattern. Two quick pushes though: I wouldn‚Äôt drop line items entirely (you‚Äôll want them for margin/returns/cohort/LTV and subscription analysis), but you can keep them in a separate table/partition and only join when needed; and I‚Äôd strongly consider adding a lightweight transform layer (dbt or even scheduled SQL) so your ‚Äúbusiness tables‚Äù (orders, customers, subscriptions, spend) are stable and not raw connector schemas.

For costs/lock-in: biggest surprise costs are usually (1) Looker exploring raw wide tables, (2) GA event data blowing up, and (3) Airbyte sync volume if you pull too much too often. oh yeah also get budgets/alerts in GCP day 1, partition/cluster core tables, and create curated views for Looker so analysts aren‚Äôt querying raw. Also, if you ever want less lock-in later, keeping transforms in dbt + storing raw exports (GCS) makes migrating much easier than relying on tool-specific modeling.",4
1q9016g,4,"Your tech stack is very sane. People have provided good feedback in terms of keeping your low level grain data.


Don't hot load into bigquery but rely on external tables through Google cloud storage. We ran dbt core as a service in a docker container being hosted if you want to go through that route. If you want to keep it low maintenance, Google dataform which is their equivalent transformation platform is good enough for your needs most likely and one less thing to maintain.¬†


We were processing more data than you and we were running maybe less than 100 dollars a month in terms of GCP billing for a very similar stack (marketing analytics purposes), including Airbyte.


If you want someone to bounce ideas more deeply, happy to discuss.",4
1q9016g,5,"I mean, it will work well and I think you know that. LookML will lock you in to Google if you‚Äôre not careful but I‚Äôm not sure how much that should matter. Like you said about Airbyte, use it to stand analytics up then start looking at an OSS stack like boring semantic layer + Superset if more obvious risks start to surface. Definitely not a higher priority than getting OSS Airbyte going. Clickhouse or StarRocks are probably good off ramps from BigQuery if you want to eventually stop renting all together in the future.",3
1q9016g,6,"Seems like you have everything quite pieced out. You mention you want to use Airbyte for ingestion, but what about transformations? Certainly depends on the idea that you have for the analytics, but i think you'd easily be also in the free tier of dbtCloud. Similarly to airbyte, quite easy to move to self hosted if you need more.",3
1q9016g,7,I like the DE stack for Ecom use case. My niche is ecom and logistics and I find most ecom teams use Google so Big Query is a solid option. It also is GA4 friendly which is a big plus. GCP has many other data tools if you ever need to scale up at some point (but can get costly if you don't have someone monitoring or have one bad query). Looker does the trick and again most Ecom / digital teams use this so easy sell internally. In terms of ETL I just go for Fivetran if the customer doesn't have an interanl DE team or a savvy analyst. Can get a bit pricey though. Landing the data into Google storage and then transform from there. I find a little bit of Python goes along way in the DIY DE space as well!,2
1q9016g,8,What do you estimate the GCP costs to be around with this solution?,1
1q9016g,9,Why not just use SQL server?,1
1q9016g,10,Are you thinking of using Looker Studio or Looker Enterprise version?,1
1q8zg1d,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1q8zg1d,2,"Once you learn one data stack, the rest become much easier. It‚Äôs like your first programming language, hard at first, but after that, new languages (or tools) are mostly syntax and platform differences.

In data engineering, the core problems never change:

* Where is the data stored?
* Where does compute run?
* How do I move data or compute efficiently?
* How do I model and serve the data?

Whether it‚Äôs Databricks, Snowflake, BigQuery, or Spark-on-whatever, you‚Äôre solving the same underlying problems with different interfaces. Many ‚Äúnew‚Äù platforms are abstractions over familiar engines (Spark) anyway.

Most content creators aren‚Äôt learning everything from scratch each time; they‚Äôre reapplying the same approach to new tools. Once you‚Äôve done that a few times, picking up a new stack is a breeze. 

Also, free tiers and cheap cloud setups make experimentation easy. You don't have to break your bank to spin up a Spark cluster. 

I‚Äôve worked across multiple stacks as a consultant, and my approach has stayed the same: understand data, compute, movement, and modelling. The tool is just the means to get there.",340
1q8zg1d,3,"These creators are¬†*experts at making tutorials*, not necessarily at wrangling production-scale DE pipelines. That's a completely different skill set. Their projects are heavily simplified for teaching. They start with pristine CSV files, skipping messy data ingestion, schema drifts, or upstream failures. No stakeholders breathing down their neck with shifting requirements, SLAs, or ""can you add one more dashboard by EOD?"" 

They do great work popularizing the field (props for that), but mastery comes from the grind of real-world chaos, not 20-min YouTube episodes",169
1q8zg1d,4,"More often than not they don't know much. This is true on YouTube for not just data engineering content. They've learned how to show confidence. Also, they're usually talking about basic information. There's little advanced content out there. When you see someone competently explaining advanced content, you know they're good.",31
1q8zg1d,5,"Go look at their work history. 

Codebasics. Dude has 12+ YOE as a DE at bloomberg, probably knows some shit. Some dude with 6 months of experience at some McTech company, meh. Not glazing this dude either, have never heard of his channel before this thread.",17
1q8zg1d,6,"what makes you believe they are experts? 

Anyone can read a script....",88
1q8zg1d,7,Tutorials and working in a company/project are so different it's tough if you haven't done this a long time. let's just say a tutorial is the easiest part. putting it all together in a project where not much works as explained in documentation or tutorials is a different animal,17
1q8zg1d,8,"Tutorials and teaching are very different from years of experience in the field. They are doing a great job, those YouTubers, but they do not necessarily qualify for the job. There are some things that you only learn by doing and the extent of that can go very high in which you can't find documentation anywhere for it. 

To make a long story short, they are just scratching the surface, there's an entire empire down there that is not very visible to the naked eye.",19
1q8zg1d,9,"Some people are good with self promotion, usually people who arent that good technically",31
1q8zg1d,10,"When you are in the field for several years, you will eventually broaden your knowledges. And learning new concepts usually get faster and faster as you broaden your knowledge.

However teaching a concept and walking through tutorial is different than applying the concept in production scale PLUS all the constraints your company may have.",5
1q8ye1d,1,This post doesn't go with the guidelines. Please do not promote yourself.,6
1q8s3le,1,"Semantic solutions that allow the business to self-serve through LLMs. 

But the meta answer here is that the data analyst role is shrinking in the market. Quickly. You‚Äôre not wrong, but be thankful to have the volume of work. Possibly reframing the situation into job security could ease the feel of annoyance. My company just laid off a decent proportion of our analysts bc we‚Äôve invested in more self serve solutions and we simply don‚Äôt need the staff anymore.",6
1q8s3le,2,"Say no to stuff. It‚Äôs in the client facing people to say no to the client. 

I can‚Äôt imagine wasting time on updating colors of a dashboard.",2
1q8s3le,3,I have seen this problem in my previous orgs and usually self serve based solutions are the only way out.,1
1q8s3le,4, try some Gemini/gpt hacks and feed the screenshots in to change the colour as long data isn't too sensitive,1
1q8pa8s,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1q8pa8s,2,"The only way is via working and understanding it. You cannot buy yourself self into everything.   
  
 Anything not earned from your blood and sweat ain't worth it. You will be caught red handed in the company anyways when you fail to do something.

Better take time to learn the fundamentals and work on yourself before doing this. You can use that money for certification or paid courses rather",5
1q8pa8s,3,"I get those pesky Data Engineering Academy ads all the time.  Sometimes they claim to know data engineers making $300k, $400k, $500k per year.  I've been in the industry a long time and I call BS on that!  Possible, but extremely rare, at best!  And only for top-tier experts with millions of years of experience. This suggests to me that it is mostly hype geared or people starting out to spend a lot of money who may not know any better.  Good that you asked first!  Your call, but I'd steer clear.  If it sounds too good to be true, it probably isn't.",2
1q8pa8s,4,avoid. it's been documented as a scam on this sub before,1
1q8pa8s,5,"95% of these are not worth it and you‚Äôd be better off watching YouTube videos and trying stuff on your own time. ChatGPT is a major tool for learning basics. If you do all this and it makes sense to you then consider finding an advanced, reputable course or a college program. If it doesn‚Äôt then you didn‚Äôt waste your money",0
1q8grqg,1,"How ""key word optimized"" is your resume?


One thing I see is the resume is peppered with enough of the right technical words that it looks good to algorithms and recruiters.¬†


But it's off putting when it goes in front of people closer to the work. Anybody that knows their stuff rolls their eyes when a resume lists every rdbms you've ever queried.¬†",3
1q8grqg,2,Do you know if you're failing the technical portion or the non-technical?  If you've failed 12 technical interviews then you're just applying to the wrong positions.  If it's not the technical portion you need to brush up on your interview skills.,2
1q8grqg,3,"An idea to tinker with is to tell stories from your experience. 
Instead of telling the company xy I used z, tell a highlight from the job.
This makes your story much more plausible and without notice you add information about how you work and what work values you have.

I have this from a job coach here in Germany. So I am not sure how this applies in your country.
Make sure that in the interview you stay authentic. The coach said.

Remember you sell work time. Your personality is the bonus.

And before the job interview say to yourself that you can do it that you want to do this job.

Sometimes it is not about you, why you did not get the job. You can only review the interview in your mind and think if you are unsatisfied with some answers.
If you think you nailed it in the interview, then I would say you had bad luck.
Hth",2
1q8grqg,4,DM me. I can help with mock,1
1q8f0kg,1,"I've been using SDP with SQL for a few months and am loving it. 
The documentation could definitely use some hella work, and expectations aren't quite as robust as I'd hope, but SDP + autoloader/autocdc is goated. AMA",5
1q8f0kg,2,"From my experience, DLT works well for standard, repeatable pipelines, but it‚Äôs not a one-size-fits-all solution. Once things get complex, you can start fighting the abstraction and dropping back into imperative Spark anyway.

It‚Äôs also very Databricks-native by design, which is fine if you‚Äôre happy committing to Databricks long-term, but it does introduce vendor lock-in at the notebook and orchestration level.",4
1q8f0kg,3,We‚Äôre using SQL materialized views (which fall into this product category) and they‚Äôre working well. None of our data sources are available from a stream so we‚Äôre just using them for simple and speedy incremental processing of batch jobs. It‚Äôs easy enough to frame everything downstream of initial load as a select statement. We‚Äôre early on in the migration but have only bumped into minor documented edge cases that caused us to break pattern. Which largely lines up with my experience elsewhere - their engineering is solid and moves very fast.,2
1q8f0kg,4,I have been trying these out this week. I'm new to Databricks so there many be case of skill issue but everything being streaming for batch workloads is a bit of a pain. You seem to lose a lot of the delta table features (eg. time travel) since your downstream tables are now all materialised views.,1
1q8f0kg,5,"Is it true that spark declarative pipeline open source, and can I use it local in my vscode for example?? Can someone enlight me¬†",1
1q8e6c8,1,"You need to know schema for shard keys and with that - you can‚Äôt reliably choose a ‚Äúgood‚Äù shard keys from schema alone.

Sharding is driven by access patterns, not just structure. A practical approach is to discover candidates at runtime: inspect PKs, unique keys, and FKs, then observe which columns actually appear in point-query predicates (where, join on, partition by clauses). Use those signals to score candidate keys. Tables can be co-located only when they share a common columns. If a query doesn‚Äôt include a routable key, reject it rather than scatter-gather. This keeps the system correct, simple, and aligned with your limited project scope.",2
1q8e6c8,2,"can you clarify what you mean by sharding databases?¬† like data on different servers, or single server but data split into multiple tables?",1
1q8ble3,1,"Disclaimer: I work for LanceDB, and I wrote most of its catalog integrations. Someone saw this post and sent to me so here I am writing my first Reddit reply lol

Lance does support most of the catalog on the market, check out this page: [https://lance.org/format/namespace/integrations/](https://lance.org/format/namespace/integrations/) I guess I did a bad job marketing these features. I did publish a few blogs, but mostly under the context of integration with compute engines, like [https://lancedb.com/blog/introducing-lance-namespace-spark-integration/](https://lancedb.com/blog/introducing-lance-namespace-spark-integration/) and [https://lancedb.com/blog/lance-namespace-lancedb-and-ray/](https://lancedb.com/blog/lance-namespace-lancedb-and-ray/) . For all the supported catalogs, you can basically use Lance side by side with your Iceberg datasets.

But I think you raised a great question - ""anyone actually cataloging their lance datasets? or is everyone just yolo-ing it"". I did these integrations because we do have real customers using them. But I do see a common trend that data engineers in enterprise corporations that have data lake, data warehouse or data lakehouse setups, and trying to integrate Lance for AI features tend to want catalog support. For the frontier labs AI engineers, they tend to just prefer having storage-only solution and yolo-ing it.

To bridge that gap, I also did one catalog within Lance, the Directory Namespace: [https://lance.org/format/namespace/dir/catalog-spec/](https://lance.org/format/namespace/dir/catalog-spec/) . It is basically a catalog living in a directory, backed by a Lance table, so everything still stays purely in storage. It is also compatible with just a plain directory of Lance tables, which is typically how people start using lance format, when they do pip install lancedb and create a table. 

In general, most of the users I interact with treat embedding as a part of real data assets. You can definitely store multiple versions of embeddings generated by different models for different use cases or just compare their effectiveness and switch over time. That is actually one key use case for Lance, which is you start with a table with just a single column, maybe images or videos. You continue to do data evolution - add embedding columns, caption columns, feature columns over time, and they accumulate and become a very wide table. That becomes your entire multimodal data asset. Check out some case studies with for example Netflix [https://lancedb.com/blog/case-study-netflix/](https://lancedb.com/blog/case-study-netflix/), Runway: [https://www.ethanrosenthal.com/talks/2025-data-council/](https://www.ethanrosenthal.com/talks/2025-data-council/)",10
1q8ble3,2,"interesting. gravitino might be your solution for cataloging lance datasets. worth looking into if you're tired of the ""ask kevin"" approach.",1
1q8biej,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1q8biej,2,Intro to statistics and intro to logic.,9
1q8biej,3,"Linear Algebra is important for normal engineering, not data engineering. Relational algebra may be useful if you want to understand how SQL works.",4
1q8biej,4,"No. Your time is much better spent learning one of the cloud providers. Even an associate cert in AWS is going to be far, far more valuable. 

You don‚Äôt need a lick of math for DE.",5
1q8biej,5,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1q89jka,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1q89jka,2,"I tend to agree it is the end. 5 years ago I needed to get something integrated with AbInitio. I could not find any reasonable info/docs. That is a strong signal for me that they are in brand extraction mode and do not care about its future. 

On another hand, I strongly recommend to have multiple tools in your kit. Products live and die quickly these days.",16
1q89jka,3,"I would suggest you to pivot into Data Engineering side. Spark (distributed computing) is a great way to start, Python programming and I assume you have good SQL skills. 

You can also start to understand building cloud native analytical applications like AWS S3+EMR/Glue+Redshift/Athena or Databricks or Snowflake. 

I was IBM DataStage developer and it‚Äôs been 5 years I pivoted away from it. It was challenging at first as I was used drag and drop GUI - but you will get used to engineering aspects as you develop skills.",13
1q89jka,4,We replaced it in our company in 2019 with a combination of databricks and airflow. I thought it was over engineered and very poorly documented. And insanely expensive.¬†,6
1q89jka,5,"I was in the same place as you 3 years ago but I was working on elt tool called ODI(Oracle data integrator) I realised there are few openings for this in the market with less pay. So I did a couple of courses and transitioned into azure databricks and data factory, now I'm working as an azure data engineer. I'm already proficient in SQL so just had to learn Pyspark and Spark architecture and basics of how it works. Now is the right time to move to cloud just pick any cloud platform of your choice (azure, gcp or AWS) as you already have experience working as an etl developer it's not that difficult to become data engineer.",2
1q89jka,6,"enlighten us with your experience. i rank this tool i. my top 10 of most hated in the data & analytics realm.
and yes, do something else as fast as you can. anything would be a great improvement",2
1q89jka,7,"I've never heard of AbInitio, but I can provide a bit of guidance from my experience.

Spark and Databricks go together more-or-less, though you can run Spark elsewhere. My career since graduating in 2019 has been built on Spark, and I absolutely love it, but it's going to be a bit complex to learn thoroughly unless you're already comfortable with Scala or Python. Snowflake (SF) would probably be much easier to learn as SF is more of a SQL-first paradigm; I'd recommend learning **Iceberg** in parallel to SF.

Maybe look into experimenting with real time streaming solutions as well like (Kafka / SQS) + (Spark Streaming / Flink / Golang) to stream table changes (CDC) from an RDBMS (e.g. Postgres) into an Iceberg table that could be queried via Snowflake.

Also be sure to know when to use a row-based file format (e.g. Avro) and when to use column-based format (e.g. Parquet).

From there you'll have a solid starting point in the SQL + Snowflake + Iceberg world and can branch out to the Spark + Databricks + Delta world if required or curious.",2
1q89jka,8,Are you trying to find a new job?,1
1q89jka,9,"Yes. Not many companies use that product because it's costly even though it's sophisticated. Outside US, it's even far less",1
1q89jka,10,"Other than few small German companies, I haven't found it in any JD worldwide. Just my personal exp though.",1
1q89cew,1,"‚ÄùControl‚Äù or ‚Äùconfiguration‚Äù is often used interchangeably with ‚Äùmetadata‚Äù in this context, in my experience.",16
1q89cew,2,"There are a number of meta-data subdomains to consider: system lineage (source/target mappings), governance (criticality, sensitivity, locality), data-classification (data-type, mappings to canonical/conceptual models, temporality, coverage), field-level-mappings (for field-level lineage), monitoring (volumes, success, system-resources, cost, data-quality results)

What you decide to label your meta-dataset is going to depend on which of these sub-domains you're likely to be supporting, and who you are anticipating to be the main consumer of your metadata.",7
1q89cew,3,"Had this exact naming debate when I consolidated three separate orchestration systems last year.

Honestly the term depends on what you want to communicate:

""Job Control Framework"" or ""Job Control Tables"" is the most established. Goes back to mainframe batch processing. If your metadata drives what runs and when, this fits.

""Pipeline Metadata Store"" is more modern, emphasizes you're tracking pipeline state rather than just ""jobs."" Works well if you're in a dbt/Airflow world.

""Orchestration Control Tables"" if you want to be explicit that this layer is decoupled from your actual data models.

""Run Manifest"" is borrowed from dbt. Better if your metadata is more about ""what happened"" (observability) vs ""what should happen"" (control).

I'd skip anything with ""connector"" unless it's actually handling source system connections. That term is overloaded now, half the modern data stack thinks connector = Fivetran/Airbyte-style ingestion.

Quick side note: if you're manually querying these tables at job start to figure out dependencies and last-run timestamps, might be worth checking if your orchestrator exposes that natively through its metadata DB or API. Saves you from maintaining two sources of truth for job state..and a huge future headache.

What orchestrator are you on?",6
1q89cew,4,Pipeline orchestration configuration?,3
1q89cew,5,"I'd like to see this called a job_log rather than anything fancy.


¬†contol, configuration, metadata or anything of that ilk is perfectly fine.¬†


But from the names it sounds like data points get set based on what happened with the job rather than dictating what should happen with the job.",2
1q89cew,6,"If it's inherent to the jobs and their construction in version control (eg ""step number""), then probably ""configuration"".

If it's a property of a specific point-in-time execution, then probably ""monitoring/tracking"".",2
1q89cew,7,"ETL control framework, technical metadata (vs. business metadata) etc.",1
1q89cew,8,Just call it configuration. Nothing fancy,1
1q857di,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1q81qwi,1,Delta lake has auto versioning of tables.,2
1q81qwi,2,"Are these breaking changes? I.e. are your consumers going to need to make changes when you deploy your fixed/features in order to ensure they get the same results with their current processes?

If not, then why version?",1
1q81qwi,3,"I guess you have a data warehouse under the hood. If that‚Äôs the case, I‚Äôm increasingly recommending to materialise results NOT on the warehouse, but on an iceberg table, possibly with a REST catalog that allow for git-like management and branching (e.g. Nessie). 

This way, you remove versioning issues from the map (just use iceberg and catalog‚Äôs versioning features and best practices)

For table data access, you can use the iceberg table as external table in your data warehouse (ok if you have infrequent access or if it‚Äôs ok to have a bit of latency) or load the latest version of data in bulk (replacing the existing table) on a single work table on the warehouse if you need high availability or low latency for reads through the warehouse 

That said, there are 100s of variants and approaches :) if you have some specific needs just drop some more info on your need and stack, I think a lot of people may be able to help",1
1q81qwi,4,"I assume the users of your model will care that improvements are made, but probably not care for the specific knowledge that they are using ""v1.0"" or ""v1.1"" of the model.  I assume *you* guys care what version is in the wild, same as any need for source control.

  
Depends what you're using, I guess.  As u/chronic4you  said, if you're using Delta Lake then versioning and metadata is part of it.  I can't comment on Iceberg, we don't use it.  We do use Delta Lake, and in my case, we wouldn't have differently-named versions knocking around in terms of production.  They just get replaced.  But we don't run parallel releases and stuff like that.  For capturing version metadata in Delta Lake, you could use custom commit messages when altering/replacing tables.

  
This is kind of something I've been thinking about a bit, too.  The potential for drift between the DDL code that *is* source controlled and the object in the catalog that the DDL defines concerns me.  I don't really have a solution.



If you're using simple SQL tables for example, good luck to you.  Times past I've had various permutations of ""x\_dev"", ""x\_revision"", ""x\_update"", and even ""x\_new"" sitting in the database alongside the proper table.  Good times.",1
1q81qwi,5,"If you are open to use open table format, delta and iceberg has versioning as some of the folks here said. Otherwise I imagine you create a materialized view named with semantic versions like mv_marketing_ads_v_1_2. And start to deprecate older ones every 10 or 20 versions or so. Maintaining an change feed table is also useful to recreate older ones which you have deprecated.",2
1q7sc09,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1q7sc09,2,"I have made this kind of transition from analytics to data engineering, so here's my 2 cents:

I'd say Designing Data-Intensive Applications would be a good book if you really want to learn some foundational stuff, especially if you already have somewhat of a comp sci background. I don't think this information is really a hard requirement for getting started though.

Data engineering is just so incredibly broad. So much of your day to day job is dictated by what size company you end up applying at as well as the industry. For example, Kafka is a foundational piece of tech for a high percentage (forget the actual number) of fortune 500 companies. But it's also totally overkill for many smaller companies who may not need near-real-time analytics or near-real-time data processing.

I'd say to become very comfortable with SQL (postgres and one data warehousing dialect such as Snowflake, Redshift, or BigQuery), Python, and dbt. While some may classify dbt as an ""Analytics Engineer"" tool, I think it's a great bridge between the worlds of data analyst and data engineering. Plus it's very common at companies of many different sizes.

Then become comfortable with some form of orchestration tool such as Airflow, Prefect, Dagster, or some equivalent of those. For Airflow, there is a MWAA local runner docker image that might be useful for running a local version of Amazon's managed Airflow instance. Just search ""github MWAA local runner"" to find the repo which should have instructions. This is way easier and cheaper than running a real airflow instance.

Maybe just read up on data ingestion tooling such as Airbyte, Fivetran, or Meltano, just to fill in that gap so you understand how transactional data gets loaded into a data warehouse (ELT is a good term to know/research).

At that point, I'd say the knowledge you've acquired is transferrable to most tools that small to mid-sized companies would require (or at least desire) for an entry level data engineer position.

Good luck!",10
1q7sc09,3,"First, dont be anxious. Carreer is not one single line, but more of a squiggle that meanders between jobs, tasks, roles.


An example, I spend 5 years doing transforms and data analysis in MATLAB before meandering into BI, SQL, Python over the last decade. Ive been called in various roles data manager, BI developer, product owner, tester, data consultant, data scientist and data engineer. Currently platform engineer.


Second, In my mind, its mostly about experience and doing. Not only read, but just be out there solving data problems for companies. You learn by doing. And all the experience contributes to growing as a data professional, whatever the role you happen to fill.


Lastly, the only thing I really recommed you to read, is kimball modeling and to write a lot of python and sql.¬† Those skills (largely) transfer. All the rest (gcp, azure, aws, snowflake, adf, fabric, databricks) is dependent on the stack you happen to end up working with.¬†¬†Great that you can learn gcp now and id do it, but chances are your next job is azure..",7
1q7sc09,4,"IMHO you should as a data engineer look at
- data vault 2.0
- modeling snowflake, Star model
- data quality
- data governance
- data lake house / data lake+ dwh

Tools depend so strongly where you are going, I would focus on one tool you like.
- python + Polars or pandas ( good if you want to have data science portfolio too.
- data bricks /spark
- Apache iceberg ( data lake house setup
- Kafka, rapidmq ( real time warehosung)

You can go for Google, azure aws big data stuff. 
Snowflake,, Oracle or other industry tech will all help you.

Data bricks and DBT seem to be popular too.",6
1q7sc09,5,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",3
1q7sc09,6,"I've gone from a Data Scientist to Data Analyst to Analytics Engineer, recently to a Data Engineer and the main difference between Analyst and Engineer is ETL tooling. Data is very broad and lots of data jobs tend to involve a bit of every area of data anyway, but the main difference between the Data Analyst jobs and Data Engineer jobs is the level of ETL being done. 

Essentially learn Airflow or Databricks jobs (or an equivalent), learn data architectures and learn how to build efficient jobs and you should be most of the way there.",3
1q7sc09,7,Following. I‚Äôm in the same boat.,2
1q7sc09,8,"I have done the complete opposite, moved from BIE to DE to AE.",1
1q7s6fv,1,"I don‚Äôt understand what you‚Äôre asking in question 1.¬†

For question 2, it‚Äôs generally simpler for endusers to query pre-joined data and yes, storage is cheap and thus denormalization/redundancy is cheap (or at least the ease of use outweighs the cost of redundancy). That‚Äôs not really unique to Snowflake.¬†",2
1q7s6fv,2,"Sounds like your IT org needs to set some architecture standards and guiding principals with enforcement.  Like where these tables should be created and the type of data model used for which use case (3nf vs star vs denormalized vs one BFT). Without  architecture guiding principles, you'll end up with a rats nest and unmanageable sprawl. Talk with senior leaders and architects in the IT org, show them what is happening and what issues it will cause.  Let them be the bad guys, that is their job.",1
1q7qr1l,1,"MS fucked up by first selling Synapse as a product, pushed it and it while it was usable, it was not what was promised. Then for unknown reasons they immediately abandoned the platform and created fabric. The entire system is patched together, has multiple issues, and is not production ready. Whilst competitors have had their own issues, at least they are backing their products.

Imagine Snowflake saying, sorry Snowflake is dead we now have frosty and are no longer advancing Snowflake. Or Databricks going... now you need to use InfoGrout our new platform.

MS fucked it's adopters in its 'future' platform by doing an immediate switch-a-roo, not the first time.
As an Azure platform user we looked at Fabric and said ""no way"". So now we are looking at alternatives.",42
1q7qr1l,2,Businesses are definitely at least trying Fabric. They‚Äôre not necessarily keeping it.,13
1q7qr1l,3,"Microsoft wants to operate lean like a startup and their leadership seems to think that AI can help reduce their headcount. There will be layoffs every year, heard numbers like 10% a year.

You should be less concerned about fabric and more about their strategy potentially working and them then selling the strategy and the tech that enables it to your employer.",38
1q7qr1l,4,"Every business I know is not keeping fabric. They try it, and give up.",26
1q7qr1l,5,"People? Who are 'people'?

‚ÄãI'm working as a Data Consultant in Europe, and I'm doing more and more migrations into Fabric in recent months
The demand is definitely real",13
1q7qr1l,6,Fabric is a dog shit product,7
1q7qr1l,7,"lol no. It has nothing to do with copilot. Thats just good ol fashioned human sacrifice.

Anything to chase the dollar. 

The entire world is obsessed with double digit profit. They have absolutely zero ability to fathom a world where they‚Äôre not growing gang busters.

The tech industry most of all. The internet was pivotal. Mobile phones was pivotal. The cloud was a pivotal shift. But now they‚Äôve drained those wells of profit and they‚Äôre desperate for the next thing.

How many ‚Äòrevolutionary‚Äô things have come and gone in the past year? Blockchain (other than crypto coins), metaverse.. cmon.

AI will have its place and uses but they‚Äôre on the tail end of a feeding frenzy from AI capital investment but the funds are running out. 

And now as the bubble bursts they will claim there going lean to maintain profit margins at the levels seen during the pandemic.

I wish I could say it‚Äôs a shame the business world has lost its ethics, but the business world reflects the rest of the world these days and it‚Äôs all kinds of messed up.

And I‚Äôm not sure it ever had ‚Äòethics‚Äô to begin with.",3
1q7qr1l,8,"I don‚Äôt know what they were thinking with Fabric to be honest. Data Engineers at enterprise companies are getting more software dev experience. We‚Äôre migrating from Data Factory to tools like Databricks and Snowflake and creating data platforms that are platform agnostic. So Microsoft release Data Factory 2.0

It also really put my back up that they changed their ‚Äòdata engineer‚Äô accreditation to a ‚Äòfabric data engineer‚Äô accreditation and changed all the content of the course/exam to be all about fabric.. who wants to hire an engineer who ONLY knows fabric?

I can imagine Fabric being appealing to smaller companies who don‚Äôt have the resources of a whole department of Data Engineers, ML Engineers, Data Analysts, BI Devs etc They are the ones who will benefit from an easy to use ‚Äòall in one‚Äô solution. Even though most of the underlying solutions are better implemented elsewhere.

TLDR: it‚Äôs not a good tool for data engineers but it‚Äôs a good tool for companies who don‚Äôt have data engineers.",3
1q7qr1l,9,"My employer sells a SaaS and we‚Äôre explicitly planning to build fabric compatibility into our product to deal with Microsoft‚Äôs marketing efforts. This is in the legal industry, where virtually every company is a ms shop, and there is little scrutiny of vendor solutions.

None of the engineers at my company have anything positive to say about fabric.",4
1q7qr1l,10,Xbox denying probably gives the rumor more credibility than Azure staying quiet. They always deny just before it happens.,1
1q7n37e,1,"Before I read your post, having only read the subject question my answer was 'No, things are changing so rapidly what methodologies and tools people are using in a year or two might be very different.' Besides the fact that the market is very slow for everyone applying and looking for work I don't think it would be particularly harder than baseline if you worked a different data related job. 

Also burn out is very real and can take multiple years to recover from, it's worth a better QoL job for less pay. A short term 6mo job that pays a year pay? Hell yeah.",10
1q7n37e,2,"""I would earn my annual salary in 6 months and definitly get some mental health recovery.""

This is enough to move frankly, AI is more of a general tool in your toolbox, it's not magical, and if you think back, the last year hasn't been transformative. You're not going to fall behind in any meaningful way.",25
1q7n37e,3,Taking a contract role isn‚Äôt a ‚Äúcareer break‚Äù.,5
1q7n37e,4,"Burnout is a real thing and a really serious matter. You should prioritize this over anything else, even pay. But in your case it would be an annual salary in 6 months; that gives you 6 months for your personal development, job applications, certifications if you want...  
Moreover a new, different job expands your experience and that could be a plus on your CV.",4
1q7n37e,5,"Your mental health is worth way more. I constantly think back to my decision to do a 2 year masters program in 1 year so I wouldn't have to pay another \~$50k in tuition. I graduated in my self-imposed timeline but had a huge mental breakdown that took years to fully recover from. I unknowingly (at the moment) put a price tag of $50k on my mental health, and it was a really bad deal.",4
1q7n37e,6,"""I would earn my annual salary in 6 months and definitly get some mental health recovery.""

yes",2
1q7n37e,7,"I'm not sure why you're so worried. If you have another non-DE role in data that pays more than DE, go for it. You should think about how that contract might open up more opportunities to get paid even more without having to do DE jobs. You don't even like the DE job you're doing. There's nothing to miss. Also if you're worried about AI take over DE job, all the more reason to get out of it? The concerns are contradictory, if you know what I mean.¬†


I'm a DE and I don't think AI can take my job in the next 5 years. Nobody is gonna tell AI what the data looks like for security reason and also it's just spouting things based on probability not actual logic so the procesing logic and domain knowledge still need to be done by human. If anything, it might generate more demands to build data warehouse to train AI models, which are separate use case from the existing analytics and financial report use case.¬†


The real concern is the market downturn and if the AI bubble bursts, people might stop hiring altogether because they want to cut cost (which has nothing to do with AI as a technology but AI as a balance sheet problem). There's no way to bullet proof that future, other than a beefy saving just in case....",2
1q7n37e,8,"Hello, OP. I think taking a short break is better than ending up on a long ride to a mental asylum.",1
1q7n37e,9,"your mental health isn't worth sacrificing for keeping up with the latest AI tools, especially when you're already burnt out. The field will still be there in 6 months, and honestly the fundamentals of data engineering don't change as fast as the hype cycle makes it seem - you'd be surprised how quickly you can catch back up when you're not running on empty. If you do decide to take the break and want to ease back into the market afterward, I've heard people have good experiences with SimpleApply for streamlining teh job search process.

But really, prioritizing your wellbeing now will make you way more effective when you do jump back in.",1
1q7n37e,10,"As a hiring manager, I would be hesitant to hire someone who has quit because they couldn‚Äôt handle the role and thinks they can just take a six month break and then get back into it. 

Some stress is real in any role worth having. Perhaps this one is more stressful than others, but why? How many hours are you working? Are you able to discuss expectations with your manager? 

If you absolutely think there something unique about this position that might not be there in another company, then focus on getting another DE job, not getting out then trying to jump back in later.",1
1q7my33,1,"I do have access to almost every system, it definitely helps with troubleshooting. However, every one on my team does not have access.",7
1q7my33,2,I dont. Dont need it either.,2
1q7my33,3,how so?,1
1q7my33,4,I do because I sometimes do front-end although mostly support. This is in part because I started as a data analyst before shifting most of my time to engineering and architecture (I do both). We're also a very small team.,1
1q7my33,5,How else do you troubleshoot integration issues,1
1q7lpg6,1,JDBC/ODBC is the only thing I know. What else is there?,216
1q7lpg6,2,Only 99% of the world,118
1q7lpg6,3,"yeah, still using them. old but reliable. can't beat simplicity sometimes, even if newer options exist.",46
1q7lpg6,4,"There's more or less a JDBC driver for everything which is a good selling point. So you can have ""one"" route and multiple DBs. Not saying it's the best choice but it is convenient.",32
1q7lpg6,5,"How else should I connect? We have some sources that can only be accessed via REST API, and often you can only get 100 records back per request and you get a json you gotta parse and a continuation token. It's super slow and annoying,",15
1q7lpg6,6,Yes.  Universal tool like ODBC is better than tech-specific ones.  And SQL is king - I will always choose SQL over an API query option.,19
1q7lpg6,7,"If not using J/ODBC, then what else should I use?",10
1q7lpg6,8,Ole db has been working for me for Power BI from sql server,4
1q7lpg6,9,JDBC all the time.,2
1q7lpg6,10,"I'm nominally a DS but my day to day looks more like MLE / Python dev and my company is an AWS shop. I work with mysql, redshift and athena/s3 tables. For mysql I switched to mysql-connector-python years ago because it just works like any other python package without the need to ship an ODBC driver with my image. I use psycopg over ODBC for redshift and boto3 over JDBC for athena for the same reason.

When the language and set of DBs you work with is stable it's a lot simpler to use purpose-built connectors than to add an extra generalized layer.",2
1q7lme0,1,Fork found in kitchen,7
1q7lme0,2,"When you run out of resources and your script shits the bed, and it‚Äôs 2am reworking the job for the fourth time, you should look into Perl.

Making the obvious post a little controversial. ü§™",1
1q7lme0,3,It is just a client library. You can ingest from several client libraries including GO/Rust/Java etc. Chillax,1
1q7lcal,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1q7lcal,2,"former de here, laid off last year too, gap freaks out recruiters way more than hiring managers in my experience for certs, do one that lines up with jobs you see, not 3, and build a small ds/ml-ish side project on github hiring is rough right now",7
1q7i1v1,1,Amazing! Looking forward to checking this out.,1
1q7i1v1,2,"Looks good.  
Thank you for creating and open sourcing.

I added a link to your book as a resource in my course

[https://github.com/evidencebp/databases-course/blob/main/KnowledgeBase/README.md](https://github.com/evidencebp/databases-course/blob/main/KnowledgeBase/README.md)",1
1q7i1v1,3,thanks a lot.,1
1q7ef9r,1,Is this assuming a very specific address format?,1
1q7aw05,1,whats an RCD?,8
1q7aw05,2,Why don‚Äôt you explain it to us then?,8
1q7aw05,3,"They‚Äôre not ‚Äúofficial‚Äù in that Kimball does not explicitly cover them, that‚Äôs why. Kimball would denormalize them into a fact (e.g., periodic snapshot, factless), use SCD1, mini dimensions etc.",10
1q7aw05,4,"From what I‚Äôm reading, RCDs focus on latest data, so a type 1 SCD?",3
1q7aw05,5,"Because RCD isn‚Äôt a data model, SCD is. Rapidly changing dimensional attributes are traditionally handled with SCD type 4 but in the modern data era type 2 is a simpler design pattern that will scale fine for most MPP query engines.",2
1q7aw05,6,Kimball calls them Fast Changing Dimensions and they are definitely covered in one of his books and also when he was running training.,1
1q7aw05,7,I've not heard of either,-1
1q7atkw,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1q7atkw,2,"All these courses have recorded content, instead go for mentorship programs where you will learn, ask questions and understand concepts live. Feel free to connect for a project recommendation in data engineering if you want",2
1q7asla,1,"yes, they are expensive af. don't do it.",2
1q7asla,2,"Assuming you're talking about Delta Lake, I'd raise the question of if you actually need SCD first.  If you absolutely need it, then fine - it's an upsert and computationally more expensive.  If you can live without it then stick with overwrites.",1
1q793r0,1,"What usually works in production: (1) broadcast the small tables explicitly when possible, (2) join biggest tables last if they aren‚Äôt small, (3) consider combining small tables into one wide table before joining, and (4) persist only when you absolutely need to reuse intermediate results. Tools like dataflint can also help visualize join plans and spot performance bottlenecks before they become painful.",29
1q793r0,2,"Many joins are fine, if your job is otherwise reasonably sized (right amount of compute for your data), common performance culprits are data skew and nulls in join keys (subset of data skew really).¬†

Check the UI to see if you have any straggling executors. Most tasks should take seconds or less.¬†For highly skewed data you will see one or more executors taking much longer on join steps. Fix can be identifying skewed keys and splitting them off to broadcast instead or salting before joining to evenly distribute.",10
1q793r0,3,"This depends a bit on what you're actually joining. How big (in GB) is your data? What about the distribution of your join keys? Are some of your joins skewed? Are there null values in your join keys (this is usually a problem that causes a lot of skew and performance issues)? How much data are you joining per row, as in what is the content of the columns you're joining (if this is big this also causes problems)? Are you increasing the number of rows with these joins? Also, what is actually failing when it does? Try to see what's going on using the Spark UI while it's running to see what's taking so long, or what's going wrong.

I think repartitions and partitions will mess up your performance. Repartitions will be pointless because I think Spark will reshuffle with the next join anyway, and persists make it more difficult for Spark to optimize the entire plan and may take up space on memory / disk, which you may already be lacking.

If column renaming seems to mess up your execution plan try to do all of the column remaining before the joins (so rename columns for each dataset first) and/or do all the renaming at the end. I don't see how your action plan would result in anything other than a bunch of BroadcastHashJoins with no shuffling in between this way, given all of the joins are joining small tables to a large table on the same unique ID.

Also check how much memory each executor is using and how the data is distributed across your workers. If your data doesn't fit in your cluster's memory it will become very slow either way.

All that said, if you're doing 27 joins on 820k rows with max 64 cores it might just take a long time either way. That's a lot of work and a big plan to distribute for very little cores.",4
1q793r0,4,"It's tough to diagnose without looking at the UI / Plan. As others mentioned, you likely have skewed keys with that many joins. Check the explain plan and try to simplify wherever possible, even if it means caching or writing intermediate results to disk,",3
1q793r0,5,Not sure this is the answer you are looking for but this is small data. Does it need spark? Can this be done in SQL or in a distributed fashion? Why add complexity?,3
1q793r0,6,"

The main flaw in a 27 join pipeline is not the number of tables. It is that each join adds complexity to the plan and expands the shuffle graph. Even if each side is small, Spark‚Äôs optimizer might not treat it that way. Tools like AQE exist to reshape joins dynamically, but in practice they are not always enough, especially if Spark does not recognize your joins as equi joins.

What changed the game for us was three things combined:

* Using explicit broadcast hints instead of letting Spark guess, for example¬†`broadcast(df_small)`¬†Forces a BroadcastHashJoin rather than a nested loop join.
* Reducing the logical plan spike by collapsing intermediate transformations. This means fewer repeated¬†`.withColumn()`¬†calls.
* Using DataFlint‚Äôs plan diff tooling to see where broadcast dropped out. That exposed the real culprit.

You can tune memory, partitions, and caching, but none of that matters if the plan never selects an efficient join algorithm.",4
1q793r0,7,"Add \`checkpoint()\` or \`localCheckpoint()\` and it will be as fast as you expected. The bottleneck here are not executors memory or shuffle or cluster size. Most probably the bottleneck here is a driver and a growing lineage of the spark's execution graph.

From the \`checkpoint\` documentation you can see that this mechanics is specifically for your case:

>Returns a checkpointed version of this `DataFrame`. Checkpointing can be used to truncate the logical plan of this `DataFrame`, which is especially useful in iterative algorithms where the plan may grow exponentially. It will be saved to files inside the checkpoint directory set with `SparkContext.setCheckpointDir()`, or spark.checkpoint.dir configuration.

If you cannot set a persistent spark checkpoint dir, use \`localCheckpoints\`.",2
1q793r0,8,Are you running on databricks?,2
1q793r0,9,Welcome to the Spark hell.,2
1q793r0,10,If you're getting a broadcast nested loop join (BNLJ) it's probably your join condition. Non-equality predicates are likely the reason for this. Ask an LLM to reformulate your query using higher order functions to avoid BNLJ.,2
1q76ve8,1,"I always do design my data first.

1. Understand what problem you need to solve and what functionality is required  
2. Design your database tables to support a piece of the functionality  
3. Write a bit of code for some part of the functionality  
4. Go back to step 1 or 2",14
1q76ve8,2,"It's an iterative thing - in the enterprise, there ought to be some canonical model of the things in your domain of interest - and ideally, some established ways to identify and name those things. So on one level, your modelling should be there to be discovered (i.e. already designed)

On the flip side, there may be things that are implementation specific or which will facilitate more performant operation if you structure your data-model just-so. There's a creative tension there that needs some judgement to negotiate, and sometimes, there'll be a shift or some contextual thing that changes that means suddenly it's time to tweak your implementation model again. You need some freedom to be able to do that - but it should be constrained - go too far off-piste and nobody will understand what you're up to.

Back to the question, if you've got multiple dev streams working in the same area, it pays to have a canonical representation that they can all agree on (whether they're human or machine) to support data-sharing and integration later on - therefore, there has to be some design-first approach to data - at least at some level.

If your approach to that is to all agree on some canonical higher-level model, then you can handle implementation-specific choices by always providing a well documented mapping from your system's model back to the canonical one and get the best of both worlds.",3
1q76ve8,3,"I‚Äôm not sure if this is a trick question.
This is the proper way to do this work.",2
1q76ve8,4,"This might be a hot take but I don‚Äôt think design documentation needs to stay in sync with the implementation. The design process doesn‚Äôt exist to ensure that an English version of the software exists in perpetuity, it exists to create alignment on the MVP between stakeholders and engineering. Once it serves that purpose, just go write code and document in code comments / data catalog.",2
1q76ve8,5,"I always design my table with PKs and metrics on paper / excalidraw first.   
I add inputs first, and the expected output. If you know the expected output table, it's the 80% of the task.  
Then it's easy to connect the dots. Always trying to join tables at the same granularity, never join and aggregate, but aggregate and join.

Not a fancy plan, would take only 15-20 minutes. With AI, it's easier to get the schema of inputs (especially if you're ingesting). It used to take time to scan the documentation before, but now you can let Claude Code scan the docs and find the available data.

You can even ask to the agent what's the possible output with the existing input. It makes it so easy to plan.",1
1q76ve8,6,"You gotta know this up front, and this will be increasingly important as object storage based data infra, such as Iceberg and Hudi become more prevelant. Much like Hadoop, there are some gotcha's that are keey to plan for in how you will read AND write your data to the system, that could have detrimental impact on performance if not considered in advance.",1
1q76ve8,7,I'm a data modeler and my current team is 100% model first approach where everything goes through model first.¬†,1
1q76ve8,8,"If we are referring to things like a data warehouse and for a company with a basic ability to spend time on design, then design of the data model is a very important step.   

Think of it this way, do you know of any skyscrapers that that have been built without detailed design plans in advance....nope.  For some reason the IT industry got into the habit of assuming data modelling is not important (the whole schema on read thing that was part of hadoop).

In a highly volatile business (many changes), the model can take that into consideration to some extent (data vault for example).  

Some of the more well known pundits on Linked-in like Joe Reis are in favour of it's return from the cold.",1
1q76ve8,9,I design my tables first and then code...but i think hard about how my code should be structured as well.,1
1q76ve8,10,It‚Äôs like you‚Äôre saying you want to plan before do the work. Maybe I‚Äôm lost,1
1q74is3,1,"You can focus on problems like the 'small files' problem, 'data skew' problem, 'Out Of Memory' issues on the Driver node, and MERGE INTO jobs getting slower by the day. These are some Databricks specific scenarios you can learn about to prepare for interviews. Also, if you get questions around cost, knowing and talking about Cluster policies shall help you too.  
  
Hope this helps.",6
1q74is3,2,"For ELT questions, typical pain points are handling large joins that blow up memory, writing to slow sinks like S3 in small batches, checkpointing failures for streaming jobs, and schema evolution surprises. A good answer is not I never had issues, it is I faced X, tried Y, and solved Z efficiently. Always emphasize thought process and mitigation.",6
1q74is3,3,"Agree with the other posters. Small files and complex joins blowing memory. 

Trend wise, I would take a look at, and learn as much as you can about Iceberg and Hudi table formats.",2
1q74is3,4,"well,You‚Äôre gonna see a lot of ‚Äúmy Spark job is slow‚Äù or ‚Äúcost overruns‚Äù or ‚Äúrandom weird failures.‚Äù Typical culprit is not tuning your Spark configs or someone writing a monster join in PySpark with no broadcast, seen it too many times. DataFlint is worth a peek, just throws light on where stuff goes sideways, and Unravel too if you want options, both save endless slogging through logs. If you can talk in the interview about how you‚Äôd spot and fix a job that‚Äôs burning money or time, it always impresses, way better than just theory.",1
1q70p8d,1,"Im 100% remote but am an exception in my company.  Sort of got grandfathered in.  my work is project based so as long as deliverables get delivered, management is happy.  it‚Äôs expected that I‚Äôm generally available throughout the day and for meetings but I can run errands, meal prep or get out for a run.

The other side of it is my work means that I‚Äôm on call 100% of the time.  any overnight processing failures hit me first.  

working remote means that Im probably not first in line for promotions or advancement but I‚Äôm fine with that since the pay is good -  I‚Äôm earning large city wages and living in a small college town.  I‚Äôm even more than fine with it because I enjoy my work and love my flexability",46
1q70p8d,2,"If I‚Äôm going to be away from my desk for anything, including picking a kid up from school, it‚Äôs on my work calendar so everyone is aware. Surprise meetings happen. I‚Äôve had the CTO ask me to hop on a call as the call was starting.

There‚Äôs a bit more flexibility, but only if you‚Äôre VERY transparent about when you‚Äôre AFK and you‚Äôre VERY on top of getting your shit done.",24
1q70p8d,3,"I‚Äôm a hundred percent remote, my flexibility working hours are normal 9-5 no on call, I have meetings maybe 2-3 times a week check in with director team meeting, progress check in, to be honest I only ‚Äúwork‚Äù like 2-3 hours a day and I just watch tv while I await progress updates, pretty low stress I love it",17
1q70p8d,4,"Currently working at n night  because I was bored and had nothing productive to do. The expectation is to get your stuff done and not sandbag and be available during work hours. Being remote doesn‚Äôt mean you can just work your desired time slots.

Anything else you‚Äôd probably have to ask the manager, find a relaxed role, or earn through hard work.

Meetings are meetings, it‚Äôs probably somewhat more though because there‚Äôs not organic opportunities to interact with people so video meetings are about it.

So to directly answer your last point. That desire is probably not going to fly and you‚Äôll be unlikely to land a role if you flat out say that. Unless you are in a way different time zone than the company",12
1q70p8d,5,">what is your flexibility with working hours?

I have core hours which I need to work with a bit of flexibility baked in.  1 or 2 hours either side of core hours if I have tasks to do during the day e.g. go to the dentist.

>I generally much prefer working in the evenings and spending my day doing what I want.

Very unlikely to happen for the reasons above.  Remote is often misunderstood as ""work anywhere at any time"".  Remote more accurately is rephrased as ""work from anywhere within the country you have working rights during specific hours which have been stipulated in a contract"".

You also have the situation where if you work in the evening and there's nobody there with you, there's a strong possibility you're doing fuck all/maverick shit which is about to murder production tomorrow and there's nobody there to help.

That being said, no harm in asking.  You have to be okay with accepting this is really unlikely though.",9
1q70p8d,6,"Not being available when everyone else is negatively impacts them. But it depends on the specific org.

We collaborate a lot, and when someone isn‚Äôt available they get left behind.",8
1q70p8d,7,"I am a data engineer working remote from 10 am to 7 pm. After that I can log out ( well most of the time). I have one daily meeting and 1 weekly client meeting. Even though i work remotely , I visit the office sometimes (which is nearby) just for a change in mood",7
1q70p8d,8,"I'm a senior data engineer and work fully remotely. The entire company is remote.

I can generally work ""U.S. business hours"" but might work East Coast hours one day and West Coast hours the next day depending on what works for my schedule. If I need to be gone for an hour in the middle of the day, I block it off on my calendar. If I need to go to a longer appointment, I'll book it either at the beginning or end of the day, block it off on my work calendar, then let people know in Slack the day before. If I need to flex more than two hours like that, I usually just take half a sick day--I think I get 9 sick days a year? ""Unlimited"" vacation days, though. The assumption is that sick days are short-notice while vacation days are approved in advance.

I usually have two 1-hour meetings on Mondays, then no meetings the rest of the week; every 4-6 weeks, I have a 30-minute meeting with my manager. There is a monthly Engineering ""meeting"", but it's recorded, so you only need to join the Zoom if you feel like watching live or want to ask questions in the Q\&A; the agenda is provided in advance. Everything else is asynchronous, including the daily standups, Product updates, etc., unless you want to ask someone questions ""live"" to debug a project you're working on. Then, a meeting is scheduled. ""Just hopping on a call"" isn't really a thing in my team, because if you need to ask detailed questions on how to do something, that means the documentation needs to be improved, which means someone needs to write more down for the next person. So again: the call should have an agenda prepared and sent in advance.

I am on-call 24/7 a week at a time, every 6ish weeks. There are rarely pages outside of business hours, though. Only a few times per year.

I would say all the flexibility on my team is because everyone is senior+, so no one really requires close ""supervision"" per se. We also have tight delivery deadlines, so regardless of schedule flexibility, you won't last long if you don't get your work done.",5
1q70p8d,9,Fully remote. Super flexible hours (just be there for important meetings/ones that you can contribute to). 2 stand ups a weeks. Additional meetings can be none a week to about 6 a week.,4
1q70p8d,10,"Hours flexibility - 9-5 strict-ish, boss is extremely flexible if given notice, but has to enforce rules because some folks abuse the flexibility. Varies by company, culture, team, boss, etc.

Meeting count - Depends on the workload, and the needs of the team that week. Could be a 10%, could be 50%, rarely more than 50% on any given day.

Work/life balance - My boss is very fair. If you gave an extra hour, it's yours, take it back some time, as long as there's not a fire drill going on. But, I think that's more specific to your boss.

Overall - If you like your boss, your tech stack, your team, and are paid well enough, that's golden handcuffs! Right now, you'd have to pay me 2-3x to go in-office.

TL,DR - 
The boss, team, type of work, salary, and growth opportunities should drive a majority of your decision. The remote part is just a big fat juicy cherry on top.",3
1q6wi44,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1q6wi44,2,"if you learn DataFlint for Spark stuff, it helps tons when teams want real project skills.  
Google and AWS certs look good on resumes but tools like this make your work shine more.",1
1q6w5sr,1,Everybody wants good metadata but nobody wants to do the work to maintain the catalog.¬†,200
1q6w5sr,2,"Everyone wants a data catalog but even when they get one, it doesn‚Äôt stop them from asking the data folk every single question",52
1q6w5sr,3,Data Catalog is a great tool for self-service data discovery. But most folks end up asking the data team anyway esp if the catalog looks complicated.,23
1q6w5sr,4,"I had a contract position at a huge company, designing and creating a data catalog for the finance org. 18 months of painstaking, detail oriented work. Contract ended, and I returned to the company in a different position 6 months later. No one had maintained the data catalog, half of entries had been overwritten to null by automated processes, and it was effectively useless. 

Everything in data could be improved by someone caring, but no one cares because it doesn‚Äôt have an immediate impact on the bottom line. I‚Äôm tired, man.",33
1q6w5sr,5,"We spent 3 painful years building one to support our agency‚Äôs initiative of enabling self-service analytics and reporting, we have even started to incorporate AI to help automate and maintain it.

But ultimately it‚Äôs probably going to go away, none of the non-technical users want to use it despite lunch and learns, trainings, info sessions. They still just come directly to us with questions easily answered by the catalog.

Our last ditch effort is actually building a custom LLM / SLM that can function as a medium between the business and the catalog.

I‚Äôm quite jaded and my role is more of an analytic engineer / data modeler but the longer I work the more I realize self-service is just a pipe dream at most mid-tier places. 


Maybe big tech or some of these other tech first companies can pull it off but for every jobs (maybe I pick shitty/low-‚Äòmaturity companies) I‚Äôve ever had a centralized data team to support the agency/organization despite having a finite scalability has been the most cost efficient solution.",8
1q6w5sr,6,"My dream would be to build an actual data catalog for the company I work for. It would solve so many issues and i love organizing and cataloging things. However building one doesn't provide immediate ""business value"" or ""actionable insights"" so it won't get approved.",3
1q6w5sr,7,i still don‚Äôt understand what a data catalog is,8
1q6w5sr,8,"We have one and a lot of metadata is actually documented in it... but I'm pretty sure nobody actually consumes it.

Still it helps to have something to point to and say ""this is how this is defined.""",2
1q6w5sr,9,OpenMetadata is pretty similar to what you‚Äôre saying,2
1q6w5sr,10,"We have been using Open Metadata, the data is still not complete but we are getting there and we are happy with the product. It is a lot of labor to fill it because there is no automatic magic ETL between all the FOSS and non FOSS tools we use and the catalog, so we have to develop them.  
This is why most teams cannot afford to maintain it. It's working because our team is very mature. I don't think you can expect it for a team that has been stable for less than 5 years and is still overloaded with requests and production issues.  
Although, I expect black box ecosystems like Databricks and Snowflake to eventually provide an Open Metadata equivalent out of the box for their walled-garden tools, if they haven't yet, which would be disappointing.",1
1q6vkqb,1,"We still doing ""hot takes"" in 2026? Really?",1
1q6urxw,1,"Or just [Ryft.io](http://Ryft.io), and take a look at this : [https://www.firebolt.io/blog/unlocking-faster-iceberg-queries-the-writer-optimizations-you-are-missing](https://www.firebolt.io/blog/unlocking-faster-iceberg-queries-the-writer-optimizations-you-are-missing)",-3
1q6uoz2,1,"You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1q6uoz2,2,"Honestly you'll learn it all on the job. The first few weeks you'll just be watching training videos and working on basic tasks to get up to speed. After that, I believe interns do a ""project"" which is usually just a task that has been sitting on your onboarding buddy's backlog because it wasnt super important. But it's a way to get you practical experience on something end-to-end without being too high stakes.

If you really want to practice before you join, build a DAG in Airflow just to understand how it works. Meta uses Data Swarm as an orchestrator which is most similar to Airflow, but they've abstracted so much as to make it trivial to create a new DAG. With Data Swarm you're not even really building a DAG, you're just building the config and the back end logic is doing most of the work for you. 

They also have a custom visualization software, so play around with the free version of Looker or Tableau. Most onboarding projects are going to be something like ""take this raw data, build a denormalized / aggregate table, build a dashboard on it""

If you can do those things, write good SQL, and write basic Python, you're already ready for the job.",4
1q6uoz2,3,How was the interview,2
1q6uoz2,4,RemindMe! 1 day,1
1q6spxv,1,"Clustering order matters, but let‚Äôs think how it‚Äôs done internally. 

Previously: your table consisted of partition files, each 1 Tb. So when you scanned from select \* from table where date = x; you scanned just this big file.

With clustering: now each partition is not one big file but many smaller files, organized in a smart way. 

Like **table\_partition\_field1** and inside this file data will be sorted by field2.


But if field1 is low cardinality (let‚Äôs say - Country or even bool field), then BigQuery can do more specific files:

Like **table\_partition\_field1\_field2**. 

So that‚Äôs why when you query still leverages clustering. 

It‚Äôs oversimplifying, in reality it‚Äôs much more complex (statistic tables etc) but still the logic is around the same.",1
1q6sbey,1,Sales pitches should be banned on this sub,4
1q6sbey,2,"A lot of promises, little credibility. I'll believe it when I see it.",1
1q6sbey,3,"They are very limited. Better to do a post than to hiddenly promote stuff in comments imo. Either way, more useful to just give some useful feedback!",1
1q6rmkj,1,Why would every single dag parse entire project? That doesn‚Äôt sound right,5
1q6rmkj,2,i think it‚Äôs easy to miss where the real costs sneak in with these big workflows you might want to try something like DataFlint or even Monte Carlo to catch those odd slowdowns before they blow up your spend having something monitor for you saves so much stress wish i had that when i was knee deep in Spark jobs,0
1q6rmkj,3,Doesnt Cosmos do this e.g. break the project into many tasks which then parse the project over and over?,1
1q6r2l0,1,"No.

DE is a subset of SWE but so is backend, frontend, devops, platform etc. Companies will post SWE jobs but in most cases they are looking for a backend or frontend engineer. There is some overlap but not enough especially in today's competitive environment (you'll be up against some experienced and capable engineers who match the description much closer than a DE can). What I'm trying to say is that even though you'll find a ton of ""software engineer"" job postings, there is no such thing as a company hiring for a ""general SWE"" (it's always one of the subsets and most likely backend).

The exception are companies who post for software engineers and make it clear in the job title or description that the focus is data. I've found things like ""Software Engineer - Data Warehousing"" or ""Software Engineer - Data"". I think those are fair game for DE's, but beware, they are significantly more technical than the generic ""Data Engineer"" roles and the interview process is much more difficult.

with all that being said, anything is possible.",1
1q6r2l0,2,"do apply, there is a distinction at least in the UK. data engineers have a reputation for poor software quality. Software engineers who are familiar with data practices and processes have more opportunity.",0
1q6omnc,1,"Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*",1
1q6omnc,2,"I don't think DE is an straight entry-level position, even as a junior one, regardless, it's really down to luck.

It depends heavily on location, and I know a lot of people are looking into data engineering as something, presumably, getting more in demand.

However, at this point, I don't think anybody can tell you what things will be like in 2027.

If you think you will like the work, and if you feel it's a field you want to grow in, it's worth trying.

If AI leaves everybody out of work, there's very little difference what you'll choose anyway, haha.",49
1q6omnc,3,Based on your location. In my location entry level data engineering jobs needs 3 years experience.¬†,17
1q6omnc,4,"I think data engineering is pretty safe for the next few years. Such a small percentage of businesses have got a good handle on it. Yes, the tooling will become more automated but there is still a large human factor at play. System integrations, business understanding, user training etc. So many facets to the job.

Data engineers will spend less time writing boilerplate code and shuffling data through medallion layers - more time will be spent working out how to extract value from the data for businesses.

The race to build so many data centers to handle the demand for AI workloads is a solid clue :-)",32
1q6omnc,5,"Honestly? If you wanna have a stable well paying job, becoming an electrician is probably the best medium term career move. Power centers are booming and there are not enough of them. Finish your electrical degree and start looking for a job! You can always program on the side and if you build something worthwhile maybe you can enter the industry.",16
1q6omnc,6,DE is not entry level job anymore.,7
1q6omnc,7,"If I had a conversation with someone about this then the first thing I'd ask them to tell me something about data engineering.

If it's obvious that there has been no real effort at all to find out what DE means in today's world then to me it is just an emotional conversation. It's a super interesting field with constant new developments. And there are a lot of jobs. So at the very least I would expect someone to have spent a dozen hours watching random things on YouTube and finding things that are interesting enough to want to talk about. 

There are a lot of people who say they want things, but they don't prove it even a little bit.

As I said, it's a very exciting field with a lot of opportunities. Foster some excitement by exposing yourself to interesting things. 

Good luck.",12
1q6omnc,8,"no one knows what the job market will be like in 12 months, just like 12 months ago no one knew how it would be today (think about the change in LLMs).

your best bet with any career change is to get as close to the title/job description with an INTERNAL transfer and then use that to line up a new job elsewhere.",3
1q6omnc,9,"Why Data Engineering specifically? What makes you so sure it‚Äôs right for you? Why don‚Äôt you enjoy electronics? These days, it looks like a far more stable career",3
1q6omnc,10,"The truth is Data Engineering (generally speaking) isn't an entry level role. It typically requires a breadth of knowledge, so inherently it's harder to get your foot in the door as a junior DE as opposed to a junior SWE, for example.

The market has also been rough for a few reasons -- market saturation due to lots of folks trying to pivot to DE, companies leaning more heavily on GenAI and senior engineers to attempt to replace junior engineers, and the overall ebb and flow of the tech field over the past couple years.

That said, if you make a concerted effort with your studies and plan of action (whether that's trying to find the relevant internships, or trying to get your foot in the door as either a SWE or DA, etc.) it could be a worthwhile endeavor.",6
1q6of6k,1,"Hello Everyone.  Data Dinosaur here of 35 years.  LC?  Low Code?  Liquid Clustering? Leet Code?  Limited Contract?  Something else?

I'm thinking based on the context, Leet Code.",9
1q6of6k,2,it's simple if you don't want to solve LC then turn down roles that require it. That's what I've done for the longest time and I've led a happy life.,1
1q6of6k,3,"Are they telling you there will be no whiteboarding or are they failing to mention that there will be whiteboarding? I've found that recruiters are often not proactive about telling interviewees the specifics of the interview process so I feel like it's important to ask every step of the way who you'll be interviewing with, whether it'll be more technical or more of a culture fit interview, and whether there will be whiteboarding.",2
1q6l5be,1,"Find a use case for your own life, something you would actually want to use. There's countless generic projects people have done 10000 times, what's something from other parts of your life, perhaps a hobby or interest that would be better with a tool like you describe? To give be little more concrete, I'm a big skier and had always questioned why ski trail ratings sometimes felt so arbitrary. So I built a simple universal ski trail rating model, and went from there. It eventually turned into a website with a data ingest pipeline from multiple sources, a database, and an ever growing set of analytics to display",10
1q6l5be,2,"Brother, respectfully, if you have been a DE for 2 years and still can't come up with your own projects, there's something going wrong.

At this point, your work as a DE with all of the things you mentioned IS your portfolio.  Call me harsh although if I was reviewing somebody's application and they're still beefing out their page with side projects after working as a DE for two years, I'd be asking wtf have they been doing for the past 2 years.",11
1q6l5be,3,"Current big-ish one Im working on is showing different ways of loading the spotify million playlist dataset into a normalized db schema. Straight sql, python single threaded (sequential load and vectorized), multithreaded, distributed w/ celery, distributed w/ airflow, and spark. 

Comparing execution time, compute resources, error handling, etc. and when / what size project I'd use each process for",2
1q6l5be,4,"I'm in a similar position to you with the same experience and started my own project a few weeks ago. The learning curve is huge, and I love everything about it.

Be sure to start with a project/data from your own environment. In my case, I extract data from Whoop, Strava, and a lunar calendar to build conversational analytics to generate insights for my training.",1
1q6l5be,5,"when you want to show what you can do in real data engineering, building a pipeline from ingestion to analytics is the way to go, and don‚Äôt skip orchestration, that‚Äôs what makes it sing. if you‚Äôve got ETL and cloud in your toolbox already, try pushing into spark with some heavy transformation logic, and see if you can automate debugging steps as part of your workflow. tools like DataFlint help a lot with spark optimization, but you could also check alternatives like Databand or Monte Carlo for monitoring. in the end, your project should reflect how real teams work, with messy data, some pipeline failures, a bit of tuning, and clear output, makes the portfolio stand out way more.",1
1q6l5be,6,DM me,1
1q6l5be,7,Something regarding real estate market perhaps?,1
1q6j5wn,1,(This question shouldn't be answered without a check issued. Focus groups are paid.),1
1q6ixt6,1,Medium?,19
1q6ixt6,2,"Promote yourself.

Don't sound like a Linkedin Lunatic.

Pick one.  Self promotion at it's very core is being a cringe merchant so if you want to self promote, then go ahead and start being okay with sounding like a twat and not worry about where you post.",25
1q6ixt6,3,"I started blogging on Medium and posted articles to LinkedIn. The key is quality articles that aren't the same as everyone else. I do career advice. Intermediate dbt topics, etc. Quality trumps quantity.

It's difficult to measure the impact, but I was contacted by two book publishers and a few companies to write for them. I also noticed company recruiters referring to my articles quite a bit in their pitches. 

FYI - Data Engineering Things on Medium is a great community for this. Great way to get exposure quick.",9
1q6ixt6,4,Get over yourself and write if you want to. No one really cares what imbeciles on reddit think about your LinkedIn posts.¬†,4
1q6ixt6,5,Substack?,2
1q6ixt6,6,How does one get into sales engineering‚Ä¶? Asking for a friend,2
1q6ixt6,7,"Totally get the hesitation. A lot of good DE writing actually lives outside LinkedIn because people just want practical stories, not personal branding. Posts that break down a weird problem you ran into, a tradeoff you made, or something that failed usually land well. Writing like you are explaining it to a coworker instead of an audience helps keep it grounded. If it feels useful or honest to you, it usually does not come off as salesy to others either.",2
1q6ixt6,8,"Write technically focused stuff **with a point of view**. I don't like using a third-party service, I like hosting my own blog so I have more control, but I can see the discoverability aspect of wanting to use medium or substack. They just suck.

Don't, don't use AI to write. You can use something like grammarly, but don't let an LLM write your posts. You can share them on LinkedIn, but don't write bombastic stuff to generate hits. Unless you want to be a lunatic. source: have had a blog for 15+ years, get paid to speak and write stuff.",2
1q6ixt6,9,"I recently released my own personal portfolio for exactly this use-case in mind!

[datamays.com](http://datamays.com)

Feel free to fork [the repo](http://github.com/dimays/datamays) and build your own, if you like the design.",2
1q6ixt6,10,"You need a cringe filter. It's developed by working in the field and getting a feel for what's right and what isn't - so if it's not you, get help or guidance there.

And after that you might still trigger some people, can't make everyone happy.

Personally I have morals I follow consistently like the content should be genuinely useful or helpful or at least amusing, and I use my gut feeling from being a practitioner (10y) to feel for cringe and not do to others what I wouldn't accept myself.",1
1q6ienl,1,"Sounds cool to me, I'd be happy to see how it works myself!",2
1q6i8vy,1,Don't normalise certification slop.,14
1q6i8vy,2,375$ ? why so expensive?,6
1q6i8vy,3,"Cool!

But are these useful only to people who are already working/using snowflake?",2
1q6i8vy,4,"It is better to create the hamster wheel than to run along someone else's hamster wheel.

""What if everybody did this?"" And now you know why every thing you might possibly be employed to do has a 10 year and $10k hamster wheel attached. Getting 100 people to run your $10k hamster wheel is better than a W2.

You may employ team members and recognize you'd prefer to see a solid project as proof of work than a hamster wheel gold star ""I was a good hamster"" sticker.",1
1q6hnaf,1,"Hey,

I've wrote a newsletter about this specific topic :

[https://thedatagovernanceplaybook.substack.com/p/how-to-land-a-job-in-data-governance](https://thedatagovernanceplaybook.substack.com/p/how-to-land-a-job-in-data-governance)

Hope this helps, i give examples of DIY projects you could do. But overall you need to come up with a clear strategy plan for the company you're applying for - as they usually don't know where to start on this.",1
1q6hnaf,2,"Pragmatically, I think you need to start posting on Linkedin where your identity is shown.

There are ton of posts there related to the Data Governance.

You search a post, and like and comment, then repeat for a while. This will not only improve your personal profile, but at the same time, you will learn what real challenges and stories people are promoting - this will give you a lot more context when it comes to prepare for interviews.",1
1q6hnaf,3,"i remember being in your shoes, with a bunch of certs and no idea how to get that first practical break, the gap between learning and doing can feel huge. practical exposure is key, and honestly, picking a tool like DataFlint or even similar ones from the market and building a mini-project around data governance issues (think lineage, quality, or access controls) is something that makes you stand out, especially since not many applicants show their thinking in action. make sure you document your process and put it up on github or a blog, gives you talking points and something to walk through during interviews, plus it shows initiative. interviewers love when you can walk them through how you solved a real problem or handled a hiccup, instead of just textbook answers. keep an eye on roles with data wrangling or process improvement tasks and don‚Äôt be shy to reach out to people in those jobs on linkedin, sometimes a quick intro leads to a referral or feedback you never expected.",1
1q6hhef,1,"Yeah, it should work. I wouldn‚Äôt enable the ‚Äòautomatic‚Äô option, but you should know your table well enough to choose which fields should be included in the liquid clustering command. 

And don‚Äôt create/populate the table and enable liquid clustering afterwards. Have the table existing with the feature enabled before writing (so don‚Äôt use CTAS or overwrite).",3
1q6hhef,2,"I think the limit is mostly hardware and cost, so as long as the cluster/warehouse can handle the IO and compute, 100 TB by itself isn‚Äôt a blocker.

But the more interesting question is why you‚Äôd need liquid clustering on a 100 TB table in the first place.

At that size, it often looks like a raw or near-raw table, and raw tables usually have predictable access patterns (time-range filters, append-only writes). In those cases, directory-level partitioning on a timestamp is usually sufficient and cheaper, because partition pruning is extremely effective and requires no ongoing reorganization.

Liquid clustering starts to make sense at this scale when:

* Queries frequently filter on high-cardinality keys (e.g. account\_id) in addition to time.
* The table isn‚Äôt strictly append-only (MERGE/UPSERT, late data, corrections).
* Access patterns evolve and you don‚Äôt want to constantly redesign partitions.
* You want to avoid over-partitioning while still getting good file-level pruning.

So the question isn‚Äôt ‚Äúcan liquid clustering handle 100 TB?‚Äù, but rather whether the workload actually needs it. For a truly raw, time-sliced dataset, partitions alone are usually the right tool. Liquid clustering is more justified when the table behaves less like raw ingestion and more like a shared, multi-consumer analytical dataset where time alone isn‚Äôt selective enough.",2
1q6hhef,3,"probably should check databricks docs, limits are usually hardware-related",1
1q6hhef,4,"liquid clustering supports large data sets, and databricks scales up pretty well, but once you hit 100tb, all sorts of edge cases pop up. you might want to use DataFlint to monitor those spark jobs since it spots pipeline issues before they get ugly. careful with the shuffle size and worker memory, small changes there can make a huge difference, keep testing as you scale.",2
1q6fxsb,1,"You should apply to similar roles only since you have experience there. If you fear there might be a job cut wave coming, start preparing on trending tools and tech stack in data engineering, focus in core concepts. Do as much hands on as you can and add that to your resume, sell your story and you will land a job in no time! If you still have any issues feel free to connect with me",1
1q6frgw,1,"I tried finding what I'm thinking of so I can be wrong, especially since I'm not a MySQL expert, but I think inserting is better with larger files. I'm not sure how many actual GB your 250e6 record file is, but it may be worth just doing that?

Also, I see that your table is indexed - remove indexing before import and index on added data after.

Edit:

What I was thinking of was actually about `LOAD DATA` -- it's better to use `LOAD DATA` for larger imports than `INSERT`: https://dev.mysql.com/doc/refman/9.5/en/load-data.html",19
1q6frgw,2,You might have a lock on the table/indexes.  Maybe drop the indexes and rebuild afterwards?,22
1q6frgw,3,"Since, your table is indexed, you are most likely running into a bottleneck. Basically, when your table is empty, MySQL just throws the data in. But as the table grows (after those first few million rows), MySQL has to work strictly harder to update the indexes and check constraints for every single batch you send. It spends more time shuffling the index tree than actually storing your data.

Try using ""rewriteBatchedStatements=true"" in your JDBC Connection String and let us knw if it solves the performance issue.

Also, if it is one time thing or full reload, you would be better off dropping the indexes, the non-primary ones at least, before you begin with the load/reload.

Hope this helps.",8
1q6frgw,4,"Make a new table, no index but same schema, different name

use load Data to insert into the new table

Remove the old table and rename the New table to the old name.

slap your indexes and point the fks to the new table if any exist.",7
1q6frgw,5,Why did you divide it into files? How did you do it? Is the table indexed?¬†,2
1q6frgw,6,"Keep in mind MySQL supports up to 1000 inserts per statement, so if speed is important you might consider batching them.

eg:

Insert into table (a,b,c)
VALUES (1,'d','e'), (2,'f','g');",2
1q6frgw,7,"""load data"" is more efficient for large imports, and as several others suggested, dropping indices helps avoiding bottlenecks. When you insert data in the empty table no checks are done and import is done in no time. But when you have already loaded many files each insert has to check for constraints and your performances drop.  
  
Updating statistics for the table after each batch import might also improve performances.",2
1q6frgw,8,"indexes cause writes to be slower because they require updating the index/maintaining its integrity. 

the simplest solution is to write each of the files to their own table, then after they're in the DB then you can just merge them into one using a union from spark.",2
1q6frgw,9,"Is the source data already in another table or are they in files? LOAD DATA INFILE  command from a supported cloud storage (S3, etc) is super fast if you find the sweet spot of the correct chunk size to divide/segment your source. look up other bulk data loading options and tools supported based on how your mysql is hosted/setup.",2
1q6frgw,10,"Have you checked metrics and logs to see why the later files are taking longer? Like do you have a monolith script that is doing both the splitting into sub files and then looping thru them all?

Can you write the total 250 records into object storage with partitions and then have jobs that run in parallel, where each job is only concerned w a single partition/file?",1
1q6cug2,1,"Ultimately the best data catalogs include data lineage information from the metadata they ingest. The purpose of a catalog ultimately is to be the single source of truth for questions around data usage, data security policies, business semantics, etc. It‚Äôs a coordination mechanism for the company to know how things come together. Lineage is a huge part of this because it shows not just how data flows from a transformation perspective, but also how systems connect. I personally wouldn‚Äôt want to make the investment in a catalog tool if it didn‚Äôt also include lineage.",8
1q6cug2,2,"Short answer is yes.  I've used Unity Catalog by Databricks, Alation, and OpenMetadata for this.  More of the last two.  Ablation is spiffy, but costly and a hard sell.  Openmetadata is what I'm running now and does the job just fine.  I think we're moving towards Unity catalog as they are going to add connections to outside sources.",3
1q6cug2,3,"To be honest, I see only two options how can it be possible:  
\- Data Catalog is tightly integrated with your query engine  
\- Data Catalog has an API to ingest lineage from the query engine

First option will work only if it is a vendor solution (like Databricks + Unity) or if the catalog itself is a part of query engine. So, you either will have a vendor lock or mixing different entities in one (like Hive Metastore + Hive SQL).

Second option looks nice, for example, an OpenLineage standard can be used, but the existence of lineage in catalog will depend on query engine.

If I would build a solution on open source technologies, I would prefer a second option. Something like DataHub that supports OpenLineage ingestion via REST API. But I would avoid heavily tighten ""all-in-one"" solutions and would keep lineage as an option.",2
1q6cug2,4,"In my experience, lineage and catalogs solve real (and different) problems, and having them together can be genuinely useful for ownership and understanding how data evolves.

Where teams still struggle is after that. Even with good lineage and a solid catalog, it‚Äôs often unclear which data is actually sensitive and whether current access is appropriate as things change.

That‚Äôs usually where security comes in - not to replace governance tools, but to make them actionable from a risk perspective.",1
1q6cug2,5,Dagster,1
1q6cug2,6,Yes there's no point having a catalog that doesn't also do lineage.,1
1q6cug2,7,Yes that would be useful. Most companies are doing a terrible job in specifically this area.,1
1q6cug2,8,"for me, the value really comes when the lineage and catalog are integrated not as two separate things but as a single pane, the context is all there, no switching around. DataFlint comes to mind if you‚Äôre deep into Spark, it picks up a lot around workflow and performance, pretty helpful if you want lineage plus operational details together. you might want to check out Atlan or Amundsen too for more catalog features, it‚Äôs kind of a spectrum depending how much automation or discovery you want. bottom line, start from whatever problem eats up your team‚Äôs time most, add tools to hit that pain point first, always easier to expand later than to backtrack.",1
1q6cug2,9,"That overlap you‚Äôre seeing is real, and honestly, a combined solution is becoming the standard rather than a ""nice-to-have."" In my experience, separating them usually leads to one becoming stale because they rely on the same underlying metadata context to be useful.

Here is how I‚Äôve seen this play out in practice -

1. The Platform Approach (Unity Catalog) **-** I‚Äôve used Unity Catalog extensively for this. It essentially forces the catalog and lineage to live together. Because the governance (permissions, access controls) creates the lineage automatically as jobs run, you don't have to ""maintain"" the lineage separately. The catalog is the map of the lineage. If you treat them as separate tools, you end up doing double the work to keep them in sync.

2. The Open Source Route (OpenMetadata) **-** If you are looking outside of a specific vendor ecosystem, tools like OpenMetadata are tackling this exact ""all-in-one"" problem. They treat data assets, lineage, and quality checks as a single entity. It proves that the market prefers a unified view. Data engineers don't want to tab switch between a catalog to find a table and a lineage tool to see where it came from.

3. The Shift to Orchestrators (Dagster) - The most interesting shift right now is actually happening in the orchestration layer. I‚Äôve been using Dagster, and they handle this through ‚Äúsoftware-defined Assets."" Instead of lineage being an afterthought or a documentation task, the lineage is defined by the code itself.

* Assets **-** You define the data asset, and the lineage is inherent.
* Asset Checks **-** You attach quality checks directly to those assets in the graph.

So, to answer your question, Yes, a combined solution is useful. In fact, things are moving even further, merging Catalog + Lineage + Execution (Orchestration) into one view so that ""accountability"" isn't just a log you look at later, but part of the pipeline run itself.

Hope this helps.",0
1q6cug2,10,"**Yes - combined catalog + lineage is the way to go**, and it's becoming the standard rather than a ""nice-to-have.""  
  
From working on DataHub at LinkedIn, here's what I learned:¬†**separating them creates semantic drift at scale.**¬†Your lineage says ""user\_events\_v2"", your catalog says ""user\_events"", and nobody knows if they're the same thing. When you have 100K+ datasets, this breaks governance completely.  
  
**Why integration matters more than people realize:**  
‚Ä¢¬†**Metadata propagation**¬†\- tag one source table as PII, lineage cascades it downstream to 50 derived tables automatically. Manual tagging doesn't scale.¬†  
‚Ä¢¬†**Impact analysis**¬†\- ""this pipeline failed, who owns the upstream data?"" If you're jumping between tools, you've already lost.¬†  
‚Ä¢¬†**Trust flows backwards**¬†\- you can't assess data quality without tracing upstream. Separate systems = manual stitching every time.  


Most of the modern catalogs are moving to a model of  
\- out-of-the-box connectors that ""pull"" lineage into the system from external systems  
\- support reporting of lineage INTO the system via standard catalog-native API-s/agents/emitters or vendor-neutral protocols like OpenLineage

  
**The caveat the other responses touched on:**¬†some solutions like Dagster's ""software-defined assets"" are elegant but only work for what the orchestrator controls. Your data doesn't start in the warehouse:  
‚Ä¢ Production systems feeding Kafka streams¬†  
‚Ä¢ Real-time stream processing (Flink, Spark Streaming)¬†  
‚Ä¢ Cross-cloud data movement that never touches your orchestrator¬†  
  
At my previous company, \~40% of critical lineage came from¬†**outside**¬†the orchestration layer. You need a catalog that can¬†**aggregate lineage from multiple systems**¬†(streaming, production databases, warehouses), not just generate it from one orchestrator.  
  
**The test:**¬†can you trace a BI dashboard ‚Üí warehouse ‚Üí dbt ‚Üí Kafka ‚Üí production database in one query? That's 5-6 systems owned by different teams. The catalog is your aggregation point. Best of luck with convincing your customers and the world!",-1
1q6862h,1,"Just a parquet file if it's cold data. For two gigs, load into duckdb when you're analyzing and import it into the cache of your reporting solution.",4
1q6862h,2,"You don‚Äôt need a data warehouse, you need a database. Postgres (or even SQLite if you‚Äôre lazy) will handle this effortlessly, give you indexes for Boolean searches, and plug straight into Metabase. People over-warehouse way too early for problems that are jus OLAP-lite.",3
1q6862h,3,"1MM rows / \~1‚Äì2GB is **not ‚Äúbig data‚Äù** in the data engineering world, but it is big enough for CSVs*.* You basically want **a database**, not a file so that you can query fast. Here are the options in the priority order but you need to convert this big fat file once. 

  
1. DuckDB - zero infra but you can query faster. I prefer to load the file and then use it  
2. Load it into the database, probably relational

#",3
1q6862h,4,"PArquet file in an Iceberg table, then you can bring whatever query engine you want to the party, and aren't tied to any one. DuckDB, Firebolt, Snowflake etc etc etc.

And no data migration when you do want to swap it out.",1
1q6862h,5,"First step: get it out of CSV (row-based). Convert it to columnar storage like parquet or iceberg. That alone will make it smaller and much faster to query.

After that, where to host it depends on budget, growth, and your existing stack. For \~1M rows / 1 - 2GB:

* Postgres (managed) works fine and integrates cleanly with many computing platforms.
* DuckDB / MotherDuck is great if you want simple, fast SQL over Parquet without much infra. Note, MotherDuck is PaaS version of DuckDB
* AWS, Azure, GCP platforms if your org already has them.

Databricks / Snowflake / Redshift are solid but likely overkill unless you already use them. The key thing is: don‚Äôt ship CSVs around, store columnar data and query it where it lives.",1
1q60wdp,1,"I have experience in bigquery, snowflake, and now databricks and I mention it in my resume even if the description doesn‚Äôt match. Different flavors of data warehousing but they all share core concepts. Yes incorporate it all. It shows that you have the ability to switch gears and learn new things.¬†",7
1q60wdp,2,"1) just add ‚Äúsimilar tools eg bigquery‚Äù and transferable skills 2) absolutely apply, learn snowflake basics fast. hiring is rough now",1
1q60wdp,3,Understanding the underlying concepts is far more important than memorizing a particular tool.,1
1q60wdp,4,"All of these are different tools but core concepts remains the same. I would add transferable skills as other mentions and then tools as well. On the other hand, I would spend a weekend exploring snowflake, couple of youtube videos and a small project on snowflake so that it gives me enough knowledge and add the tool along the core skills.",1
1q60wdp,5,"If you don't have experience with any of the specific tools they are asking for then there is no point in applying for the role - becasue there will always be people applying for the role who do have this experience and you won't be able to compete with them.

If you have experience of the main tool(s) they are asking for but not of the less important tools then it's probably worth applying, otherwise not.

For example if the role is looking for a Fivetran ETL developer with experience of Snowflake and you're a Fivetran developer with no Snowflake experience then it may be worth applying - assuming you have time to learn the basics of Snowflake before any interview. However, if you were a Snowflake developer with no Fivetran experience then there's no point applying for the role",1
